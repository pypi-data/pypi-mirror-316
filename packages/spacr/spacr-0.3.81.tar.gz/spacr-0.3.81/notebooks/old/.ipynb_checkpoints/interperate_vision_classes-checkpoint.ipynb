{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9152f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(output['feature_importance_channel'])\n",
    "display(output['feature_importance_compartment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd001ba8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models already downloaded to: /home/carruthers/anaconda3/envs/spacr/lib/python3.9/site-packages/spacr/resources/models\n",
      "plate: 1 cells:60816\n",
      "plate: 1 nucleus:80889 \n",
      "plate: 1 pathogens:111317\n",
      "plate: 1 cytoplasms: 60816\n",
      "cells: 60816\n",
      "cells grouped: 60816\n",
      "cytoplasms: 60816\n",
      "cytoplasms grouped: 60816\n",
      "nucleus: 80889\n",
      "nucleus grouped: 60816\n",
      "afer multiinfected Float: 111317\n",
      "pathogens: 111317\n",
      "pathogens grouped: 60816\n",
      "Generated dataframe with: 781 columns and 60816 rows\n",
      "Expanded dataframe to 2575 columns with relative features\n",
      "Feature Importance ...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "%matplotlib inline\n",
    "\n",
    "def interperate_vision_model(settings={}):\n",
    "    from spacr.io import _read_and_merge_data, _results_to_csv\n",
    "    from spacr.plot import plot_image_mask_overlay, _plot_controls, _plot_recruitment\n",
    "    from spacr.utils import _object_filter, annotate_conditions, _calculate_recruitment, _group_by_well, save_settings\n",
    "    from spacr.settings import get_analyze_recruitment_default_settings\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    import shap\n",
    "    \n",
    "    def generate_comparison_columns(df, compartments=['cell', 'nucleus', 'pathogen', 'cytoplasm']):\n",
    "\n",
    "        comparison_dict = {}\n",
    "\n",
    "        # Get columns by compartment\n",
    "        compartment_columns = {comp: [col for col in df.columns if col.startswith(comp)] for comp in compartments}\n",
    "\n",
    "        for comp0, comp0_columns in compartment_columns.items():\n",
    "            for comp0_col in comp0_columns:\n",
    "                related_cols = []\n",
    "                base_col_name = comp0_col.replace(comp0, '')  # Base feature name without compartment prefix\n",
    "\n",
    "                # Look for matching columns in other compartments\n",
    "                for prefix, prefix_columns in compartment_columns.items():\n",
    "                    if prefix == comp0:  # Skip same-compartment comparisons\n",
    "                        continue\n",
    "                    # Check if related column exists in other compartment\n",
    "                    related_col = prefix + base_col_name\n",
    "                    if related_col in df.columns:\n",
    "                        related_cols.append(related_col)\n",
    "                        new_col_name = f\"{prefix}_{comp0}{base_col_name}\"  # Format: prefix_comp0_base\n",
    "\n",
    "                        # Calculate ratio and handle infinite or NaN values\n",
    "                        df[new_col_name] = df[related_col] / df[comp0_col]\n",
    "                        df[new_col_name].replace([float('inf'), -float('inf')], pd.NA, inplace=True)  # Replace inf values with NA\n",
    "                        df[new_col_name].fillna(0, inplace=True)  # Replace NaN values with 0 for ease of further calculations\n",
    "\n",
    "                # Generate all-to-all comparisons\n",
    "                if related_cols:\n",
    "                    comparison_dict[comp0_col] = related_cols\n",
    "                    for i, rel_col_1 in enumerate(related_cols):\n",
    "                        for rel_col_2 in related_cols[i + 1:]:\n",
    "                            # Create a new column name for each pairwise comparison\n",
    "                            comp1, comp2 = rel_col_1.split('_')[0], rel_col_2.split('_')[0]\n",
    "                            new_col_name_all = f\"{comp1}_{comp2}{base_col_name}\"\n",
    "\n",
    "                            # Calculate pairwise ratio and handle infinite or NaN values\n",
    "                            df[new_col_name_all] = df[rel_col_1] / df[rel_col_2]\n",
    "                            df[new_col_name_all].replace([float('inf'), -float('inf')], pd.NA, inplace=True)  # Replace inf with NA\n",
    "                            df[new_col_name_all].fillna(0, inplace=True)  # Replace NaN with 0\n",
    "\n",
    "        return df, comparison_dict\n",
    "\n",
    "    def group_feature_class(df, feature_groups=['cell', 'cytoplasm', 'nucleus', 'pathogen'], name='compartment', include_all=False):\n",
    "\n",
    "        # Function to determine compartment based on multiple matches\n",
    "        def find_feature_class(feature, compartments):\n",
    "            matches = [compartment for compartment in compartments if re.search(compartment, feature)]\n",
    "            if len(matches) > 1:\n",
    "                return '-'.join(matches)\n",
    "            elif matches:\n",
    "                return matches[0]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        from spacr.plot import spacrGraph\n",
    "\n",
    "        df[name] = df['feature'].apply(lambda x: find_feature_class(x, feature_groups))\n",
    "\n",
    "        if name == 'channel':\n",
    "            df['channel'].fillna('morphology', inplace=True)\n",
    "\n",
    "        # Create new DataFrame with summed importance for each compartment and channel\n",
    "        importance_sum = df.groupby(name)['importance'].sum().reset_index(name=f'{name}_importance_sum')\n",
    "        \n",
    "        if include_all:\n",
    "            total_compartment_importance = importance_sum[f'{name}_importance_sum'].sum()\n",
    "            importance_sum = pd.concat(\n",
    "                [importance_sum,\n",
    "                 pd.DataFrame(\n",
    "                     [{name: 'all', f'{name}_importance_sum': total_compartment_importance}])]\n",
    "                , ignore_index=True)\n",
    "\n",
    "        return importance_sum\n",
    "\n",
    "    # Function to create radar plot for individual and combined values\n",
    "    def create_extended_radar_plot(values, labels, title):\n",
    "        values = list(values) + [values[0]]  # Close the loop for radar chart\n",
    "        angles = [n / float(len(labels)) * 2 * pi for n in range(len(labels))]\n",
    "        angles += angles[:1]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(labels, fontsize=10, rotation=45, ha='right')\n",
    "        plt.title(title, pad=20)\n",
    "        plt.show()\n",
    "\n",
    "    def extract_compartment_channel(feature_name):\n",
    "        # Identify compartment as the first part before an underscore\n",
    "        compartment = feature_name.split('_')[0]\n",
    "        \n",
    "        if compartment == 'cells':\n",
    "            compartment = 'cell'\n",
    "\n",
    "        # Identify channels based on substring presence\n",
    "        channels = []\n",
    "        if 'channel_0' in feature_name:\n",
    "            channels.append('channel_0')\n",
    "        if 'channel_1' in feature_name:\n",
    "            channels.append('channel_1')\n",
    "        if 'channel_2' in feature_name:\n",
    "            channels.append('channel_2')\n",
    "        if 'channel_3' in feature_name:\n",
    "            channels.append('channel_3')\n",
    "\n",
    "        # If multiple channels are found, join them with a '+'\n",
    "        if channels:\n",
    "            channel = ' + '.join(channels)\n",
    "        else:\n",
    "            channel = 'morphology'  # Use 'morphology' if no channel identifier is found\n",
    "\n",
    "        return (compartment, channel)\n",
    "\n",
    "    def read_and_preprocess_data(settings):\n",
    "\n",
    "        df, _ = _read_and_merge_data(\n",
    "            locs=[settings['src']+'/measurements/measurements.db'], \n",
    "            tables=settings['tables'], \n",
    "            verbose=True, \n",
    "            nuclei_limit=settings['nuclei_limit'], \n",
    "            pathogen_limit=settings['pathogen_limit']\n",
    "        )\n",
    "                \n",
    "        df, _dict = generate_comparison_columns(df, compartments=['cell', 'nucleus', 'pathogen', 'cytoplasm'])\n",
    "        print(f\"Expanded dataframe to {len(df.columns)} columns with relative features\")\n",
    "        scores_df = pd.read_csv(settings['scores'])\n",
    "\n",
    "        # Clean and align columns for merging\n",
    "        df['object_label'] = df['object_label'].str.replace('o', '')\n",
    "\n",
    "        if 'row_name' not in scores_df.columns:\n",
    "            scores_df['row_name'] = scores_df['row']\n",
    "\n",
    "        if 'column_name' not in scores_df.columns:\n",
    "            scores_df['column_name'] = scores_df['col']\n",
    "\n",
    "        if 'object_label' not in scores_df.columns:\n",
    "            scores_df['object_label'] = scores_df['object']\n",
    "\n",
    "        # Remove the 'o' prefix from 'object_label' in df, ensuring it is a string type\n",
    "        df['object_label'] = df['object_label'].str.replace('o', '').astype(str)\n",
    "\n",
    "        # Ensure 'object_label' in scores_df is also a string\n",
    "        scores_df['object_label'] = scores_df['object'].astype(str)\n",
    "\n",
    "        # Ensure all join columns have the same data type in both DataFrames\n",
    "        df[['plate', 'row_name', 'column_name', 'field', 'object_label']] = df[['plate', 'row_name', 'column_name', 'field', 'object_label']].astype(str)\n",
    "        scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label']] = scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label']].astype(str)\n",
    "\n",
    "        # Select only the necessary columns from scores_df for merging\n",
    "        scores_df = scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label', settings['score_column']]]\n",
    "\n",
    "        # Now merge DataFrames\n",
    "        merged_df = pd.merge(df, scores_df, on=['plate', 'row_name', 'column_name', 'field', 'object_label'], how='inner')\n",
    "\n",
    "        # Separate numerical features and the score column\n",
    "        X = merged_df.select_dtypes(include='number').drop(columns=[settings['score_column']])\n",
    "        y = merged_df[settings['score_column']]\n",
    "\n",
    "        return X, y, merged_df\n",
    "    \n",
    "    X, y, merged_df = read_and_preprocess_data(settings)\n",
    "    \n",
    "    output = {}\n",
    "    \n",
    "    # Step 1: Feature Importance using Random Forest\n",
    "    if settings['feature_importance'] or settings['feature_importance']:\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=settings['n_jobs'])\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        if settings['feature_importance']:\n",
    "            print(f\"Feature Importance ...\")\n",
    "            feature_importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "            feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "            top_feature_importance_df = feature_importance_df.head(settings['top_features'])\n",
    "\n",
    "            # Plot Feature Importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(top_feature_importance_df['feature'], top_feature_importance_df['importance'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f\"Top {settings['top_features']} Features - Feature Importance\")\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.show()\n",
    "        \n",
    "        if settings['save']:\n",
    "            _results_to_csv(feature_importance_df, filename='feature_importance.csv')\n",
    "            \n",
    "        output['feature_importance'] = feature_importance_df\n",
    "        fi_compartment_df = group_feature_class(feature_importance_df, feature_groups=settings['tables'], name='compartment', include_all=settings['include_all'])\n",
    "        fi_channel_df = group_feature_class(feature_importance_df, feature_groups=settings['channels'], name='channel', include_all=settings['include_all'])\n",
    "        \n",
    "        output['feature_importance_compartment'] = fi_compartment_df\n",
    "        output['feature_importance_channel'] = fi_channel_df\n",
    "    \n",
    "    # Step 2: Permutation Importance\n",
    "    if settings['permutation_importance']:\n",
    "        print(f\"Permutation Importance ...\")\n",
    "        perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=settings['n_jobs'])\n",
    "        perm_importance_df = pd.DataFrame({'feature': X.columns, 'importance': perm_importance.importances_mean})\n",
    "        perm_importance_df = perm_importance_df.sort_values(by='importance', ascending=False)\n",
    "        top_perm_importance_df = perm_importance_df.head(settings['top_features'])\n",
    "\n",
    "        # Plot Permutation Importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(top_perm_importance_df['feature'], top_perm_importance_df['importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f\"Top {settings['top_features']} Features - Permutation Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "        \n",
    "        if settings['save']:\n",
    "            _results_to_csv(perm_importance_df, filename='permutation_importance.csv')\n",
    "            \n",
    "        output['permutation_importance'] = perm_importance_df\n",
    "    \n",
    "    # Step 3: SHAP Analysis\n",
    "    if settings['shap']:\n",
    "        print(f\"SHAP Analysis ...\")\n",
    "\n",
    "        # Select top N features based on Random Forest importance and fit the model on these features only\n",
    "        top_features = feature_importance_df.head(settings['top_features'])['feature']\n",
    "        X_top = X[top_features]\n",
    "\n",
    "        # Refit the model on this subset of features\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=settings['n_jobs'])\n",
    "        model.fit(X_top, y)\n",
    "\n",
    "        # Sample a smaller subset of rows to speed up SHAP\n",
    "        if settings['shap_sample']:\n",
    "            sample = int(len(X_top) / 100)\n",
    "            X_sample = X_top.sample(min(sample, len(X_top)), random_state=42)\n",
    "        else:\n",
    "            X_sample = X_top\n",
    "\n",
    "        # Initialize SHAP explainer with the same subset of features\n",
    "        explainer = shap.Explainer(model.predict, X_sample)\n",
    "        shap_values = explainer(X_sample, max_evals=1500)\n",
    "\n",
    "        # Plot SHAP summary for the selected sample and top features\n",
    "        shap.summary_plot(shap_values, X_sample, max_display=settings['top_features'])\n",
    "\n",
    "        # Convert SHAP values to a DataFrame for easier manipulation\n",
    "        shap_df = pd.DataFrame(shap_values.values, columns=X_sample.columns)\n",
    "        \n",
    "        # Apply the function to create MultiIndex columns with compartment and channel\n",
    "        shap_df.columns = pd.MultiIndex.from_tuples(\n",
    "            [extract_compartment_channel(feat) for feat in shap_df.columns], \n",
    "            names=['compartment', 'channel']\n",
    "        )\n",
    "        \n",
    "        # Aggregate SHAP values by compartment and channel\n",
    "        compartment_mean = shap_df.abs().groupby(level='compartment', axis=1).mean().mean(axis=0)\n",
    "        channel_mean = shap_df.abs().groupby(level='channel', axis=1).mean().mean(axis=0)\n",
    "\n",
    "        # Calculate combined importance for each pair of compartments and channels\n",
    "        combined_compartment = {}\n",
    "        for i, comp1 in enumerate(compartment_mean.index):\n",
    "            for comp2 in compartment_mean.index[i+1:]:\n",
    "                combined_compartment[f\"{comp1} + {comp2}\"] = shap_df.loc[:, (comp1, slice(None))].abs().mean().mean() + \\\n",
    "                                                              shap_df.loc[:, (comp2, slice(None))].abs().mean().mean()\n",
    "        \n",
    "        combined_channel = {}\n",
    "        for i, chan1 in enumerate(channel_mean.index):\n",
    "            for chan2 in channel_mean.index[i+1:]:\n",
    "                combined_channel[f\"{chan1} + {chan2}\"] = shap_df.loc[:, (slice(None), chan1)].abs().mean().mean() + \\\n",
    "                                                          shap_df.loc[:, (slice(None), chan2)].abs().mean().mean()\n",
    "\n",
    "        # Prepare values and labels for radar charts\n",
    "        all_compartment_importance = list(compartment_mean.values) + list(combined_compartment.values())\n",
    "        all_compartment_labels = list(compartment_mean.index) + list(combined_compartment.keys())\n",
    "\n",
    "        all_channel_importance = list(channel_mean.values) + list(combined_channel.values())\n",
    "        all_channel_labels = list(channel_mean.index) + list(combined_channel.keys())\n",
    "\n",
    "        # Create radar plots for compartments and channels\n",
    "        #create_extended_radar_plot(all_compartment_importance, all_compartment_labels, \"SHAP Importance by Compartment (Individual and Combined)\")\n",
    "        #create_extended_radar_plot(all_channel_importance, all_channel_labels, \"SHAP Importance by Channel (Individual and Combined)\")\n",
    "        \n",
    "        output['shap'] = shap_df\n",
    "        \n",
    "    if settings['save']:\n",
    "        dst = os.path.join(settings['src'], 'results')\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "        for key, df in output.items(): \n",
    "            save_path = os.path.join(dst, f\"{key}.csv\")\n",
    "            df.to_csv(save_path)\n",
    "            print(f\"Saved {save_path}\")\n",
    "        \n",
    "    return output\n",
    "\n",
    "settings = {'src':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1',\n",
    "           'scores':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/241007_241003_tsg101_screen_plate1_maxvit_t_epoch_99_acc_99.0_channels_rgb_result.csv',\n",
    "           'tables':['cell', 'nucleus', 'pathogen','cytoplasm'],\n",
    "           'channels':['channel_0', 'channel_1', 'channel_2', 'channel_3'],\n",
    "           'include_all':False,\n",
    "           'feature_importance':True,\n",
    "           'permutation_importance':False,\n",
    "           'shap':False,\n",
    "           'save':False,\n",
    "           'nuclei_limit':1000,\n",
    "           'pathogen_limit':1000,\n",
    "           'top_features':30,\n",
    "           'shap_sample':True,\n",
    "           'n_jobs':-1,\n",
    "           'shap_approximate':True,\n",
    "           'score_column':'cv_predictions'}\n",
    "\n",
    "output = interperate_vision_model(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85662a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacr.plot import spacrGraph\n",
    "%matplotlib inline\n",
    "\n",
    "spacr_graph = spacrGraph(\n",
    "    df=output['feature_importance_channel'],                                     \n",
    "    grouping_column='channel',                         \n",
    "    data_column='channel_importance_sum',         \n",
    "    graph_type='jitter_bar',          \n",
    "    graph_name='test',          \n",
    "    summary_func='mean',                         \n",
    "    colors=None,                                \n",
    "    output_dir=None,                              \n",
    "    save=None,                       \n",
    "    y_lim=None,                     \n",
    "    error_bar_type='std',                       \n",
    "    representation='object',\n",
    "    theme='pastel',                    \n",
    ")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "spacr_graph.create_plot()\n",
    "\n",
    "# Get the figure object if needed\n",
    "fig = spacr_graph.get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d5bfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacr.plot import spacrGraph\n",
    "%matplotlib inline\n",
    "\n",
    "spacr_graph = spacrGraph(\n",
    "    df=output['feature_importance_compartment'],                                     \n",
    "    grouping_column='compartment',                         \n",
    "    data_column='compartment_importance_sum',         \n",
    "    graph_type='jitter_bar',          \n",
    "    graph_name='test',          \n",
    "    summary_func='mean',                         \n",
    "    colors=None,                                \n",
    "    output_dir=None,                              \n",
    "    save=None,                       \n",
    "    y_lim=None,                     \n",
    "    error_bar_type='std',                       \n",
    "    representation='object',\n",
    "    theme='pastel',                    \n",
    ")\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "spacr_graph.create_plot()\n",
    "\n",
    "# Get the figure object if needed\n",
    "fig = spacr_graph.get_figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = output['feature_importance']\n",
    "\n",
    "# Define compartments using regex patterns\n",
    "df['compartment'] = df['feature'].apply(lambda x: 'cell' if 'cell' in x else 'nucleus' if 'nucleus' in x else 'cytoplasm' if 'cytoplasm' in x else 'pathogen' if 'pathogen' in x else None)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075f589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4399cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08874e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5c3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0629f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45e10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73593c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "settings = {'src':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1',\n",
    "           'scores':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/241007_241003_tsg101_screen_plate1_maxvit_t_epoch_99_acc_99.0_channels_rgb_result.csv',\n",
    "           'tables':['cell', 'nucleus', 'pathogen','cytoplasm'],\n",
    "           'feature_importance':True,\n",
    "           'permutation_importance':False,\n",
    "           'shap':True,\n",
    "           'save':False,\n",
    "           'nuclei_limit':1000,\n",
    "           'pathogen_limit':1000,\n",
    "           'top_features':30,\n",
    "           'shap_sample':True,\n",
    "           'n_jobs':-1,\n",
    "           'shap_approximate':True,\n",
    "           'score_column':'cv_predictions'}\n",
    "\n",
    "merged_df = interperate_vision_model(settings)\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in merged_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23da9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "%matplotlib inline\n",
    "\n",
    "def interperate_vision_model(settings={}):\n",
    "    from spacr.io import _read_and_merge_data, _results_to_csv\n",
    "    from spacr.plot import plot_image_mask_overlay, _plot_controls, _plot_recruitment\n",
    "    from spacr.utils import _object_filter, annotate_conditions, _calculate_recruitment, _group_by_well, save_settings\n",
    "    from spacr.settings import get_analyze_recruitment_default_settings\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    import shap\n",
    "    \n",
    "    # Function to create radar plot for individual and combined values\n",
    "    def create_extended_radar_plot(values, labels, title):\n",
    "        values = list(values) + [values[0]]  # Close the loop for radar chart\n",
    "        angles = [n / float(len(labels)) * 2 * pi for n in range(len(labels))]\n",
    "        angles += angles[:1]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid')\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(labels, fontsize=10, rotation=45, ha='right')\n",
    "        plt.title(title, pad=20)\n",
    "        plt.show()\n",
    "\n",
    "    def extract_compartment_channel(feature_name):\n",
    "        # Identify compartment as the first part before an underscore\n",
    "        compartment = feature_name.split('_')[0]\n",
    "        \n",
    "        if compartment == 'cells':\n",
    "            compartment = 'cell'\n",
    "\n",
    "        # Identify channels based on substring presence\n",
    "        channels = []\n",
    "        if 'channel_0' in feature_name:\n",
    "            channels.append('channel_0')\n",
    "        if 'channel_1' in feature_name:\n",
    "            channels.append('channel_1')\n",
    "        if 'channel_2' in feature_name:\n",
    "            channels.append('channel_2')\n",
    "        if 'channel_3' in feature_name:\n",
    "            channels.append('channel_3')\n",
    "\n",
    "        # If multiple channels are found, join them with a '+'\n",
    "        if channels:\n",
    "            channel = ' + '.join(channels)\n",
    "        else:\n",
    "            channel = 'morphology'  # Use 'morphology' if no channel identifier is found\n",
    "\n",
    "        return (compartment, channel)\n",
    "\n",
    "    def read_and_preprocess_data(settings):\n",
    "\n",
    "        df, _ = _read_and_merge_data(\n",
    "            locs=[settings['src']+'/measurements/measurements.db'], \n",
    "            tables=settings['tables'], \n",
    "            verbose=True, \n",
    "            nuclei_limit=settings['nuclei_limit'], \n",
    "            pathogen_limit=settings['pathogen_limit']\n",
    "        )\n",
    "\n",
    "        scores_df = pd.read_csv(settings['scores'])\n",
    "\n",
    "        # Clean and align columns for merging\n",
    "        df['object_label'] = df['object_label'].str.replace('o', '')\n",
    "\n",
    "        if 'row_name' not in scores_df.columns:\n",
    "            scores_df['row_name'] = scores_df['row']\n",
    "\n",
    "        if 'column_name' not in scores_df.columns:\n",
    "            scores_df['column_name'] = scores_df['col']\n",
    "\n",
    "        if 'object_label' not in scores_df.columns:\n",
    "            scores_df['object_label'] = scores_df['object']\n",
    "\n",
    "        # Remove the 'o' prefix from 'object_label' in df, ensuring it is a string type\n",
    "        df['object_label'] = df['object_label'].str.replace('o', '').astype(str)\n",
    "\n",
    "        # Ensure 'object_label' in scores_df is also a string\n",
    "        scores_df['object_label'] = scores_df['object'].astype(str)\n",
    "\n",
    "        # Ensure all join columns have the same data type in both DataFrames\n",
    "        df[['plate', 'row_name', 'column_name', 'field', 'object_label']] = df[['plate', 'row_name', 'column_name', 'field', 'object_label']].astype(str)\n",
    "        scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label']] = scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label']].astype(str)\n",
    "\n",
    "        # Select only the necessary columns from scores_df for merging\n",
    "        scores_df = scores_df[['plate', 'row_name', 'column_name', 'field', 'object_label', settings['score_column']]]\n",
    "\n",
    "        # Now merge DataFrames\n",
    "        merged_df = pd.merge(df, scores_df, on=['plate', 'row_name', 'column_name', 'field', 'object_label'], how='inner')\n",
    "\n",
    "        # Separate numerical features and the score column\n",
    "        X = merged_df.select_dtypes(include='number').drop(columns=[settings['score_column']])\n",
    "        y = merged_df[settings['score_column']]\n",
    "\n",
    "        return X, y, merged_df\n",
    "    \n",
    "    X, y, merged_df = read_and_preprocess_data(settings)\n",
    "    \n",
    "    # Step 1: Feature Importance using Random Forest\n",
    "    if settings['feature_importance'] or settings['feature_importance']:\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=settings['n_jobs'])\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        if settings['feature_importance']:\n",
    "            print(f\"Feature Importance ...\")\n",
    "            feature_importances = model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importances})\n",
    "            feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "            top_feature_importance_df = feature_importance_df.head(settings['top_features'])\n",
    "\n",
    "            # Plot Feature Importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(top_feature_importance_df['feature'], top_feature_importance_df['importance'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f\"Top {settings['top_features']} Features - Feature Importance\")\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.show()\n",
    "        \n",
    "        if settings['save']:\n",
    "            _results_to_csv(feature_importance_df, filename='feature_importance.csv')\n",
    "    \n",
    "    # Step 2: Permutation Importance\n",
    "    if settings['permutation_importance']:\n",
    "        print(f\"Permutation Importance ...\")\n",
    "        perm_importance = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=settings['n_jobs'])\n",
    "        perm_importance_df = pd.DataFrame({'feature': X.columns, 'importance': perm_importance.importances_mean})\n",
    "        perm_importance_df = perm_importance_df.sort_values(by='importance', ascending=False)\n",
    "        top_perm_importance_df = perm_importance_df.head(settings['top_features'])\n",
    "\n",
    "        # Plot Permutation Importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(top_perm_importance_df['feature'], top_perm_importance_df['importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f\"Top {settings['top_features']} Features - Permutation Importance\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()\n",
    "        \n",
    "        if settings['save']:\n",
    "            _results_to_csv(perm_importance_df, filename='permutation_importance.csv')\n",
    "    \n",
    "    # Step 3: SHAP Analysis\n",
    "    if settings['shap']:\n",
    "        print(f\"SHAP Analysis ...\")\n",
    "\n",
    "        # Select top N features based on Random Forest importance and fit the model on these features only\n",
    "        top_features = feature_importance_df.head(settings['top_features'])['feature']\n",
    "        X_top = X[top_features]\n",
    "\n",
    "        # Refit the model on this subset of features\n",
    "        model = RandomForestClassifier(random_state=42, n_jobs=settings['n_jobs'])\n",
    "        model.fit(X_top, y)\n",
    "\n",
    "        # Sample a smaller subset of rows to speed up SHAP\n",
    "        if settings['shap_sample']:\n",
    "            sample = int(len(X_top) / 100)\n",
    "            X_sample = X_top.sample(min(sample, len(X_top)), random_state=42)\n",
    "        else:\n",
    "            X_sample = X_top\n",
    "\n",
    "        # Initialize SHAP explainer with the same subset of features\n",
    "        explainer = shap.Explainer(model.predict, X_sample)\n",
    "        shap_values = explainer(X_sample, max_evals=1500)\n",
    "\n",
    "        # Plot SHAP summary for the selected sample and top features\n",
    "        shap.summary_plot(shap_values, X_sample, max_display=settings['top_features'])\n",
    "\n",
    "        # Convert SHAP values to a DataFrame for easier manipulation\n",
    "        shap_df = pd.DataFrame(shap_values.values, columns=X_sample.columns)\n",
    "        \n",
    "        # Apply the function to create MultiIndex columns with compartment and channel\n",
    "        shap_df.columns = pd.MultiIndex.from_tuples(\n",
    "            [extract_compartment_channel(feat) for feat in shap_df.columns], \n",
    "            names=['compartment', 'channel']\n",
    "        )\n",
    "        \n",
    "        # Aggregate SHAP values by compartment and channel\n",
    "        compartment_mean = shap_df.abs().groupby(level='compartment', axis=1).mean().mean(axis=0)\n",
    "        channel_mean = shap_df.abs().groupby(level='channel', axis=1).mean().mean(axis=0)\n",
    "\n",
    "        # Calculate combined importance for each pair of compartments and channels\n",
    "        combined_compartment = {}\n",
    "        for i, comp1 in enumerate(compartment_mean.index):\n",
    "            for comp2 in compartment_mean.index[i+1:]:\n",
    "                combined_compartment[f\"{comp1} + {comp2}\"] = shap_df.loc[:, (comp1, slice(None))].abs().mean().mean() + \\\n",
    "                                                              shap_df.loc[:, (comp2, slice(None))].abs().mean().mean()\n",
    "        \n",
    "        combined_channel = {}\n",
    "        for i, chan1 in enumerate(channel_mean.index):\n",
    "            for chan2 in channel_mean.index[i+1:]:\n",
    "                combined_channel[f\"{chan1} + {chan2}\"] = shap_df.loc[:, (slice(None), chan1)].abs().mean().mean() + \\\n",
    "                                                          shap_df.loc[:, (slice(None), chan2)].abs().mean().mean()\n",
    "\n",
    "        # Prepare values and labels for radar charts\n",
    "        all_compartment_importance = list(compartment_mean.values) + list(combined_compartment.values())\n",
    "        all_compartment_labels = list(compartment_mean.index) + list(combined_compartment.keys())\n",
    "\n",
    "        all_channel_importance = list(channel_mean.values) + list(combined_channel.values())\n",
    "        all_channel_labels = list(channel_mean.index) + list(combined_channel.keys())\n",
    "\n",
    "        # Create radar plots for compartments and channels\n",
    "        create_extended_radar_plot(all_compartment_importance, all_compartment_labels, \"SHAP Importance by Compartment (Individual and Combined)\")\n",
    "        create_extended_radar_plot(all_channel_importance, all_channel_labels, \"SHAP Importance by Channel (Individual and Combined)\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "settings = {'src':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1',\n",
    "           'scores':'/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/241007_241003_tsg101_screen_plate1_maxvit_t_epoch_99_acc_99.0_channels_rgb_result.csv',\n",
    "           'tables':['cell', 'nucleus', 'pathogen','cytoplasm'],\n",
    "           'feature_importance':True,\n",
    "           'permutation_importance':False,\n",
    "           'shap':True,\n",
    "           'save':False,\n",
    "           'nuclei_limit':1000,\n",
    "           'pathogen_limit':1000,\n",
    "           'top_features':30,\n",
    "           'shap_sample':True,\n",
    "           'n_jobs':-1,\n",
    "           'shap_approximate':True,\n",
    "           'score_column':'cv_predictions'}\n",
    "\n",
    "merged_df = interperate_vision_model(settings)\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9129b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacr",
   "language": "python",
   "name": "spacr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
