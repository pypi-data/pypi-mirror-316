model:
  embedding_type: tp
  embedding_dim: 128
  max_len: 100
  num_layers: 4
  n_heads: 8
  tokenizer_path: output/ctle/tokenizer.pkl
  hidden_size: 512

data:
  tokenizer_path: output/ctle/tokenizer.pkl
  train_parquet_path: output/ctle/train_ds.parquet
  val_parquet_path: output/ctle/test_ds.parquet
  train_batch_size: 128
  val_batch_size: 128
  num_cpus: -1
  mask_prob: 0.2

seed_everything: 42

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-3
    weight_decay: 1e-5

trainer:
  default_root_dir: lightning_logs/ctle
  max_epochs: 1000
  precision: 32-true
  gradient_clip_val: 0.1
  plugins:
    - class_path: lightning.pytorch.plugins.io.AsyncCheckpointIO
  callbacks:
    - class_path: lightning.pytorch.callbacks.EarlyStopping
      init_args:
        monitor: val_loss
        mode: min
        patience: 5
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 3
        monitor: val_loss
        mode: min
        filename: model-{epoch:03d}-{val_loss:.6f}