version: 1.1
gai_url: "http://localhost:12033"
logging:
    level: "INFO"
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    datefmt: "%Y-%m-%d %H:%M:%S"
    filename: ""
    filemode: "a"
    stream: "stdout"
    loggers:
        gai.ttt: "DEBUG"
        gai.common.http_utils: "DEBUG"
clients:
    ttt:
        default: "ttt-llamacpp-dolphin"
        configs:
            ttt-llamacpp-dolphin:
                type: "ttt"
                engine: "llamacpp"
                model: "dolphin"
                url: "http://localhost:12031/gen/v1/chat/completions"
            ttt-openai-gpt4:
                type: "ttt"
                engine: "openai"
                model: "gpt4"
                env:
                    OPENAI_API_KEY: ${OPENAI_API_KEY}
    rag:
        default: "rag-sentencepiece-instructor"
        configs:
            rag-sentencepiece-instructor:
                type: "rag"
                engine: "sentencepiece"
                model: "instructor"
                url: "http://localhost:12036/gen/v1/rag"
                extra:
                    {
                        "ws_url": "ws://localhost:12036/gen/v1/rag/index-file/ws",
                    }
    tti:
        default: "tti-sd1.5-runwayml"
        configs:
            tti-sd1.5-runwayml:
                type: "tti"
                engine: "sd1.5"
                model: "runwayml"
                url: "http://localhost:12035/sdapi/v1/txt2img"
    tts:
        default: "tts-coqui-xttsv2"
        configs:
            tts-coqui-xttsv2:
                type: "tts"
                engine: "coqui"
                model: "xttsv2"
                url: "http://localhost:12032/gen/v1/audio/speech"
    stt:
        default: "stt-whisperv3-huggingface"
        configs:
            stt-whisperv3-huggingface:
                type: "stt"
                engine: "huggingface"
                model: "whisperv3"
                url: "http://localhost:12033/gen/v1/audio/transcriptions"
    itt:
        default: "itt-vicuna7b-llavav1.6"
        configs:
            itt-vicuna7b-llavav1.6:
                type: "itt"
                engine: "vicuna7b"
                model: "llavav1.6"
                url: "http://localhost:12034/gen/v1/vision/completions"
    ttc:
        default: "ttc-exllamav2-deepseek"
        configs:
            ttc-exllamav2-deepseek:
                type: "ttc"
                engine: "exllamav2"
                model: "deepseek"

generators:
    rag:
        default: "rag-sentencepiece-instructor"
        configs:
            rag-sentencepiece-instructor:
                type: "rag"
                engine: "sentencepiece"
                model: "instructor"
                model_path: "models/instructor-large"
                chromadb:
                    path: "rag/chromadb"
                    n_results: 3
                sqlite:
                    path: "rag/gai-rag.db"
                device: "cuda"
                chunks:
                    size: 1000
                    overlap: 100
                    path: "chunks"
                module:
                    name: "gai.rag.server.gai_rag"
                    class: "RAG"
    tts:
        default: "tts-coqui-xttsv2"
        configs:
            tts-coqui-xttsv2:
                type: "tts"
                engine: "coqui"
                model: "xttsv2"
                model_path: "models/xttsv2-coqui/tts/tts_models--multilingual--multi-dataset--xtts_v2"
                max_seq_len: 128
                hyperparameters:
                    stopping_words: []
                module:
                    name: "gai.tts.server.gai_xtts"
                    class: "GaiXTTS"
    stt:
        default: "stt-whisperv3-huggingface"
        configs:
            stt-whisperv3-huggingface:
                type: "stt"
                engine: "huggingface"
                model: "whisperv3"
                model_path: "models/whisper-large-v3"
                max_seq_len: 128
                hyperparameters:
                    stopping_words: []
                    chunk_length_s: 30
                    batch_size: 16
                    max_new_tokens: 128
                module:
                    name: "gai.stt.server.gai_stt"
                    class: "GaiSTT"
    itt:
        default: "itt-vicuna7b-llavav1.6"
        configs:
            itt-vicuna7b-llavav1.6:
                type: "itt"
                engine: "vicuna7b"
                model: "llavav1.6"
                model_path: "models/llava-v1.6-vicuna-7b"
                max_seq_len: 512
                hyperparameters:
                    temperature: 0.2
                    max_new_tokens: 512
                    stopping_words: []
                extra:
                    load_8bit: false
                    load_4bit: true
                    image_aspect_ratio: "pad"
                module:
                    name: "gai.itt.server.Llava_ITT"
                    class: "Llava_ITT"
    ttc:
        default: "ttc-exllamav2-deepseek"
        configs:
            ttc-exllamav2-deepseek:
                type: "ttc"
                engine: "exllamav2"
                model: "deepseek"
                model_path: "models/exllamav2-deepseek"
                model_basename: "model"
                max_seq_len: 8192
                prompt_format: "mistral"
                hyperparameters:
                    temperature: 0.85
                    top_p: 0.8
                    top_k: 50
                    max_tokens: 2000
                    tool_choice: "auto"
                    max_retries: 5
                    stop_conditions: ["<|im_end|>", "</s>", "[/INST]"]
                extra:
                    no_flash_attn: true
                    seed: null
                    decode_special_tokens: false
                module:
                    name: "gai.ttc.server.gai_exllamav2"
                    class: "GaiExLlamav2"
