"""
Reusable Python utilities (top-level API).
"""
from __future__ import annotations

import atexit
import csv
import ctypes
import inspect
import json
import locale
import logging
import logging.config
import os
import re
import socket
import ssl
import sys
import threading
import unicodedata
from argparse import ArgumentParser, RawTextHelpFormatter, _SubParsersAction
from configparser import _UNSET, ConfigParser, RawConfigParser
from contextlib import contextmanager, nullcontext
from copy import copy
from datetime import date, datetime, time, timedelta, timezone, tzinfo
from decimal import Decimal
from enum import Enum, Flag
from http.client import HTTPResponse
from importlib import import_module
from io import (BufferedIOBase, IOBase, StringIO, TextIOWrapper,
                UnsupportedOperation)
from ipaddress import AddressValueError, IPv4Address, IPv6Address, ip_address
from pathlib import Path
from pkgutil import iter_modules
from pprint import pprint
from sqlite3 import Connection
from subprocess import CompletedProcess, SubprocessError
from textwrap import dedent
from threading import Thread
from traceback import format_exception
from types import FunctionType, GeneratorType, ModuleType, TracebackType
from typing import (TYPE_CHECKING, Any, Callable, Generic, Iterable, Iterator,
                    MutableMapping, Sequence, TypeVar, Union, overload)
from urllib.error import HTTPError, URLError
from urllib.parse import quote, urlencode, urlparse, urlunparse
from urllib.request import Request, urlopen
from uuid import UUID

# Literal: was introduced in Python 3.8
try:
    from typing import Literal, get_args, get_origin
except ImportError:
    from typing_extensions import Literal, get_args, get_origin

# GenericAlias: was introducted in Python 3.9
try:
    from types import GenericAlias
except ImportError:
    GenericAlias = None

# ----- Optional dependencies -----
# ZoneInfo: introduced in Python 3.9
try:
    from zoneinfo import ZoneInfo
except ImportError:
    ZoneInfo = None

# tzdata: used to parse timezones from strings through ZoneInfo on Windows (Windows does not maintain a database of timezones)
try:
    import tzdata
except ImportError:
    tzdata = None

# tzlocal: used to parse timezones from strings on Windows (Windows does not maintain a database of timezones and `tzdata` only is not enough)
try:
    import tzlocal
except ImportError:
    tzlocal = None

# pytz: used to parse timezones on Python < 3.9 (no ZoneInfo available)
try:
    import pytz    
except ImportError:
    pytz = None

if TYPE_CHECKING:
    from zut.db import DbAdapter

__prog__ = 'zut'

# version: generated by setuptools_scm during build
try:
    from zut._version import __version__, __version_tuple__
except ImportError:
    __version__ = None
    __version_tuple__ = None

ZUT_ROOT = Path(__file__).resolve().parent

_logger = logging.getLogger(__name__)

T = TypeVar('T')


#region Text

def slugify(value: str, *, separator: str = '-', keep: str = '_', as_separator: str = None, strip_separator: bool = True, strip_keep: bool = True, if_none: str = 'none') -> str:
    """ 
    Generate a slug.

    Difference between `keep` and `as_separator`
    - `keep`: these characters are kept as is in the resulting slug
    - `as_separator`: these characters are transformed to a separator before the operation

    Identical to `django.utils.text.slugify` if no options are given.
    """
    if value is None:
        return if_none
    
    separator = separator if separator is not None else ''
    keep = keep if keep is not None else ''

    if as_separator:
        value = re.sub(f"[{re.escape(as_separator)}]", separator, value)

    # Normalize the string: replace diacritics by standard characters, lower the string, etc
    value = str(value)
    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
    value = value.lower()

    # Remove special characters
    remove_sequence = r'^a-zA-Z0-9\s' + re.escape(separator) + re.escape(keep)
    value = re.sub(f"[{remove_sequence}]", "", value)

    # Replace spaces and successive separators by a single separator
    replace_sequence = r'\s' + re.escape(separator)
    value = re.sub(f"[{replace_sequence}]+", separator, value)
    
    # Strips separator and kept characters
    strip_chars = (separator if strip_separator else '') + (keep if strip_keep else '')
    value = value.strip(strip_chars)

    return value


def slugify_snake(value: str, separator: str = '_', if_none: str = 'none') -> str:
    """
    CamÃ¨lCase => camel_case
    """
    if value is None:
        return if_none
    
    separator = separator if separator is not None else ''
    
    # Normalize the string: replace diacritics by standard characters, etc
    # NOTE: don't lower the string
    value = str(value)
    value = unicodedata.normalize("NFKD", value).encode("ascii", "ignore").decode("ascii")
    
    value = re.sub(r"[^\w\s-]", "", value)
    value = re.sub(r"[-_\s]+", separator, value).strip(separator)
    value = re.sub(r'(.)([A-Z][a-z]+)', f'\\1{separator}\\2', value)
    return re.sub(r'([a-z0-9])([A-Z])', f'\\1{separator}\\2', value).lower()


def skip_utf8_bom(fp: TextIOWrapper|BufferedIOBase, encoding: str|None = None):
    """
    Skip UTF8 byte order mark, if any.
    - `fp`: opened file pointer.
    - `encoding`: if given, do nothing unless encoding is utf-8 or alike.
    """
    if encoding is not None and not encoding in {'utf8', 'utf-8', 'utf-8-sig'}:
        return False

    try:
        data = fp.read(1)
    except UnsupportedOperation: # e.g. empty file
        return False
    
    if isinstance(data, str): # text mode
        if len(data) >= 1 and data[0] == UTF8_BOM:
            return True
        
    elif isinstance(data, bytes): # binary mode
        if len(data) >= 1 and data[0] == UTF8_BOM_BINARY[0]:
            data += fp.read(2)
            if data[0:3] == UTF8_BOM_BINARY:
                return True
    
    fp.seek(0)
    return False


UTF8_BOM = '\ufeff'
UTF8_BOM_BINARY = UTF8_BOM.encode('utf-8')


def parse_uuid_or_none(uuid: str, version: int = None):
    """
    Check if the given string is a valid UUID, if yes, return the `UUID` object, otherwise return `None`.
    """    
    try:
        return UUID(uuid, version=version)
    except ValueError:
        return None


def parse_ipaddress_or_none(address: str, version: int = None):
    """
    Check if the given string is a valid IP address, if yes, return the `IPv4Address` or `IPv6Address` object, otherwise return `None`.
    """    
    try:
        ip = ip_address(address)
    except ValueError:
        return False
    
    if version is None or ip.version == version:
        return ip
    else:
        return None


class Filter:
    def __init__(self, spec: str|re.Pattern, *, normalize: bool = False):
        self.normalize = normalize

        if isinstance(spec, re.Pattern):
            self.spec = spec

        elif isinstance(spec, str) and spec.startswith('^'):
            m = re.match(r'^(.*\$)(A|I|L|U|M|S|X)+$', spec, re.IGNORECASE)
            if m:
                pattern = m[1]
                flags = re.NOFLAG
                for letter in m[2]:
                    flags |= re.RegexFlag[letter.upper()]
            else:
                pattern = spec
                flags = re.NOFLAG

            self.spec = re.compile(pattern, flags)

        elif isinstance(spec, str):
            if self.normalize:
                spec = self.normalize_spec(spec)

            if '*' in spec:
                name_parts = spec.split('*')
                pattern_parts = [re.escape(name_part) for name_part in name_parts]
                pattern = r'^' + r'.*'.join(pattern_parts) + r'$'
                self.spec = re.compile(pattern)
            else:
                self.spec = spec

        else:
            raise TypeError(f"Filter spec must be a string or regex pattern, got {type(spec).__name__}")
       

    def __repr__(self) -> str:
        return self.spec.pattern if isinstance(self.spec, re.Pattern) else self.spec


    def matches(self, value: str, is_normalized: bool = False):
        if value is None:
            value = ""
        elif not isinstance(value, str):
            value = str(value)

        if self.normalize and not is_normalized:
            value = self.normalize_value(value)

        if isinstance(self.spec, re.Pattern):
            if self.spec.match(value):
                return True
            
        elif self.spec == value:
            return True


    @classmethod
    def normalize_spec(cls, spec: str):
        return slugify(spec, separator=None, keep='*', strip_keep=False, if_none=None)
    
    
    @classmethod
    def normalize_value(cls, value: str):
        return slugify(value, separator=None, keep=None, if_none=None)


class Filters:
    def __init__(self, specs: list[str|re.Pattern]|str|re.Pattern, *, normalize: bool = False):
        self.filters: list[Filter] = []

        if specs:
            if isinstance(specs, (str,re.Pattern)):
                specs = [specs]

            for spec in specs:
                self.filters.append(Filter(spec, normalize=normalize))


    def __len__(self):
        return len(self.filters)


    def matches(self, value: str, if_no_filter: bool = False):
        if not self.filters:
            return if_no_filter
        
        if value is None:
            value = ""
        elif not isinstance(value, str):
            value = str(value)
        
        normalized_value = None    

        for str_filter in self.filters:
            if str_filter.normalize:
                if normalized_value is None:
                    normalized_value = Filter.normalize_value(value)
                if str_filter.matches(normalized_value, is_normalized=True):
                    return True
            else:
                if str_filter.matches(value):
                    return True
                
        return False
    

_to_erase: list[int] = []

def write_live(text: str, newline=False):
    """
    Write text to stdout, keeping track of what was written, so that it can be erased next time.

    Text lines are stripped to terminal column length.
    """
    erase_live()    
    columns, _ = os.get_terminal_size()

    lines = text.split('\n')
    for i, line in enumerate(lines):
        line = line.rstrip()
        
        nb_chars = len(line)
        if nb_chars > columns:
            line = line[:columns-1] + 'â¦'
            nb_chars = columns

        _to_erase.insert(0, nb_chars)

        sys.stdout.write(line)
        if newline or i < len(lines) - 1:
            sys.stdout.write('\n')

    if newline:
        _to_erase.insert(0, 0)
    
    sys.stdout.flush()


def erase_live():
    """
    Erase text written using :func:`write_live`.

    Text lines are stripped to terminal column length.
    """
    if not _to_erase:
        return
    
    for i, nb_chars in enumerate(_to_erase):
        if i == 0:
            sys.stdout.write('\r') # move to beginning of line
        else:
            sys.stdout.write('\033[F') # move to beginning of previous line
        sys.stdout.write(' ' * nb_chars)
    sys.stdout.write('\r')

    _to_erase.clear()


#endregion


#region Numbers

def gigi_bytes(value: int) -> float:
    """
    Convert from bytes to GigiBytes.

    See: https://simple.wikipedia.org/wiki/Gibibyte
    """
    if value is None:
        return None
    return value / 1024**3


def megi_bytes(value: int) -> float:
    """
    Convert from bytes to MegiBytes.

    See: https://simple.wikipedia.org/wiki/Mebibyte
    """
    if value is None:
        return None
    return value / 1024**2


def kili_bytes(value: int) -> float:
    """
    Convert from bytes to KiliBytes.

    See: https://simple.wikipedia.org/wiki/Kibibyte
    """
    if value is None:
        return None
    return value / 1024


def human_bytes(value: int, *, unit: str = 'iB', divider: int = 1024, decimals: int = 1, max_multiple: str = None) -> str:
    """
    Get a human-readable representation of a number of bytes.
    
    :param max_multiple: may be `K`, `M`, `G` or `T`.
    """
    return human_number(value, unit=unit, divider=divider, decimals=decimals, max_multiple=max_multiple)


def human_number(value: int, *, unit: str = '', divider: int = 1000, decimals: int = 1, max_multiple: str = None) -> str:
    """
    Get a human-readable representation of a number.

    :param max_multiple: may be `K`, `M`, `G` or `T`.
    """
    if value is None:
        return None

    suffixes = []

    # Append non-multiple suffix (bytes)
    # (if unit is 'iB' we dont display the 'i' as it makes more sens to display "123 B" than "123 iB")
    if unit:
        suffixes.append(' ' + (unit[1:] if len(unit) >= 2 and unit[0] == 'i' else unit))
    else:
        suffixes.append('')

    # Append multiple suffixes
    for multiple in ['K', 'M', 'G', 'T']:
        suffixes.append(f' {multiple}{unit}')
        if max_multiple and max_multiple.upper() == multiple:
            break

    i = 0
    suffix = suffixes[i]
    divided_value = value

    while divided_value > 1000 and i < len(suffixes) - 1:
        divided_value /= divider
        i += 1
        suffix = suffixes[i]

    # Format value
    formatted_value = ('{0:,.'+('0' if i == 0 else str(decimals))+'f}').format(divided_value)
    
    # Display formatted value with suffix
    return f'{formatted_value}{suffix}'

#endregion


#region Convert & typing

def is_iterable_of(iterable: Iterable, element_type: type|tuple[type]):
    for element in iterable:
        if not isinstance(element, element_type):
            return False        
    return True

@overload
def convert(value: Any, to: type[T], *, nullval = None, if_none = None) -> T:
    ...

def convert(value: Any, to: type[T]|Callable|None, *, nullval = None, if_none = None, accept_localized = True):    
    if value == nullval:
        return if_none
    
    if to is None:
        return value
    
    if GenericAlias and isinstance(to, GenericAlias):
        type_args = get_args(to)
        to = get_origin(to)
        if to == list or to == set:
            if len(type_args) != 1:
                raise ValueError(f"Only one generic type parameter may be used for {to}")
        else:
            raise ValueError(f"Generic {to} not supported")
    elif not isinstance(to, type):
        return to(value)
    
    if isinstance(value, to):
        return value
    
    if value is None:
        return if_none
    
    strvalue = str(value)

    if to == str:
        return strvalue
    
    elif to == bool:
        return parse_bool(strvalue)

    elif to == float or to == Decimal:
        return parse_decimal(strvalue, to, accept_localized=accept_localized)
    
    elif to == date:
        return parse_date(strvalue, accept_localized=accept_localized)
    
    elif to == datetime or to == time:
        return parse_datetime(strvalue, accept_localized=accept_localized)
    
    elif to == list:
        return parse_list(strvalue)
    
    else:
        return to(strvalue)
    

def parse_bool(value: bool|str):
    if value is None or value == '':
        return None
    elif isinstance(value, bool):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")

    lower = value.lower()
    if lower not in RawConfigParser.BOOLEAN_STATES:
        raise ValueError('Not a boolean: %s' % lower)
    return RawConfigParser.BOOLEAN_STATES[lower]


def parse_decimal(value: Decimal|float|str, to = Decimal, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, to):
        return value
    elif isinstance(value, (float,Decimal)):
        return to(value)
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")
    
    if accept_localized and get_lang() == 'fr_FR' and isinstance(value, str):
        if re.match(r'^\-?[0-9]{1,3}(?:[ 0-9]{3})*(?:,[0-9]+)?$', value):
            value = value.replace(' ', '').replace(',', '.')
    return to(value)


def parse_datetime(value: datetime|str, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, datetime):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")

    if accept_localized and get_lang() == 'fr_FR':
        m = re.match(r'^(?P<day>[0-9]{1,2})/(?P<month>[0-9]{1,2})/(?P<year>[0-9]{4}) (?P<hour>[0-9]{1,2}):(?P<minute>[0-9]{1,2})(?::(?P<second>[0-9]{1,2}))$', value)
        if m:
            return datetime(int(m['year']), int(m['month']), int(m['day']), int(m['hour']), int(m['minute']), int(m['second']) if m['second'] else None)

    try:
        return datetime.fromisoformat(value)
    except ValueError:
        return datetime.strptime(value, '%Y-%m-%d %H:%M:%S.%f %z') # format not accepted by fromisoformat (contrary to other non-ISO but still frequent "%Y-%m-%d %H:%M:%S.%f")


def parse_date(value: date|str, *, accept_localized = True):
    if value is None or value == '':
        return None
    elif isinstance(value, datetime):
        return value.date()
    elif isinstance(value, date):
        return value
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")
    
    if accept_localized and get_lang() == 'fr_FR':
        m = re.match(r'^(?P<day>[0-9]{1,2})/(?P<month>[0-9]{1,2})/(?P<year>[0-9]{4})$', value)
        if m:
            return date(int(m['year']), int(m['month']), int(m['day']))
    
    return date.fromisoformat(value)


def parse_list(value: date|str, *, separator: str = None):
    if value is None:
        return None
    elif value == '':
        return []
    elif not isinstance(value, str):
        raise TypeError(f"value: {type(value.__name__)}")
    
    if separator:
        return value.split(separator)
    else:
        return re.split(r'[\W,;|]+', value)


def convert_to_bool(value: Any, *, nullval = None, if_none = None):
    return convert(value, bool, nullval=nullval, if_none=if_none)


def convert_to_int(value: Any, *, nullval = None, if_none = None):
    return convert(value, int, nullval=nullval, if_none=if_none)


def convert_to_decimal(value: Any, *, nullval = None, if_none = None):
    return convert(value, Decimal, nullval=nullval, if_none=if_none)


def convert_to_date(value: Any, *, nullval = None, if_none = None):
    return convert(value, date, nullval=nullval, if_none=if_none)


def convert_to_datetime(value: Any, *, nullval = None, if_none = None):
    return convert(value, datetime, nullval=nullval, if_none=if_none)


def convert_str_args(func: Callable, *args: str):
    if not args:
        return tuple(), dict()
    
    # Determine argument types
    signature = inspect.signature(func)
    var_positional_type = None
    var_keyword_type = None
    parameter_types = {}
    positional_types = []
    for parameter in signature.parameters.values():
        parameter_type = None if parameter.annotation is inspect.Parameter.empty else parameter.annotation
        if parameter.kind == inspect.Parameter.VAR_POSITIONAL:
            var_positional_type = parameter_type
        elif parameter.kind == inspect.Parameter.VAR_KEYWORD:
            var_keyword_type = parameter_type
        else:
            parameter_types[parameter.name] = parameter_type
            if parameter.kind in [inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]:
                positional_types.append(parameter_type)
    
    # Distinguish args and kwargs
    positionnal_args = []
    keyword_args = {}
    for arg in args:
        m = re.match(r'^([a-z0-9_]+)=(.+)$', arg)
        if m:
            keyword_args[m[1]] = m[2]
        else:
            positionnal_args.append(arg)

    # Convert kwargs
    for parameter, value in keyword_args.items():
        if parameter in parameter_types:
            target_type = parameter_types[parameter]
            if target_type:
                keyword_args[parameter] = convert(value, target_type)

        elif var_keyword_type:
            keyword_args[parameter] = convert(value, var_keyword_type)

    # Convert args
    for i, value in enumerate(positionnal_args):
        if i < len(positional_types):
            target_type = positional_types[i]
            if target_type:
                positionnal_args[i] = convert(value, target_type)

        elif var_positional_type:
            positionnal_args[i] = convert(value, var_positional_type)

    return positionnal_args, keyword_args


def is_truthy(value: str):
    if value is None:
        return False
    if value is True:
        return True
    if value is False:
        return False
    if not isinstance(value, str):
        raise TypeError(f"value: {value}")
    return value.lower() not in _falsy_lower_strings


def is_falsy(value: str):
    if value is None:
        return True
    if value is False:
        return True
    if value is True:
        return False
    if not isinstance(value, str):
        raise TypeError(f"value: {value}")
    return value.lower() in _falsy_lower_strings


_falsy_lower_strings  = {key.lower() for key, value in RawConfigParser.BOOLEAN_STATES.items() if not value}
    

#endregion


#region JSON
    
class ExtendedJSONEncoder(json.JSONEncoder):
    """
    Adapted from: django.core.serializers.json.DjangoJSONEncoder
    
    Usage example: json.dumps(data, indent=4, cls=ExtendedJSONEncoder)
    """
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def default(self, o):
        if isinstance(o, datetime):
            r = o.isoformat()
            if o.microsecond and o.microsecond % 1000 == 0:
                r = r[:23] + r[26:]
            if r.endswith("+00:00"):
                r = r[:-6] + "Z"
            return r
        elif isinstance(o, date):
            return o.isoformat()
        elif isinstance(o, time):
            if o.tzinfo is not None:
                raise ValueError("JSON can't represent timezone-aware times.")
            r = o.isoformat()
            if o.microsecond and o.microsecond % 1000 == 0:
                r = r[:12]
            return f'T{r}'
        elif isinstance(o, timedelta):
            return duration_iso_string(o)
        elif isinstance(o, (Decimal, UUID)):
            return str(o)
        else:                        
            try:
                from django.utils.functional import Promise
            except ImportError:
                Promise = None

            if Promise and isinstance(o, Promise):
                return str(o)
            elif isinstance(o, (Enum,Flag)):
                return o.value
            elif isinstance(o, bytes):
                return str(o)
            else:
                return super().default(o)

#endregion


#region Locale

def register_locale(name: str = ''):
    """
    Register a locale for the entire application (system default locale if argument `value` is None).
    """
    locale.setlocale(locale.LC_ALL, name)


@contextmanager
def use_locale(name = ''):
    """
    Use a locale temporary (in the following thread-local block/context).

    See: https://stackoverflow.com/a/24070673
    """
    with _locale_lock:
        saved = locale.setlocale(locale.LC_ALL)
        try:
            yield locale.setlocale(locale.LC_ALL, name)
        finally:
            locale.setlocale(locale.LC_ALL, saved)

_locale_lock = threading.Lock()


def get_lang():
    """
    Return current locale lang, for example: `fr_FR`.
    """
    global _lang
    if _lang is None:
        with use_locale():
            _lang = locale.getlocale()[0]
    return _lang

_lang = None


def get_locale_decimal_separator():
    global _locale_decimal_separator
    if _locale_decimal_separator is None:
        with use_locale():
            _locale_decimal_separator = locale.localeconv()["decimal_point"]
    return _locale_decimal_separator

_locale_decimal_separator = None

#endregion


#region Time

T_WithTime = TypeVar('T_WithTime', datetime, time)

def parse_tz(tz: tzinfo|str|Literal['localtime']|None = None):
    if tz is None or tz == 'localtime':
        if not ZoneInfo or sys.platform == 'win32':
            if not tzlocal:
                raise ValueError(f"Package `tzlocal` is required on Windows or on Python < 3.9 to retrieve local timezone")
            return tzlocal.get_localzone()
        return ZoneInfo('localtime')
    elif isinstance(tz, tzinfo):
        return tz
    elif tz == 'UTC':
        return timezone.utc
    elif isinstance(tz, str):
        if not ZoneInfo:
            if not pytz:
                raise ValueError(f"Package `pytz` is required on Python < 3.9 to parse timezones from strings")
            return pytz.timezone(tz)
        if sys.platform == 'win32':
            if not tzdata:
                raise ValueError(f"Package `tzdata` is required on Windows to parse timezones from strings")
        return ZoneInfo(tz)
    else:
        raise TypeError(f"Invalid timezone type: {tz} ({type(tz).__name__})")


def is_aware(value: T_WithTime):
    # See: https://docs.python.org/3/library/datetime.html#determining-if-an-object-is-aware-or-naive
    if value is None:
        return False
    return value.tzinfo is not None and value.utcoffset() is not None


def make_aware(value: T_WithTime, tz: tzinfo|str = None, after1970: bool = False) -> T_WithTime:
    """
    Make a datetime aware in timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    if value is None:
        return None

    if after1970 and value.year <= 1970:
        return None
    
    if is_aware(value):
        raise ValueError("make_aware expects a naive datetime, got %s" % value)
    
    tz = parse_tz(tz)
    if hasattr(tz, 'localize'):
        # See: https://stackoverflow.com/a/6411149
        return tz.localize(value)
    else:
        return value.replace(tzinfo=tz)


def is_naive(value: T_WithTime):
    return not is_aware(value)


def make_naive(value: T_WithTime, tz: tzinfo = None, after1970: bool = False) -> T_WithTime:
    """
    Make a datetime naive and expressed in timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    if value is None:
        return None

    if after1970 and value.year <= 1970:
        return None

    if not is_aware(value):
        raise ValueError("make_naive expects an aware datetime, got %s" % value)
    
    value = value.astimezone(None if tz is None or tz == 'localtime' else parse_tz(tz))
    value = value.replace(tzinfo=None)
    return value


def now_aware(tz: tzinfo|str = None, *, ms = True):
    """
    Get the current datetime in the timezone `tz` (use `tz=None` or `tz='localtime'` for the system local timezone).
    """
    now = datetime.now().astimezone(None if tz is None or tz == 'localtime' else parse_tz(tz))
    if not ms:
        now = now.replace(microsecond=0)
    return now


def now_naive_utc(*, ms = True):
    """
    Get the current datetime in expressed as a naive UTC.
    """
    now = datetime.now(tz=timezone.utc).replace(tzinfo=None)
    if not ms:
        now = now.replace(microsecond=0)
    return now


def duration_iso_string(duration: timedelta):
    # Adapted from: django.utils.duration.duration_iso_string
    if duration < timedelta(0):
        sign = "-"
        duration *= -1
    else:
        sign = ""

    days, hours, minutes, seconds, microseconds = _get_duration_components(duration)
    ms = ".{:06d}".format(microseconds) if microseconds else ""
    return "{}P{}DT{:02d}H{:02d}M{:02d}{}S".format(
        sign, days, hours, minutes, seconds, ms
    )


def _get_duration_components(duration: timedelta):
    days = duration.days
    seconds = duration.seconds
    microseconds = duration.microseconds

    minutes = seconds // 60
    seconds = seconds % 60

    hours = minutes // 60
    minutes = minutes % 60

    return days, hours, minutes, seconds, microseconds


_today = None

def today():
    global _today
    if _today is None:    
        _today = datetime.today().date()
    return _today

#endregion


#region URLs


def build_url(*, scheme: str = '', hostname: str|IPv4Address|IPv6Address = None, port: int = None, username: str = None, password: str = None, path: str = None, params: str = None, query: str = None, fragment: str = None, noquote = False, hide_password = False):
    netloc = build_netloc(hostname=hostname, port=port, username=username, password=password, noquote=noquote, hide_password=hide_password)

    if noquote:
        actual_query = query
    else:
        if isinstance(query, dict):
            actual_query = urlencode(query)
        elif isinstance(query, list):
            named_parts = []
            unnamed_parts = []
            for part in query:
                if isinstance(part, tuple):
                    named_parts.append(part)
                else:
                    unnamed_parts.append(part)
            actual_query = urlencode(named_parts, quote_via=quote)
            actual_query += ('&' if actual_query else '') + '&'.join(quote(part) for part in unnamed_parts)
        else:
            actual_query = query

    return urlunparse((scheme or '', netloc or '', (path or '') if noquote else quote(path or ''), (params or '') if noquote else quote(params or ''), actual_query or '', (fragment or '') if noquote else quote(fragment or '')))


def build_netloc(*, hostname: str|IPv4Address|IPv6Address = None, port: int = None, username: str = None, password: str = None, noquote = False, hide_password = False):
    netloc = ''
    if username or hostname:
        if username:
            netloc += username if noquote else quote(username)
            if password:
                netloc += ':' + ('***' if hide_password else (password if noquote else quote(password)))
            netloc += '@'

        if hostname:
            if isinstance(hostname, IPv4Address):
                netloc += hostname.compressed
            elif isinstance(hostname, IPv6Address):
                netloc += f"[{hostname.compressed}]"
            else:
                ipv6 = None
                if ':' in hostname:
                    try:
                        ipv6 = IPv6Address(hostname)
                    except AddressValueError:
                        pass

                if ipv6:
                    netloc += f"[{ipv6.compressed}]"
                else:
                    netloc += hostname if noquote else quote(hostname)

            if port:
                if not isinstance(port, int):
                    raise ValueError(f"invalid type for port: {type(port)}")
                netloc += f':{port}'

    return netloc


def hide_url_password(url: str):
    r = urlparse(url)
    return build_url(scheme=r.scheme, hostname=r.hostname, port=r.port, username=r.username, password=r.password, path=r.path, params=r.params, query=r.query, fragment=r.fragment, noquote=True, hide_password=True)


#endregion


#region Network

def resolve_host(host: str, *, timeout: float = None, ip_version: int = None) -> list[str]:
    """
    Make a DNS resolution with a timeout.
    """
    ip = parse_ipaddress_or_none(host)
    if ip:
        return [ip.compressed]
    
    if ip_version is None:
        family = 0
    elif ip_version == 4:
        family = socket.AddressFamily.AF_INET
    elif ip_version == 6:
        family = socket.AddressFamily.AF_INET6
    else:
        raise ValueError(f"Invalid ip version: {ip_version}")

    class Bucket:
        addresses = []
        exception = None

    bucket = Bucket()

    def target():
        try:
            for af, socktype, proto, canonname, sa in socket.getaddrinfo(host, port=0, family=family):
                bucket.addresses.append(sa[0])
        except BaseException as err:
            bucket.exception = err

    if timeout is not None:
        thread = Thread(target=target, daemon=True)
        thread.start()
        thread.join(timeout=timeout)
        if thread.is_alive():
            raise TimeoutError(f"Name resolution for host \"{host}\" timed out")

    else:
        target()

    if bucket.exception:
        err = NameError(str(bucket.exception))
        err.name = host
        raise err
        
    return bucket.addresses


class JSONApiClient:
    base_url : str = None
    timeout: float = None
    """ Timeout in seconds. """

    force_trailing_slash: bool = False

    default_headers = {
        'Content-Type': 'application/json; charset=utf-8',
        'Accept': 'application/json; charset=utf-8',
    }

    json_encoder_cls: type[json.JSONEncoder] = ExtendedJSONEncoder
    json_decoder_cls: type[json.JSONDecoder] = json.JSONDecoder
    
    nonjson_error_maxlen = 400

    no_ssl_verify = False


    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs) # necessary to allow this class to be used as a mixin
        self._logger = logging.getLogger(type(self).__module__ + '.' + type(self).__name__)
        self._ssl_context = None
        if self.no_ssl_verify or kwargs.get('no_ssl_verify'):
            self._ssl_context = ssl.create_default_context()
            self._ssl_context.check_hostname = False
            self._ssl_context.verify_mode = ssl.CERT_NONE


    def __enter__(self):
        return self


    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        pass


    def get(self, endpoint: str = None, *, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False):
        return self.request(endpoint, method='GET', params=params, headers=headers, return_headers=return_headers)


    def post(self, endpoint: str = None, data = None, *, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False, content_type: str = None, content_length: int = None, content_filename: str = None):
        return self.request(endpoint, data, method='POST', params=params, headers=headers, return_headers=return_headers, content_type=content_type, content_length=content_length, content_filename=content_filename)
    

    def request(self, endpoint: str = None, data = None, *, method = None, params: dict = None, headers: MutableMapping[str,str] = None, return_headers = False, content_type: str = None, content_length: int = None, content_filename: str = None):
        url = self.prepare_url(endpoint, params=params)

        all_headers = self.get_request_headers(url)
        if headers:
            for key, value in headers.items():
                all_headers[key] = value
                if key == 'Content-Type' and not content_type:
                    content_type = value
                elif key == 'Content-Length' and content_length is None:
                    content_length = int(value) if isinstance(value, str) else value
                elif key == 'Content-Disposition' and not content_filename:
                    m = re.search(r'attachment\s*;\s*filename\s*=\s*(.+)', value)
                    if m:
                        content_filename = m[1].strip()

        if content_type:
            all_headers['Content-Type'] = content_type
        if content_length is not None:
            all_headers['Content-Length'] = content_length
        if content_filename:
            all_headers['Content-Disposition'] = f"attachment; filename={content_filename}"
                
        if data is not None:
            if not method:
                method = 'POST'

            if isinstance(data, IOBase) or (content_type and not 'application/json' in content_type):
                # keep data as is: this is supposed to be an uploaded file
                if not content_type:
                    content_type = 'application/octet-stream'
            else:
                data = json.dumps(data, ensure_ascii=False, cls=self.json_encoder_cls).encode('utf-8')
            
            self._logger.debug('%s %s', method, url)
            request = Request(url,
                method=method,
                headers=all_headers,
                data=data,
            )
        else:
            if not method:
                method = 'GET'
            
            self._logger.debug('%s %s', method, url)
            request = Request(url,
                method=method,
                headers=all_headers,
            )

        response_headers = {}
        try:
            response: HTTPResponse
            with urlopen(request, timeout=self.timeout, context=self._ssl_context) as response:
                response_headers = response.headers
                if self._logger.isEnabledFor(logging.DEBUG):
                    content_type = response.headers.get('content-type', '-')
                    self._logger.debug('%s %s %s %s', response.status, url, response.length, content_type)
                decoded_response = self.decode_response(response)
            
            if return_headers:
                return decoded_response, response.headers
            else:
                return decoded_response
            
        except HTTPError as error:
            with error:
                http_data = self.decode_response(error)
            built_error = self.build_client_error(error, http_data)
        except URLError as error:
            built_error = self.build_client_error(error, None)

        if isinstance(built_error, Exception):
            raise built_error from None
        else:
            if return_headers:
                return built_error, response_headers
            else:
                return built_error


    def prepare_url(self, endpoint: str, *, params: dict = None, base_url: str = None):
        if endpoint is None:
            endpoint = ''

        if not base_url and self.base_url:
            base_url = self.base_url

        if '://' in endpoint or not base_url:
            url = endpoint
            
        else:            
            if endpoint.startswith('/'):
                if base_url.endswith('/'):                    
                    endpoint = endpoint[1:]
            else:
                if not base_url.endswith('/') and endpoint:
                    endpoint = f'/{endpoint}'
            
            if self.force_trailing_slash and not endpoint.endswith('/'):
                endpoint = f'{endpoint}/'

            url = f'{base_url}{endpoint}'

        if params:
            url += "?" + urlencode(params)
        
        return url
    

    def get_request_headers(self, url: str) -> MutableMapping[str,str]:
        headers = {**self.default_headers}
        return headers


    def decode_response(self, response: HTTPResponse):
        rawdata = response.read()
        try:
            strdata = rawdata.decode('utf-8')
        except UnicodeDecodeError:
            strdata = str(rawdata)
            if self.nonjson_error_maxlen is not None and len(strdata) > self.nonjson_error_maxlen:
                strdata = strdata[0:self.nonjson_error_maxlen] + 'â¦'
            return f"[non-utf-8] {strdata}"
        
        try:
            jsondata = json.loads(strdata, cls=self.json_decoder_cls)
        except json.JSONDecodeError:
            if self.nonjson_error_maxlen is not None and len(strdata) > self.nonjson_error_maxlen:
                strdata = strdata[0:self.nonjson_error_maxlen] + 'â¦'
            return f"[non-json] {strdata}"
        
        return jsondata


    def build_client_error(self, error: URLError, http_data):
        if isinstance(error, HTTPError):
            return ApiClientError(error.reason, code=error.status, code_nature='status', data=http_data)
        else:
            return ApiClientError(error.reason, code=error.errno, code_nature='errno', data=http_data)
        

class ApiClientError(Exception):
    def __init__(self, message: str, *, code: int = None, code_nature = None, data = None):
        self.raw_message = message
        self.code = code
        self.code_nature = code_nature
        self.data = data

        super().__init__(self.raw_to_message())


    def raw_to_message(self):
        message = self.raw_message

        if self.code:
            message = (message + ' ' if message else '') + f"[{self.code_nature or 'code'}: {self.code}]"
        
        if self.data:
            if isinstance(self.data, dict):
                for key, value in self.data.items():
                    message = (message + '\n' if message else '') + f"{key}: {value}"
            else:
                message = (message + '\n' if message else '') + str(self.data)

        return message


#endregion


#region Tables

class ColumnsProvider:
    def __init__(self):
        self._columns: list[str]|None = None
        self._column_indexes: dict[str, int]|None = None
        self._headers: list[Header]|None = None
        self._invalidated = False
    
    def _invalidate_columns(self, *, invalidate_headers: bool = False):
        # For actual provider internal usage
        self._invalidated = True
        self._column_indexes = None
        if invalidate_headers:
            self._headers = None

    def _build_columns(self) -> list[str]:
        # For ColumnsProvider internal usage, may be overriden by actual provider
        if self._headers is not None:
            return [header.name for header in self._headers]
        else:
            raise NotImplementedError()

    @property
    def columns(self) -> list[str]:
        if self._columns is None or self._invalidated:
            self._columns = self._build_columns()
            self._invalidated = False
        return self._columns

    def get_column_index(self, name: str) -> int:
        if self._column_indexes is None:
            self._column_indexes = {n: i for i, n in enumerate(self.columns)}
        return self._column_indexes[name]

    @property
    def headers(self):
        if self._headers is None:
            self._headers = [Header(name) for name in self.columns]
        return self._headers


class Row:
    def __init__(self, provider: ColumnsProvider, values: Iterable[Any], *, skip_convert = False, nullval: str|None = None):
        self._provider = provider
        self._values = values
        self._values_converted = True if skip_convert else False
        self._nullval = nullval

    def _build_values(self) -> list[str]:
        # For Row internal usage, may be overriden by actual row
        raise NotImplementedError()
    
    @property
    def values(self) -> list[Any]:
        if not isinstance(self._values, list):
            self._values = list(self._values)
        if not self._values_converted:
            if self._provider.headers:
                for index, header in enumerate(self._provider.headers):
                    self._values[index] = header.to_python(self._values[index], nullval=self._nullval) # ROADMAP: decimal_format (from csvfmt.decimal_format?) could be passed here to avoid issues when parsing strings such as "1,234.56"
            else:
                for index, value in enumerate(self._values):
                    if value == self._nullval:
                        self._values[index] = None
            self._values_converted = True
        return self._values

    def __repr__(self):
        return f"{self.__class__.__name__}{self.values}"
        
    def __len__(self):
        return len(self._values)

    def __getitem__(self, key: int|str):
        if not isinstance(key, int):
            key = self._provider.get_column_index(key)            
        return self.values[key]
    
    def get(self, key: int|str, default = None):
        try:
            return self[key]
        except KeyError:
            return default
        
    def as_tuple(self):
        if isinstance(self._values, tuple) and self._values_converted:
            return self._values
        else:
            return tuple(self.values)
    
    def as_dict(self):
        return {column: self[i] for i, column in enumerate(self._provider.columns)}


class Header:
    _type_ = type

    def __init__(self,
                 name: str|None = None, *,
                 key: Any|None = None,
                 type: type|None = None,
                 sql_type: str|None = None,
                 primary_key: bool|None = None,
                 unique: bool|list[list[str]]|None = None,
                 identity: bool|str|None = None,
                 null: bool|None = None,
                 fmt: str|Callable = None,
                 multiply: int = None):
        """
        Create a header for tabular data.
        - `key`: how to obtain the value from a dict object (may be a key or a list of keys for recursive get).
        - `name`: name of the header (defaults to stringified key).
        """
        if key is None:
            if name is None:
                raise ValueError(f"Key and name both null")
            key = name

        if isinstance(key, list):
            if len(key) == 0:
                raise TypeError(f"key: cannot be an empty list")
            self.key = key
        else:
            self.key = [key]

        if name is None:
            self.name = '.'.join(str(part) for part in self.key)
        elif not isinstance(name, str):
            self.name = str(name)
        else:
            self.name = name

        self.type = type

        if isinstance(sql_type, str):
            self.sql_type = sql_type
        elif sql_type is None:
            self.sql_type = None
        else:
            raise TypeError(f"sql_type: {self._type_(sql_type).__name__}")

        # ROADMAP: add sql_default value

        self.null = null
        """ Indicate whether this column is nullable. """

        self.primary_key = primary_key
        """ Indicate whether this column is part of the primary key. """

        self.unique = unique
        """ If a list, unique togethers. """

        self.identity = identity
        """ Indicate whether this is an auto-generated identity column. """

        self.fmt = fmt
        self.multiple = multiply

    def __repr__(self):
        return self.name
    
    @property
    def path(self):
        return '.'.join(str(part) for part in self.key)
    
    @property
    def toplevel_key(self):
        return self.key[0]
    
    def get_value(self, obj: dict):
        def recurse(obj: dict, remaining_keys: list[Any], parent_path: str):
            if obj is None:
                return None
            elif not remaining_keys:
                return obj
            else:                
                key = remaining_keys[0]
                path = parent_path + ('.' if parent_path else '') + str(key)
                remaining_keys = remaining_keys[1:]
                if not isinstance(obj, dict):
                    raise NotADirectoryError(path)
                else:
                    try:
                        sub = obj[key]
                    except KeyError:
                        raise KeyError(path)
                return recurse(sub, remaining_keys, path)
            
        return recurse(obj, self.key, '')

    def to_str(self, value, *, after1970: bool):
        if value is None:
            return None
        
        if after1970 and isinstance(value, datetime) and value.year <= 1970:
            return None
        
        if isinstance(self.fmt, str):
            if self.fmt == 'gib':
                if not isinstance(value, int):
                    value = convert_to_int(value)
                value = gigi_bytes(value)
            
            elif self.fmt == 'mib':
                if not isinstance(value, int):
                    value = convert_to_int(value)
                value = megi_bytes(value)
            
            elif self.fmt == 'kib':
                if not isinstance(value, int):
                    value = convert_to_int(value)
                value = kili_bytes(value)
            
            elif self.fmt == 'human_bytes':
                if not isinstance(value, int):
                    value = convert_to_int(value)
                value = human_bytes(value)
        
            elif self.fmt == 'datetime' or self.fmt == 'dateortime' or self.fmt == 'datetime1970' or self.fmt == 'dateortime1970':
                if not isinstance(value, datetime):
                    value = convert_to_datetime(value)
                if value.year <= 1970 and (after1970 or self.fmt == 'datetime1970' or self.fmt == 'dateortime1970'):
                    value = None
                if self.fmt == 'dateortime' or self.fmt == 'dateortime1970':
                    value = value.strftime('%H:%M:%S') if value.date() == today() else value.strftime('%Y-%m-%d')

        elif callable(self.fmt):
            if self.fmt in {bool, int, float, Decimal, datetime, date, time, list}:
                value = convert(value, self.fmt)
            else:
                value = self.fmt(value)

        if self.multiple is not None:
            value = value * self.multiple
    
        return value
    
    def to_python(self, value, *, nullval):
        if self.fmt:
            raise NotImplementedError()
        
        value = convert(value, self.type, nullval=nullval)
        
        if self.multiple is not None and value is not None:
            value = value / self.multiple
    
        return value
    
    def _get_csv_floatfmt(self):
        if self.fmt in ['gib']:
            return '.9f'
        elif self.fmt in ['mib']:
            return '.6f'
        elif self.fmt in ['kib']:
            return '.3f'
        else:
            return None
    
    def _get_tab_floatfmt(self):
        if self.fmt in ['gib', 'mib', 'kib']:
            return '.2f'
        else:
            return None

    def copy(self, *, no_constraints = False, no_identity = False):
        newheader = copy(self)
        if no_constraints:
            newheader.null = True
            newheader.unique = False
            newheader.primary_key = False
        if no_identity:
            newheader.identity = False
        return newheader


class CsvFormat:
    """
    - `excel`: format CSV so that it may be displayed easily with Excel:
        - Delimiter and decimal separator are chosen depending on the locale.
        - Datetimes are expressed naively (without a timezone), in the local timezone, without microseconds.
        - The file starts with UTF8 BOM, which Excel needs to display accentuated characters correctly.
    """
    
    # Configurable globally
    encoding = 'utf-8-sig'
    delimiter = ','
    decimal_separator = '.'
    quotechar = '"'
    nullval = None
    no_headers = False
    excel = None

    def __init__(self, *,
                 encoding: str|None = None,
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,
                 quotechar: str|None = None,
                 nullval: str|None = None,
                 no_headers: bool|None = None,
                 excel: bool|None = None):
        
        self._logger = logging.getLogger(f'{__name__}.{self.__class__.__qualname__}')

        if excel is None:
            excel = get_bool_variable('CSV_EXCEL')
            if excel is None:
                excel = self.__class__.excel
        
        self._delimiter_specified = delimiter is not None
        if excel:
            if decimal_separator is None:
                locale_decimal_separator = get_locale_decimal_separator()
                if locale_decimal_separator == delimiter:
                    if delimiter == ',':
                        decimal_separator = '.'
                    elif delimiter == ';':
                        decimal_separator = ','
                    else:
                        raise ValueError(f"Provided delimiter and the locale decimal separator are both \"{delimiter}\". Please provide a distinct decimal separator.")
                else:
                    decimal_separator = locale_decimal_separator
            
            if delimiter is None:
                delimiter = ';' if decimal_separator == ',' else ','
        else:
            if decimal_separator is None:
                decimal_separator = self.__class__.decimal_separator
            if delimiter is None:
                delimiter = self.__class__.delimiter

        if encoding is None:
            encoding = self.__class__.encoding

        if quotechar is None:
            quotechar = self.__class__.quotechar
        
        if nullval is None:
            nullval = self.__class__.nullval

        if no_headers is None:
            no_headers = self.__class__.no_headers

        # Register configuration for this instance
        self.delimiter = delimiter
        self.decimal_separator = decimal_separator
        self.quotechar = quotechar
        self.nullval = nullval
        self.no_headers = no_headers
        self.excel = excel

        # Available after examining file
        self._first_row: list[str]|None = _UNSET
        self._ends_with_newline: bool = _UNSET

    def examine_file(self, file: TextIOWrapper|str|Path, *, file_name: str = None, need_ends_with_newline: bool = False):
        from zut import files

        if not isinstance(file, IOBase):
            if not files.exists(file):
                self._first_row = None
                self._ends_with_newline = False
                return self

            if not file_name:
                file_name = str(file)

        with nullcontext(file) if isinstance(file, IOBase) else files.open(file, 'r', encoding='utf-8' if self.encoding == 'utf-8-sig' else 'utf-8', newline='') as fp:
            skip_utf8_bom(fp, self.encoding)

            first_line = StringIO()
            first_line_ended = False
            self._ends_with_newline = False
            buf_size = 65536
            while True:
                chunk = fp.read(buf_size)
                if not chunk:
                    break

                if not first_line_ended:
                    pos = chunk.find('\n')
                    if pos >= 0:
                        first_line.write(chunk[:pos])
                        first_line_ended = True
                        if not need_ends_with_newline:
                            break
                    else:
                        first_line.write(chunk)

                self._ends_with_newline = chunk[-1] == '\n'

            if first_line.tell() == 0:
                self._first_row = None
                return self
        
            # Guess the delimiter
            first_line_str = first_line.getvalue()
            if first_line_str.count(';') > first_line_str.count(','):
                guessed_delimiter = ';'
            else:
                guessed_delimiter = ','

            # Compare with configured delimiter
            if guessed_delimiter != self.delimiter:
                guessed_decimal_separator = '.' if guessed_delimiter == ',' else ','
                if self._delimiter_specified:
                    self._logger.warning("CSV format seem invalid for %s: delimiter: \"%s\" (seem to be \"%s\"), decimal separator: \"%s\" (seem to be \"%s\")", file_name, self.delimiter, guessed_delimiter, self.decimal_separator, guessed_decimal_separator)
                else:                    
                    self._logger.debug("Change CSV default format after detection in %s: delimiter: \"%s\" => \"%s\", decimal separator: \"%s\" => \"%s\"", file_name, self.delimiter, guessed_delimiter, self.decimal_separator, guessed_decimal_separator)
                    self.delimiter = guessed_delimiter
                    self.decimal_separator = guessed_decimal_separator

            # Retrieve column names
            first_line.seek(0)
            reader = csv.reader(first_line, delimiter=self.delimiter)
            self._first_row = next(reader)

        # Ensure we move back the fp if it was externally built
        if isinstance(file, IOBase):
            file.seek(0)
            skip_utf8_bom(file, self.encoding)

        return self
    
    @property
    def first_row(self):
        if self._first_row is _UNSET:
            raise NotAvailableError(f"First row not available: no file examined")
        return self._first_row
    
    @property
    def headers(self):
        if self.no_headers:
            raise NotAvailableError(f"No headers in files")
        if self._first_row is _UNSET:
            raise NotAvailableError(f"Headers not available: no file examined")
        if self._first_row is None:
            return None
        return [Header(value) for value in self._first_row]
    
    @property
    def ends_with_newline(self):
        if self._ends_with_newline is _UNSET:
            raise NotAvailableError(f"ends_with_newline not available: no file examined with `need_ends_with_newline` option")
        return self._ends_with_newline
    
    def escape_value(self, value: str):
        if value is None:    
            return self.nullval if self.nullval is not None else ''
        if not isinstance(value, str):
            value = str(value)
        if value == '':
            return f'{self.quotechar}{self.quotechar}'

        need_escape = False
        result = ''
        for c in value:
            if c == self.delimiter:
                result += c
                need_escape = True
            elif c == self.quotechar:
                result += f'{c}{c}'
                need_escape = True
            elif c == '\n' or c == '\r':
                result += c
                need_escape = True
            else:
                result += c

        if need_escape:
            result = f'{self.quotechar}{result}{self.quotechar}'
        else:
            result = result

        return result

#endregion


#region Dump

def dump(data, *,
         dst: str|Path|IOBase,
         tabular: bool = None,
         truncate: bool = None,
         archivate: bool|str|Path|None = None,
         title: str|bool|None = None,
         dst_name: str|bool = True,
         dir: str|Path = None,
         # Other TabularDumper options
         headers: Iterable[str|Header]|None = None,
         delay: bool = True,
         optional: str|Sequence[str]|Literal['*',True]|None = None,
         add_columns: bool|Literal['warn'] = False,
         after1970: bool|None = None,         
         notz: bool|tzinfo|str|None = None,
         # TabDumper and CsvDumper options
         encoding: str|None = None,
         intfmt: str|None = None,
         floatfmt: str|None = None,
         # CsvDumper options
         delimiter: str|None = None,
         decimal_separator: str|None = None,
         quotechar: str|None = None,
         nullval: str|None = None,
         excel: bool|None = None,
         # Destination mask values and other options
         **kwargs):
    """
    Dump an object (if `data` is not an iterable) or a an iterable of dicts or iterables (if `data` is an iterable).

    NOTE: `delay` is True by default for `dump` because we expect the input data to be already produced entirely so we can take time to analyze all the dict keys to determine headers,
    whereas `delay` is False by default for `dump_tabular` or `CsvDumper` because we expect appended data to be produced on the fly so we favor immediate processing.
    """
    extended_kwargs = {
        'headers': headers,
        'truncate': truncate,
        'archivate': archivate,
        'title': title,
        'dst_name': dst_name,
        'dir': dir,
        'delay': delay,
        'optional': optional,
        'add_colunms': add_columns,
        'after1970': after1970,
        'notz': notz,
        'encoding': encoding,
        'intfmt': intfmt,
        'floatfmt': floatfmt,
        'delimiter': delimiter,
        'decimal_separator': decimal_separator,
        'quotechar': quotechar,
        'nullval': nullval,
        'excel': excel,
    }

    if tabular is None:
        tabular = 'headers' in extended_kwargs or 'delimiter' in extended_kwargs or dst == 'tab' or dst == 'csv' or (isinstance(dst, (str,Path)) and is_tabular_path(dst))
    
    if tabular:
        if data is None or isinstance(data, (str,dict)) or not hasattr(data, '__iter__'):
            raise TypeError(f"data: {type(data).__name__} (not iterable)")

        with tabular_dumper(dst, **extended_kwargs) as t:
            for elem in data:
                t.append(elem)
    
    else:
        dump_object(data, **extended_kwargs)


def is_tabular_path(path: str|Path):
    from zut import files
    from zut.excel import is_excel_path
    
    if path is None:
        return False

    if isinstance(path, Path):
        path = str(path)
    elif not isinstance(path, str):
        raise TypeError(f"path: {type(path).__name__}")
    
    _, ext = files.splitext(path)
    return ext.lower() in {'.csv', '.xlsx'} or is_excel_path(path)


def dump_object(data, *,
                dst: str|Path|IOBase|None = None,
                truncate: bool = None,
                archivate: bool|str|Path|None = None,
                title: str|bool|None = None,
                dst_name: str|bool = True,
                dir: str|Path = None,
                sort_keys: bool = False,
                **kwargs):
    """
    Export any data object, in JSON format, or using pretty print.
    """
    from zut import files
    
    if dst == os.devnull: # Do nothing
        return
    
    if isinstance(data, GeneratorType):
        data = [elem for elem in data]

    use_pprint = False
    if dst is None: # Display on stdout using pretty print
        use_pprint = True
        if dst_name is True:
            dst_name = '<stdout>'
    elif dst == 'json': # Display on stdout using JSON
        dst = sys.stdout
        if dst_name is True:
            dst_name = '<stdout>'
    elif isinstance(dst, (str,Path)):
        dst = files.indir(dst, dir=dir, title=title, **kwargs)
        if archivate:
            files.archivate(dst, archive_dir=archivate, missing_ok=True)

        if truncate is None:
            truncate = TabularDumper.truncate
        if truncate:
            files.remove(dst, missing_ok=True)
        
        parent = files.dirname(dst)
        if parent:
            files.makedirs(parent, exist_ok=True)

        if title is None:
            title = True
        if dst_name is True:
            dst_name = dst
    else:
        raise TypeError(f"dst: {type(dst).__name__}")

    if title is not False:
        _logger.info(f"Export{f' {title}' if title and not title is True else ''}{f' to {dst_name}' if dst_name else ''}")

    if use_pprint:        
        pprint(data, sort_dicts=sort_keys, stream=dst)
    
    else:
        with nullcontext(dst) if isinstance(dst, IOBase) else files.open(dst, 'w', encoding='utf-8') as fp:
            json.dump(data, fp=fp, indent=4, cls=ExtendedJSONEncoder, ensure_ascii=False, sort_keys=sort_keys)
            fp.write('\n')


def tabular_dumper(dst: str|Path|IOBase|DbAdapter|Connection|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[str|Header]|None = None,
                 truncate: bool = None,
                 archivate: bool|str|Path|None = None,
                 title: str|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|Path = None,
                 delay: bool = False,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,                 
                 add_columns: bool|Literal['warn'] = False,
                 after1970: bool|None = None,
                 notz: bool|tzinfo|str|None = None,
                 # TabDumper and CsvDumper options
                 encoding: str|None = None,
                 intfmt: str|None = None,
                 floatfmt: str|None = None,
                 # CsvDumper options
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,
                 quotechar: str|None = None,
                 nullval: str|None = None,
                 excel: bool|None = None,
                 # DbDumper options
                 table: str|tuple|None = None,
                 add_autoincrement_pk: bool|str = False,
                 # Destination mask values and other options
                 **kwargs):
    """
    Export tabular data, in CSV format, or using `tabulate`.

    Options:

    - `delay`: if True, wait the end of the context to determine headers (based on appended dictionnaries) and start outputing. This only applies if we're not appending to an existing file.

    - `optional`: list of header names that are expected but might not be given as keys of the first appended dict row.

    - `notz`: transform all aware (= with timezone info) datetimes into naive (= timezone info removed), expressing them in the given timezone:
        - Use `True` for local timezone (this is the default for CSV excel exports).
        - Use `False` to keep aware datetimes unchanged (this is the default for other exports).

    NOTE: empty strings are written with double quotes ("") in order to distinguish them from NULL values (written as an
    unquoted string). This is compatible with PostgreSQL's COPY FROM command.
    """
    from zut.db import (DbAdapter, get_db_adapter_from_connection,
                        get_db_adapter_from_url)
    from zut.excel import ExcelDumper, is_excel_path

    extended_kwargs = {
        'headers': headers,
        'truncate': truncate,
        'archivate': archivate,
        'title': title,
        'dst_name': dst_name,
        'dir': dir,
        'delay': delay,
        'optional': optional,
        'add_columns': add_columns,
        'after1970': after1970,
        'notz': notz,
        'encoding': encoding,
        'intfmt': intfmt,
        'floatfmt': floatfmt,
        'delimiter': delimiter,
        'decimal_separator': decimal_separator,
        'quotechar': quotechar,
        'nullval': nullval,
        'excel': excel,
        'table': table,
        'add_autoincrement_pk': add_autoincrement_pk,
        **kwargs
    }

    if dst == os.devnull: # Do nothing
        return NoopDumper()
    
    elif dst is None or dst == 'tab' or dst == 'csv': # Display on stdout using tabulate or CSV
        if (dst is None or dst == 'tab') and TabDumper.is_available():
            return TabDumper(sys.stdout, **extended_kwargs)
        else:
            return CsvDumper(sys.stdout, **extended_kwargs)
        
    elif is_excel_path(dst, accept_table_suffix=True):
        return ExcelDumper(dst, **extended_kwargs)
    
    else:
        if isinstance(dst, DbAdapter):
            adapter = dst
            keep_connection = None
        elif isinstance(dst, str):
            adapter = get_db_adapter_from_url(dst, autocommit=False) # commit will be done explicitely in DbDumper
            keep_connection = False
        else:
            adapter = get_db_adapter_from_connection(dst)
            keep_connection = True

        if adapter:
            return adapter.dumper(keep_connection=keep_connection, **extended_kwargs)
        else:
            # By default: CSV
            return CsvDumper(dst, **extended_kwargs)


T_TabularDumper_Dst = TypeVar('T_TabularDumper_Dst')

class TabularDumper(Generic[T_TabularDumper_Dst]):
    truncate = False
    after1970 = False
    foreign_id_keys = {'id', 'pk', 'ref'}
    foreign_name_keys = {'name', 'title', 'text', '__str__'}
    dst: T_TabularDumper_Dst
    
    @classmethod
    def is_available(cls):
        return True
    
    def __init__(self,
                 dst: T_TabularDumper_Dst,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[Header|Any]|None = None,
                 truncate: bool = None,
                 archivate: bool|str|Path|None = None,
                 title: str|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|Path|Literal[False]|None = None,
                 delay: bool = False,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|Literal['warn'] = False,
                 after1970: bool|None = None,                 
                 notz: bool|tzinfo|str|None = None,
                 # Destination mask values
                 **kwargs):
        
        self._logger = logging.getLogger(f'{__name__}.{self.__class__.__qualname__}')
        
        self.headers: list[Header]|None = [Header(header) if not isinstance(header, Header) else header for header in headers] if headers is not None else None
        self.truncate = truncate if truncate is not None else self.__class__.truncate
        self.archivate = archivate # managed by each subclass
        
        self.delayed: list[Iterable|dict] = [] if delay else None

        self.optional = ('*' if delay else []) if optional is None else '*' if optional == '*' or optional is True else [optional] if isinstance(optional, str) else optional        
        self.add_columns = add_columns
        
        self.count = 0
        """ Number of appended rows. """

        self.exported_count = 0
        """ Number of exported rows. """

        if isinstance(dst, IOBase):
            self.dst = dst
            self.dst_name = getattr(dst, 'name', f'<{type(dst).__name__}>') if dst_name is True else dst_name
            self.title = False if title is None else title
        elif isinstance(dst, (str,Path)):
            if dir is not False:
                from zut import files
                self.dst = files.indir(dst, dir, title=title, **kwargs)
            else:
                self.dst = dst if isinstance(dst, str) else str(dst)
            self.dst_name = self.dst if dst_name is True else dst_name
            self.title = True if title is None else title
        else:
            self.dst = dst
            self.dst_name = str(self.dst) if dst_name is True else dst_name
            self.title = False if title is None else title

        self.after1970 = after1970 if after1970 is not None else self.__class__.after1970

        if isinstance(notz, str):
            notz = notz if notz == 'localtime' else parse_tz(notz)
        self.notz = notz

        self._closed = False

        # Manage existing headers
        self._combined_headers: list[Header]|None = None
        """ All headers, including those collected from dict rows, starting with previously existing headers, in order. Stays None if target headers exactly match existing headers. """
        self._combined_indexes: list[int] = None
        """ Indexes of row values (for rows appended as iterables) within combined headers. """
        self._headers_from_dicts = False
        self._existing_headers: list[Header]|None = None

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        self.close()

    def open(self) -> list[Header]|None:
        """
        Called at first exported row, before headers are analyzed.
        
        Return list of existing headers, or None if file/headers must be created.
        """
        return None
    
    def export_headers(self, headers: list[Header]):
        """
        Called at first exported row, if there are no pre-existing headers. 
        """
        pass
    
    def new_headers(self, headers: list[Header]) -> bool|None:
        """
        Called at first exported row, if there are headers that did not exist in pre-existing headers, and for dict rows
        having additional keys (if `additional_headers` is True).

        Return `None` to prevent adding any new header, `False` to allow new columns but without a header name, and
        `True` to allow updating header names.
        """
        return False

    def export(self, row: list):
        """
        Called for each exported row. Row has been prepared and is a list, with values already converted if necessary.
        Rows are already reordered to match order of existing rows (if any), i.e. they match `self._combined_headers` (if defined).
        """
        pass

    def flush(self):
        if self.delayed:
            if self.headers is None:
                self.headers = self._get_headers_from_all_dict_rows(self.delayed)
                self._headers_from_dicts = True

            for row in self.delayed:
                self._prepare_and_export_row(row)
            
            self.delayed = []

    def close(self):
        """
        Called at exit.
        """
        if self._closed:
            return

        self.flush()

        if self.title is not False:
            self._logger.info(f"{self.count:,}{f' {self.title}' if self.title and not self.title is True else ''} row{'s' if self.count > 1 else ''} exported{f' to {self.dst_name}' if self.dst_name else ''}")
        
        self._closed = True

    def append(self, row: Iterable|Row|dict):
        if not isinstance(row, (Row,list,tuple,dict)):
            if row is None or isinstance(row, (str,Path)) or not (hasattr(row, '__iter__') or (hasattr(row, '__len__') and hasattr(row, '__getitem__'))):
                raise TypeError(f"row: {type(row).__name__}")

        self.count += 1

        if self.delayed is not None:
            self.delayed.append(row)
        else:
            if self.count == 1 and self.headers is None and isinstance(row, (Row,dict)):
                self.headers = self._get_headers_from_first_dict_row(row)
                self._headers_from_dicts = True
            self._prepare_and_export_row(row)

    def _prepare_and_export_row(self, row: Iterable|Row|dict):
        self.exported_count += 1

        if self.exported_count == 1:
            if self.title is not False:
                self._logger.info(f"Export{f' {self.title}' if self.title and not self.title is True else ''}{f' to {self.dst_name}' if self.dst_name else ''} â¦")

            self._existing_headers = self.open()

            if self.headers is not None:                
                if self._existing_headers is None:
                    self.export_headers(self.headers)
                else:
                    header_names = [header.name for header in self.headers]
                    existing_names = [header.name for header in self._existing_headers]

                    if header_names != existing_names:
                        self._combined_headers = self._existing_headers if self._existing_headers is not None else []
                        self._combined_indexes = []
                        new_headers: dict[int,Header] = {}               

                        for i, header in enumerate(self.headers):
                            try:
                                index = existing_names.index(header.name)
                                self._combined_headers[index] = header
                                self._combined_indexes.append(index)
                            except ValueError:
                                new_headers[i] = header
                                self._combined_indexes.append(None)

                        if new_headers:
                            names = ', '.join(f'"{header.name}"' for header in new_headers.values())

                            if self._headers_from_dicts and not self.add_columns:
                                self._logger.warning(f"New header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                            else:
                                result = self.new_headers(list(new_headers.values()))
                                if result is None:
                                    self._logger.warning(f"New header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                                else:
                                    next_index = len(self._existing_headers)

                                    if result:
                                        self._logger.log(logging.WARNING if self.add_columns == 'warn' else logging.INFO, f"New header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} added")
                                    else:
                                        col_min = next_index + 1
                                        col_max = col_min + len(new_headers) - 1
                                        self._logger.log(logging.WARNING if self.add_columns == 'warn' else logging.INFO, f"New header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} {col_min}{'-'+str(col_max) if len(new_headers) > 1 else ''} added without a column name")
                                    
                                    for i, header in new_headers.items():
                                        self._combined_headers.append(header)
                                        self._combined_indexes[i] = next_index
                                        next_index += 1
        
        if isinstance(row, Row):
            if self.headers is not None:
                combined_row = self._dict_to_combined_row(row.as_dict())
            else:
                combined_row = self._iterable_to_combined_row(row.values)
        elif isinstance(row, dict):
            combined_row = self._dict_to_combined_row(row)
        else:            
            combined_row = self._iterable_to_combined_row(row)

        combined_row = self._convert_combined_row(combined_row)
        self.export(combined_row)

    def _iterable_to_combined_row(self, row: Iterable):    
        if self.headers is not None:
            expected = len(self.headers)
            actual = len(row)
            if actual < expected:
                self._logger.warning(f"Row {self.exported_count}: only {actual} column{'s' if actual > 1 else ''} (expected {expected} column{'s' if expected > 1 else ''}): null values will be added")
                row += [None] * (expected - actual)
            elif actual > expected:                    
                self._logger.warning(f"Row {self.exported_count}: too many column{'s' if actual > 1 else ''}: {actual} (expected {expected} column{'s' if expected > 1 else ''}): additional values will be discarded")
                row = row[:expected]
                
        if self._combined_indexes is None:
            return row
        
        combined_row = [None] * len(self._combined_headers)
        for i, value in enumerate(row):
            index = self._combined_indexes[i]
            combined_row[index] = value
        
        return combined_row

    def _dict_to_combined_row(self, obj: dict):
        if self.headers is None:
            raise ValueError("Cannot use dict rows when there are no headers")
        
        headers = self._combined_headers if self._combined_headers is not None else self.headers

        combined_row = []
        used_toplevel_keys = set()
        missing_paths = []
        for header in headers:
            used_toplevel_keys.add(header.toplevel_key)
            try:
                value = header.get_value(obj)
            except NotADirectoryError:
                value = obj.get(header.toplevel_key)
                self._logger.warning(f"Row {self.exported_count}: column \"{header.path}\" was not applied to a dictionnary")
            except KeyError as err:
                path = err.args[0]
                if self.optional != '*' and not path in self.optional and header in self.headers:
                    missing_paths.append(path)
                value = None
            combined_row.append(value)

        new_keys = [key for key in obj.keys() if not key in used_toplevel_keys]
        if new_keys:
            new_headers = [Header(key=key) for key in new_keys]
            names = ', '.join(f'"{header.name}"' for header in new_headers)

            if (self._existing_headers is not None or not self._headers_from_dicts) and not self.add_columns:
                self._logger.warning(f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
            else:
                result = self.new_headers(new_headers)
                if result is None:
                    self._logger.warning(f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: values will be discarded")
                else:
                    if self._combined_headers is None:
                        self._combined_headers = [header for header in self.headers]

                    if result:
                        self._logger.log(logging.WARNING if self.add_columns == 'warn' else logging.INFO, f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} added")
                    else:
                        col_min = len(self._combined_headers) + 1
                        col_max = col_min + len(new_headers) - 1
                        self._logger.log(logging.WARNING if self.add_columns == 'warn' else logging.INFO, f"Row {self.exported_count}: new header{'s' if len(new_headers) > 1 else ''} {names}: new column{'s' if len(new_headers) > 1 else ''} {col_min}{'-'+str(col_max) if len(new_headers) > 1 else ''} added without a column name")
                        
                    for i, key in enumerate(new_keys):
                        header = new_headers[i]
                        self._combined_headers.append(header)                    
                        combined_row.append(obj.get(key))

        if missing_paths:
            missing_paths_str = ', '.join(f'"{k}"' for k in missing_paths)
            self._logger.warning(f"Row {self.exported_count}: dict misses key{'s' if len(missing_paths) > 1 else ''} {missing_paths_str}")

        return combined_row

    def _convert_combined_row(self, row: Iterable):
        headers = self._combined_headers if self._combined_headers is not None else self.headers
        return [self._convert_value(value, headers[index] if headers is not None and index < len(headers) else None) for index, value in enumerate(row)]

    def _convert_value(self, value: Any, header: Header|None):
        if header:
            value = header.to_str(value, after1970=self.after1970)
        
        if self.notz and isinstance(value, (datetime,time)):            
            if value.tzinfo:
                value = value.astimezone(None if self.notz is True or self.notz == 'localtime' else self.notz).replace(tzinfo=None)
            else:
                pass # value is already naive (we consider it expressed as localtime)

        return value
    
    def _get_headers_from_first_dict_row(self, row: Row|dict):
        return self._get_headers_from_all_dict_rows([row])

    def _get_headers_from_all_dict_rows(self, rows: Iterable[Iterable|Row|dict]):
        headers = []
        header_keys = []
        remaining_optionals = [*self.optional] if isinstance(self.optional, list) else None

        def insert_header(header: Header, following_keys: list[str]):
            # Try to keep the global order of headers: insert just before the first existing that we know is after the given header
            pos = len(headers)
            for following_key in following_keys:
                try:
                    pos = header_keys.index([following_key])
                    break
                except ValueError:
                    continue

            headers.insert(pos, header)

            # Update other caches
            header_keys.insert(pos, header.key)
            if remaining_optionals:
                try:
                    remaining_optionals.remove(header.name)
                except:
                    pass
            
        for row in rows:
            if isinstance(row, Row):
                row = row.as_dict()
            elif not isinstance(row, dict):
                continue

            dict_keys = list(row.keys())
            for i, dict_key in enumerate(dict_keys):
                if [dict_key] in header_keys:
                    continue
                
                value = row[dict_key]
                if isinstance(value, dict):
                    dict_headers = self._get_dict_value_foreign_headers(dict_key, value.keys())
                    for header in dict_headers:
                        if not header.key in header_keys:
                            insert_header(header, dict_keys[i+1:])
                else:
                    insert_header(Header(dict_key), dict_keys[i+1:])

        if remaining_optionals:
            for optional in remaining_optionals:
                headers.append(Header(name=optional))
        
        return headers

    @classmethod
    def _get_dict_value_foreign_headers(cls, foreign_name: str, dict_keys):
        """
        Inline the keys of a dictionnary value as top-level header keys, if it is considered as a foreign key:
        - First key is either 'id', 'pk', or 'ref'.
        - optionaly, second key is either 'name', 'title', 'text' or '__str__'.
        - If there are other keys, an additional column named as the original one is kept.
        """
        if not isinstance(foreign_name, str):
            yield Header(foreign_name)
            return
        
        it = iter(dict_keys)

        try:
            id_dict_key = next(it)
        except StopIteration:
            yield Header(foreign_name)
            return
        
        if id_dict_key in cls.foreign_id_keys:
            yield Header(f'{foreign_name}_{id_dict_key}', key=[foreign_name, id_dict_key])
        else:
            yield Header(foreign_name)
            return
        
        try:
            name_dict_key = next(it)
        except StopIteration:
            return
                
        remaining = False
        if name_dict_key in cls.foreign_name_keys:
            yield Header(f'{foreign_name}_{name_dict_key}', key=[foreign_name, name_dict_key])
        else:
            remaining = True

        if not remaining:
            try:
                next(it)
                remaining = True
            except StopIteration:
                pass

        if remaining:
            yield Header(foreign_name)


class NoopDumper(TabularDumper[str]):   
    def __init__(self, *args, **kwargs):
        super().__init__(os.devnull)


class TabDumper(TabularDumper[Union[str,Path,IOBase]]):
    @classmethod
    def is_available(cls):
        try:
            import tabulate
            return True
        except ImportError:
            return False

    def __init__(self,
                 dst: str|Path|IOBase|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[Header|Any]|None = None,
                 truncate: bool = None,
                 archivate: bool|str|Path|None = None,
                 title: str|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|Path = None,
                 delay = False,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|Literal['warn'] = False,
                 after1970: bool|None = None,                 
                 notz: bool|tzinfo|str|None = None,
                 # TabDumper and CsvDumper options
                 encoding: str|None = None,
                 intfmt: str|None = None,
                 floatfmt: str|None = None,
                 # Destination mask values
                 **kwargs):
        
        if dst is None:
            dst = sys.stdout
        
        super().__init__(dst, headers=headers, truncate=truncate, archivate=archivate, title=title, dst_name=dst_name, dir=dir, delay=delay, optional=optional, add_columns=add_columns, after1970=after1970, notz=notz, **kwargs)

        self.encoding = 'utf-8' if encoding is None else encoding
        self.intfmt = intfmt
        self.floatfmt = floatfmt

        self._rows: list[Iterable] = []

    def close(self):
        super().close()

        from tabulate import tabulate
        if self.headers:
            intfmts = []
            floatfmts = []
            for header in self.headers:
                intfmts.append(self.intfmt or 'n')
                floatfmt = header._get_tab_floatfmt() or self.floatfmt
                floatfmts.append(floatfmt or 'n')
            result = tabulate(self._rows, headers=self.headers, intfmt=tuple(intfmts), floatfmt=tuple(floatfmts))
        else:
            result = tabulate(self._rows)

        file = None
        try:
            if isinstance(self.dst, IOBase):
                file = self.dst
            else:
                from zut import files
                
                if self.archivate:
                    files.archivate(self.dst, archive_dir=self.archivate, missing_ok=True)

                if self.truncate:
                    files.remove(self.dst, missing_ok=True)

                parent = files.dirname(self.dst)
                if parent and not files.exists(parent):
                    files.makedirs(parent)

                file = files.open(self.dst, 'a', encoding=self.encoding, newline='')
                skip_utf8_bom(file, self.encoding)

            file.write(result)
            file.write('\n')
            file.flush()
        finally:
            if file and not isinstance(self.dst, IOBase):
                file.close()

    def export(self, row: list):
        self._rows.append(row)

    def _convert_value(self, value: Any, header: Header|None):
        value = super()._convert_value(value, header)

        if value is None:
            return None
    
        elif isinstance(value, (Enum,Flag)):
            return value.name

        return value


class CsvDumper(TabularDumper[Union[str,Path,IOBase]]):
    def __init__(self,
                 dst: str|Path|IOBase|None = None,
                 *,
                 # Common TabularDumper options
                 headers: Iterable[str|Header]|None = None,
                 truncate: bool = None,
                 archivate: bool|str|Path|None = None,
                 title: str|bool|None = None,
                 dst_name: str|bool = True,
                 dir: str|Path = None,
                 delay = False,
                 optional: str|Sequence[str]|Literal['*',True]|None = None,
                 add_columns: bool|Literal['warn'] = False,
                 after1970: bool|None = None,                 
                 notz: bool|tzinfo|str|None = None,
                 # TabDumper and CsvDumper options
                 encoding: str|None = None,
                 intfmt: str|None = None,
                 floatfmt: str|None = None,
                 # CsvDumper options
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,                 
                 quotechar: str|None = None,
                 nullval: str|None = None,
                 excel: bool|None = None,
                 # Destination mask values
                 **kwargs):
                
        self.csvfmt = CsvFormat(encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, excel=excel)
        
        if notz is None:
            notz = True if self.csvfmt.excel else False

        if dst is None:
            dst = sys.stdout
        
        super().__init__(dst, headers=headers, truncate=truncate, archivate=archivate, title=title, dst_name=dst_name, dir=dir, delay=delay, optional=optional, add_columns=add_columns, after1970=after1970, notz=notz, **kwargs)
        
        self.intfmt = intfmt
        self.floatfmt = floatfmt

        if isinstance(self.dst, IOBase):
            self.file = self.dst
        elif isinstance(self.dst, (str,Path)):
            self.file = None # will be opened when necessary
        else:
            raise TypeError(f"dst: {type(self.dst).__name__}")

        # Manage existing headers
        self._need_newline = False

    def open(self):
        if not self.file:
            from zut import files
            
            if self.archivate:
                files.archivate(self.dst, archive_dir=self.archivate, missing_ok=True)

            if self.truncate:
                files.remove(self.dst, missing_ok=True)

            parent = files.dirname(self.dst)
            if parent and not files.exists(parent):
                files.makedirs(parent)

            if files.exists(self.dst):
                self.csvfmt.examine_file(self.dst, need_ends_with_newline=True)
                existing_headers = self.csvfmt.headers
                if existing_headers and not self.csvfmt.ends_with_newline:
                    self._need_newline = True
            else:
                existing_headers = None
            
            self.file = files.open(self.dst, 'a', encoding=self.csvfmt.encoding, newline='')
            skip_utf8_bom(self.file, self.csvfmt.encoding)

            return existing_headers

    def export_headers(self, headers: list[Header]):
        self.export([header.name for header in headers])

    def close(self):
        super().close()

        if self.file and not isinstance(self.dst, IOBase):
            self.file.close()
            self.file = None

    def export(self, row: list):        
        if self._need_newline:
            # CSV newline is standardized for all platforms to '\r\n' as recommended by RFC 4180 (see https://www.rfc-editor.org/rfc/rfc4180#section-2)
            # NOTE: when writing to piped stdout or stderr on Windows, we need to use '\n' (which will be converted to '\r\n' automatically) otherwise an additional empty newline is added
            if sys.platform == 'win32' and (self.dst == sys.stdout or self.dst == sys.stderr) and not self.dst.isatty():
                self.file.write('\n') 
            else:
                self.file.write('\r\n') 

        for i, value in enumerate(row):
            if i > 0:
                self.file.write(self.csvfmt.delimiter)
            self.file.write(self.csvfmt.escape_value(value))

        self.file.flush()

        self._need_newline = True

    def _convert_value(self, value, header: Header|None):
        value = super()._convert_value(value, header)

        if value is None:
            return None
    
        elif isinstance(value, (Enum,Flag)):
            return value.value
            
        elif isinstance(value, bool):
            return 'true' if value else 'false'
        
        elif isinstance(value, int):                
            if self.intfmt:
                value = format(value, self.intfmt)

            return value
        
        elif isinstance(value, (float,Decimal)):
            if header:
                floatfmt = header._get_csv_floatfmt() or self.floatfmt
                if floatfmt:
                    value = format(value, floatfmt)

            if self.csvfmt.decimal_separator != '.':
                return str(value).replace('.', self.csvfmt.decimal_separator)
            
            return value
        
        elif isinstance(value, (datetime,time)):
            if self.csvfmt.excel:
                # No microseconds
                return value.strftime('%H:%M:%S' if isinstance(value, time) else '%Y-%m-%d %H:%M:%S')
            
            return value
        
        elif isinstance(value, (list,tuple)):
            if is_iterable_of(value, (str,int,float,Decimal,bool,type(None),date,datetime,time,Enum,Flag)):
                if len(value) == 0:
                    return None
                
                elif len(value) == 1:
                    return self._convert_value(value[0], header)
                
                else:                    
                    parts = []
                    for element in value:
                        if element is None:
                            part = ''
                        else:                      
                            part = self._convert_value(element, header)
                            if isinstance(part, str) and '|' in part:                            
                                return str(value) # cancel
                                
                        parts.append(part)

                    return '|'.join(str(part) for part in parts)
                
            return value
                
        else:
            return value

#endregion


#region Load

def load_tabular(src: str|Path, *,
             headers: Iterable[str|Header]|None = None,
             encoding: str|None = None,
             delimiter: str|None = None,
             decimal_separator: str|None = None,
             quotechar: str|None = None,
             nullval: str|None = None,
             no_headers: bool|None = None,
             dir: str|Path = None,
             **kwargs):
    
    data: list[Row] = []
    with tabular_loader(src, headers=headers, encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, no_headers=no_headers, dir=dir, **kwargs) as table:
        for row in table:
            data.append(row)
    return data


def tabular_loader(src: str|Path, *,
             headers: Iterable[str|Header]|None = None,
             encoding: str|None = None,
             delimiter: str|None = None,
             decimal_separator: str|None = None,
             quotechar: str|None = None,
             nullval: str|None = None,
             no_headers: bool|None = None,
             dir: str|Path = None,
             **kwargs):

    from zut import files

    src = files.indir(src, dir=dir, **kwargs)
    return CsvLoader(src=src, headers=headers, encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, no_headers=no_headers)
    

class CsvLoader(ColumnsProvider):
    def __init__(self, src: str|Path, *,
                 headers: Iterable[str|Header]|None = None,
                 add_columns: bool|Literal['warn'] = False,
                 encoding: str|None = None,
                 delimiter: str|None = None,
                 decimal_separator: str|None = None,
                 quotechar: str|None = None,
                 nullval: str|None = None,
                 no_headers: bool|None = None):
                
        super().__init__()
        self._headers: list[Header] = None
        self.add_columns = add_columns
        if headers is not None:
            self._headers = []
            for header in headers:
                if header == '*':
                    self.add_columns = True
                else:
                    self._headers.append(Header(header) if not isinstance(header, Header) else header)
                    
        self.csvfmt = CsvFormat(encoding=encoding, delimiter=delimiter, decimal_separator=decimal_separator, quotechar=quotechar, nullval=nullval, no_headers=no_headers)

        self.src = src
        self.count = 0
        
        self._logger = logging.getLogger(f'{__name__}.{self.__class__.__qualname__}')

        # Set during prepare()
        self.file: IOBase = None
        self.src_columns: list[str]|None = None
        self._src_values_reorder: list[int]|None = None
        self._reader: csv._reader = None

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type = None, exc_value = None, exc_traceback = None):
        self.close()

    def prepare(self):
        # NOTE: QUOTE_STRINGS is not interpreted correctly by csv reader in Python 3.12 - https://github.com/python/cpython/issues/116633
        from zut import files

        self.file = files.open(self.src, 'r', encoding=self.csvfmt.encoding, newline='')
        self.csvfmt.examine_file(self.file, file_name=self.src)
        self._reader = csv.reader(self.file, delimiter=self.csvfmt.delimiter, quotechar=self.csvfmt.quotechar) #ROADMAP: custom CSV reader so that we can differenciate empty strings (=> None) from quoted empty string (=> "")
        
        if not self.csvfmt.no_headers:
            self.src_columns = next(self._reader)

        if self._headers is None:
            if self.csvfmt.no_headers:
                raise ValueError("Headers must be explicitely given if they are not in source")
            else:
                self._headers = [Header(name) for name in self.src_columns]
        else:
            if self.csvfmt.no_headers:
                self.src_columns = [header.name for header in self._headers]
            else:
                warn_not_found = []
                for header in self._headers:
                    if not header.name in self.src_columns:
                        warn_not_found.append(f'"{header}"')
                if warn_not_found:
                    self._logger.warning(f"Header{'s' if len(warn_not_found) > 1 else ''} {', '.join(warn_not_found)} not found in source")

                warn_additional = []
                reorder = [None] * len(self.src_columns)
                reorder_has_diff = False
                for src_i, column in enumerate(self.src_columns):
                    index = None
                    for i, header in enumerate(self._headers):
                        if header.name == column:
                            index = i
                            break
                    if index is not None:
                        reorder[src_i] = index
                        if index != src_i:
                            reorder_has_diff = True
                    else:
                        if self.add_columns == 'warn':
                            warn_additional.append(f'"{column}"')
                            reorder_has_diff = True
                        elif self.add_columns:
                            self._headers.append(Header(column))
                            index = len(self._headers) - 1
                            reorder[src_i] = index
                            if index != src_i:
                                reorder_has_diff = True

                if reorder_has_diff:
                    self._src_values_reorder = reorder
                
                if warn_additional:
                    self._logger.warning(f"Additional column{'s' if len(warn_additional) > 1 else ''} {', '.join(warn_additional)} found in source")
    
    def close(self):
        if self.file:
            self.file.close()
            self.file = None

    def __iter__(self):
        return self

    def __next__(self):
        if not self._reader:
            self.prepare()
        
        src_values = next(self._reader)
        self.count += 1
        
        if len(src_values) > len(self.src_columns):
            start_column_num = len(self.src_columns)
            end_column_num = len(src_values) - 1
            self._logger.warning(f"Row {self.count}: unexpected additional column{'s' if end_column_num > start_column_num else ''} {start_column_num}{f'-{end_column_num}' if end_column_num > start_column_num else ''}")

        elif len(src_values) < len(self.src_columns):
            start_column_num = len(src_values)
            end_column_num = len(self.src_columns) - 1
            self._logger.warning(f"Row {self.count}: missing column{'s' if end_column_num > start_column_num else ''} {start_column_num}{f'-{end_column_num}' if end_column_num > start_column_num else ''}")
            while len(src_values) < len(self.src_columns):
                src_values.append(None)

        if self._src_values_reorder:
            values = [None] * len(self._headers)
            for src_i, value in enumerate(src_values):
                index = self._src_values_reorder[src_i]
                if index is not None:
                    values[index] = value
        else:
            values = src_values

        return Row(self, values, nullval=self.csvfmt.nullval)

#endregion


#region Colors

class Color:
    RESET = '\033[0m'

    BLACK = '\033[0;30m'
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[0;33m'
    BLUE = '\033[0;34m'
    PURPLE = '\033[0;35m'
    CYAN = '\033[0;36m'
    WHITE = '\033[0;37m'
    GRAY = '\033[0;90m'
    BG_RED = '\033[0;41m'

    # Disable coloring if environment variable NO_COLORS is set to 1 or if stderr is piped/redirected
    NO_COLORS = False
    if is_truthy(os.environ.get('NO_COLORS')) or not sys.stderr.isatty():
        NO_COLORS = True
        for _ in dir():
            if isinstance(_, str) and _[0] != '_' and _ not in ['DISABLED']:
                locals()[_] = ''

    # Set Windows console in VT mode
    if not NO_COLORS and sys.platform == 'win32':
        _kernel32 = ctypes.windll.kernel32
        _kernel32.SetConsoleMode(_kernel32.GetStdHandle(-11), 7)
        del _kernel32

#endregion


#region Errors

class NotFoundError(Exception):
    pass

class SeveralFoundError(Exception):
    pass

class NotAvailableError(Exception):
    pass

class ParseError(Exception):
    pass

class SimpleError(ValueError):
    """
    An error that should result to only an error message being printed on the console, without a stack trace.
    """
    def __init__(self, msg: str, *args, **kwargs):
        if args or kwargs:
            msg = msg.format(*args, **kwargs)
        super().__init__(msg)


def check_completed_subprocess(cp: CompletedProcess, logger: logging.Logger = None, *, label: str = None, level: int|str = None, accept_returncode: int|list[int]|bool = False, accept_stdout: bool = False, accept_stderr: bool = False, maxlen: int = 200):
    """
    Improve message in case of subprocess error, to ease debugging.
    """
    if not label:
        label = cp.args[0]

    if not logger and level is not None:
        logger = globals()["logger"]
    elif logger and level is None:
        level = logging.ERROR


    def is_returncode_issue(returncode: int):
        if accept_returncode is True:
            return False
        elif isinstance(accept_returncode, int):
            return returncode != accept_returncode
        elif isinstance(accept_returncode, (list,tuple)):
            return returncode not in accept_returncode
        else:
            return returncode != 0
    

    def extract_stream(content: str|bytes|None, name: str, color: str):
        if content is None:
            return None
        elif not isinstance(content, str):
            try:
                content = content.decode('utf-8')
            except UnicodeDecodeError:
                content = content.decode('cp1252')
        
        data = content.strip()
        if maxlen and len(data) > maxlen:
            data = data[0:maxlen] + 'â¦'

        result = ''
        for line in data.splitlines():
            result += f"\n{color}[{label} {name}]{Color.RESET} {line}"
        return result
    

    issue = False

    if is_returncode_issue(cp.returncode):
        message = f"{label} returned {Color.YELLOW}code {cp.returncode}{Color.RESET}"
        issue = True
    else:
        message = f"{label} returned {Color.CYAN}code {cp.returncode}{Color.RESET}"
    

    result = extract_stream(cp.stdout, 'stdout', Color.CYAN if accept_stdout else Color.YELLOW)
    if result:
        message += result
        if not accept_stdout:
            issue = True

    result = extract_stream(cp.stderr, 'stderr', Color.CYAN if accept_stderr else Color.YELLOW)
    if result:
        message += result
        if not accept_stderr:
            issue = True

    if issue:
        if logger:
            logger.log(level, message)
        else:
            raise SubprocessError(message)
    else:
        if logger:
            logger.log(logging.DEBUG, message)

    return issue

#endregion


#region Commands

def add_package_commands(subparsers: _SubParsersAction[ArgumentParser], package: ModuleType|str):
    """
    Add all sub modules of the given package as commands.
    """
    if not isinstance(package, ModuleType):
        package = import_module(package)

    if not hasattr(package, '__path__'):
        raise ValueError(f"Module is not a package: {package.__name__}")

    for module_info in iter_modules(package.__path__):
        if module_info.name.startswith('_'):
            continue # skip
        
        module = import_module(f'{package.__name__}.{module_info.name}')
        add_command(subparsers, module)

    
def add_command(subparsers: _SubParsersAction[ArgumentParser], handle: FunctionType|ModuleType, *, name: str|None = None, doc: str|None = None, help: str|None = None, **defaults):
    """
    Add the given function or module as a command.
    """
    if isinstance(handle, ModuleType):
        module = handle

        if name is None:
            name = module.__name__.split('.')[-1]
            if name.endswith('cmd') and len(name) > len('cmd'):
                name = name[0:-len('cmd')]

        handle = getattr(module, 'handle', None)
        if handle is None:
            handle = getattr(module, '_handle', None)
        if handle is None:
            handle = getattr(module, name, None)
        if handle is None:
            raise ValueError(f"Cannot use module {module.__name__} as a command: no function named \"handle\" or \"{name}\"")
        
        if doc is None:
            if module.__doc__ and not handle.__doc__:
                doc = module.__doc__
        
        return add_command(subparsers, handle, name=name, doc=doc, help=help, **defaults)
    
    if name is None:
        name = handle.__name__
    
    if doc is None:
        doc = handle.__doc__
        add_doc = getattr(handle, 'add_doc', None)
        if add_doc:
            doc = (doc if doc else '') + add_doc()

    if help is None:
        help = doc

    cmdparser = subparsers.add_parser(name, help=get_help_text(help) if help else None, description=get_description_text(doc) if doc else None, formatter_class=RawTextHelpFormatter)

    add_arguments = getattr(handle, 'add_arguments', None)
    if add_arguments:
        add_arguments(cmdparser)

    already_has_help = False
    for action in cmdparser._actions:
        if '--help' in action.option_strings:
            already_has_help = True
            break

    if not already_has_help:
        group = cmdparser.add_argument_group(title='info')
        group.add_argument('-h', '--help', action='help', help=f"Show this help message and exit.")

    cmdparser.set_defaults(handle=handle, **defaults)

    return cmdparser


def get_help_text(doc: str):
    if doc is None:
        return None
    
    doc = doc.strip()
    try:
        return doc[0:doc.index('\n')].strip()
    except:
        return doc
    

def get_description_text(docstring: str):
    if docstring is None:
        return None
    
    return dedent(docstring)


def get_exit_code(return_value: Any) -> int:
    if not isinstance(return_value, int):
        return_value = 0 if return_value is None or return_value is True else 1
    return return_value


def run_command(handle: ArgumentParser|Callable, args: dict = {}, *, default: Callable|None = None):
    if isinstance(handle, ArgumentParser):            
        args = {**args, **vars(handle.parse_args())}
        handle = args.pop('handle', default)
    else:
        if default is not None:
            raise ValueError("Argument 'default' is applicable only when 'handle' is an ArgumentParser object.")
    
    if not handle:
        _logger.error("Missing command")
        return 1

    try:
        r = handle(**args)
        return get_exit_code(r)
    except SimpleError as err:
        _logger.error(str(err))
        return 1
    except KeyboardInterrupt:
        _logger.error("Interrupted")
        return 1
    except BaseException as err:
        message = str(err)
        _logger.exception(f"{type(err).__name__}{f': {message}' if message else ''}")
        return 1


def exec_command(handle: Callable, args: dict = {}, *, default: Callable|None = None):
    r = run_command(handle, args, default=default)
    sys.exit(r)

#endregion


#region Config

class ExtendedConfigParser(ConfigParser):
    list_separator: str|None = None

    def __init__(self, *args, list_separator: str|None = None, **kwargs):
        super().__init__(*args, **kwargs)
        self.list_separator = list_separator if list_separator is not None else self.__class__.list_separator

    def getlist(self, section: str, option: str, *, raw=False, vars=None, separator=None, fallback: list[str] = _UNSET) -> list[str]:
        values_str = self.get(section, option, raw=raw, vars=vars, fallback=fallback)
        if not isinstance(values_str, str):
            return values_str # fallback
    
        if not values_str:
            return []
        
        if separator is None:
            separator = self.list_separator
        
        if separator:
            values = values_str.split(separator)
        else:
            values = re.split(r'[\W,;|]+', values_str)
        
        result = []
        for value in values:
            value = value.strip()
            if not value:
                continue
            result.append(value)

        return result


def get_variable(name: str, fallback: str|None = None):
    """
    Read an environment variable.
    """
    value = os.environ.get(name, None)
    if value is not None:
        return value
    
    return fallback


def get_bool_variable(name: str, fallback: bool|None = None):
    """
    Read an environment variable as a boolean value.
    """
    value = os.environ.get(name, '')
    if value != '': # we don't accept '' (not a boolean => will return None by default)
        value = value.lower()
        if value in RawConfigParser.BOOLEAN_STATES:
            return RawConfigParser.BOOLEAN_STATES[value]
        else:
            raise ValueError(f"Invalid value for variable \"{name}\": not a boolean")
    
    return fallback


def get_int_variable(name: str, fallback: int|None = None):
    """
    Read an environment variable as an integer value.
    """
    value = os.environ.get(name, '')
    if value != '': # we don't accept '' (not an integer => will return None by default)
        try:
            return int(value)
        except ValueError:
            raise ValueError(f"Invalid value for variable \"{name}\": not an integer")
    
    return fallback


def get_list_variable(name: str, fallback: list[str]|None = None, *, sep = ','):
    """
    Read an environment variable as a list value.
    """
    value = os.environ.get(name, None)
    if value is not None:
        values: list[str] = []
        for v in value.split(sep):
            v = v.strip()
            if v:
                values.append(v)
        return values
    
    return fallback


def get_secret_variable(name: str, fallback: str|None = None):
    """
    Read a secret value from an environment variable if given, or from the file indicated by environment variable
    `{name}_FILE` if given, or from the file `/run/secrets/{name}` if the application is running in a Docker container.
    """
    from zut import files

    value = os.environ.get(name, None)
    if value is not None:
        return value
    
    file = os.environ.get(f'{name}_FILE', None)
    if file is not None and files.exists(file):
        with files.open(file, 'r', encoding='utf-8-sig') as fp:
            return fp.read()

    if in_docker_container():
        file = f'/run/secrets/{name.lower()}'
        if files.exists(file):
            with files.open(file, 'r', encoding='utf-8-sig') as fp:
                return fp.read()

    return fallback


_in_docker_container: bool|None = None

def in_docker_container():
    """
    Indicate whether the application is running in a Docker container.
    """
    global _in_docker_container
    if _in_docker_container is None:
        _in_docker_container = os.path.exists('/.dockerenv')
    return _in_docker_container


def load_env(path: Path|str = None, *, encoding = 'utf-8', override = False, from_dir = False):
    """
    Load the given file (or `.env` file found from the given or current directory) to environment variables.

    - If `from_dir` is `True`, `path` is a directory or file from which we will try to find the `.env` file.

    Usage example:

    ```
    # Load configuration file
    load_env() # ... from the current working directory or its parents
    load_env(__file__, from_dir=True) #... from the Python module installation directory or its parents
    load_env(f'C:\\ProgramData\\my-app\\my-app.env' if sys.platform == 'win32' else f'/etc/my-app/my-app.env') # .. from system configuration directory
    ```
    """
    if not path or os.path.isdir(path) or from_dir:
        path = find_to_root('.env', start_dir=path)
        if not path:
            return None # not found

    if not os.path.isfile(path):
        return None # does not exist
    
    with open(path, 'r', encoding=encoding, newline=None) as fp:
        skip_utf8_bom(fp, encoding=encoding)
        for name, value in parse_properties(fp.read()):
            if not override:
                if name in os.environ:
                    continue
            os.environ[name] = value

    return path


def find_to_root(name: str, *, start_dir: str|Path = None):
    """
    Find the given file name from the given start directory (or current working directory if none given), up to the root.

    Return None if not found.
    """    
    if start_dir:            
        if not os.path.exists(start_dir):
            raise IOError('Starting directory not found')
        elif not os.path.isdir(start_dir):
            start_dir = os.path.dirname(start_dir)
    else:
        start_dir = os.getcwd()

    last_dir = None
    current_dir = os.path.abspath(start_dir)
    while last_dir != current_dir:
        path = os.path.join(current_dir, name)
        if os.path.exists(path):
            return path
        parent_dir = os.path.abspath(os.path.join(current_dir, os.path.pardir))
        last_dir, current_dir = current_dir, parent_dir

    return None


def parse_properties(content: str) -> Iterator[tuple[str,str]]:
    """
    Parse properties/ini/env file content.
    """
    def find_nonspace_on_same_line(start: int):
        pos = start
        while pos < len(content):
            c = content[pos]
            if c == '\n' or not c.isspace():
                return pos
            else:
                pos += 1
        return None

    def find_closing_quote(start: int):
        """ Return the unquoted value and the next position """
        pos = content.find('"', start)
        if pos == -1:
            return content[start:], None
        elif pos+1 < len(content) and content[pos+1] == '"': # escaped
            begining_content = content[start:pos+1]
            remaining_content, remaining_pos = find_closing_quote(pos+2)
            return begining_content + remaining_content, remaining_pos
        else:
            return content[start:pos], pos

    name = None
    value = '' # value being build (or name being build if variable `name` is None)
    i = find_nonspace_on_same_line(0)
    while i is not None and i < len(content):
        c = content[i]
        if c == '"':
            unquoted, end = find_closing_quote(i+1)
            value += unquoted
            if end is None:
                return
            i = end + 1
        elif c == '=' and name is None:
            name = value
            value = ''
            i += 1
        elif c == '\n':
            if name or value:
                yield (name, value) if name is not None else (value, '')
            name = None
            value = ''
            i += 1            
        elif c == '#': # start of comment
            if name or value:
                yield (name, value) if name is not None else (value, '')
            name = None
            value = ''
            pos = content.find('\n', i+1)
            if pos == -1:
                return
            else:
                i = pos + 1
        elif c.isspace(): # start of whitespace
            end = find_nonspace_on_same_line(i+1)
            if value:
                if end is None or content[end] in ({'#', '\n', '='} if name is None else {'#', '\n'}):
                    pass # strip end
                else:
                    value += content[i:end]
            i = end
        else:
            value += c
            i += 1

    if name or value:
        yield (name, value) if name is not None else (value, '')


#endregion
    

#region Logging

def configure_logging(*, level: str|int = None, manage_exit = True):
    config = get_logging_dict_config(level=level, manage_exit=manage_exit)
    logging.config.dictConfig(config)


def get_logging_dict_config(*, level: str|int = None, manage_exit = True):
    if isinstance(level, int):
        level = logging.getLevelName(level)
    
    config = {
        'version': 1,
        'disable_existing_loggers': False,
        'formatters': {
            'default': {
                'format': '%(levelname)s [%(name)s] %(message)s',
            },
        },
        'handlers': {
            'console': {
                'class': 'logging.StreamHandler',
                'formatter': 'default',
            },
        },
        'root': {
            'handlers': ['console'],
            'level': level if level else (os.environ.get('LOG_LEVEL', '').upper() or 'INFO'),
        },
        'loggers': {
            'django': { 'level': os.environ.get('DJANGO_LOG_LEVEL', '').upper() or 'INFO', 'propagate': False },
            'PIL': { 'level': 'INFO' },
            'celery.utils.functional': { 'level': 'INFO' },
            'smbprotocol': { 'level': 'WARNING' },
        },
    }

    if not Color.NO_COLORS:
        config['formatters']['colored'] = {
            '()': ColoredFormatter.__module__ + '.' + ColoredFormatter.__qualname__,
            'format': '%(levelcolor)s%(levelname)s%(reset)s %(gray)s[%(name)s]%(reset)s %(messagecolor)s%(message)s%(reset)s',
        }

        config['handlers']['console']['formatter'] = 'colored'

    
    if manage_exit:
        config['handlers']['exit'] = {
            'class': ExitHandler.__module__ + '.' + ExitHandler.__qualname__,
            'level': 'WARNING',
        }

        config['root']['handlers'].append('exit')

    return config


class ColoredRecord:
    LEVELCOLORS = {
        logging.DEBUG:     Color.GRAY,
        logging.INFO:      Color.CYAN,
        logging.WARNING:   Color.YELLOW,
        logging.ERROR:     Color.RED,
        logging.CRITICAL:  Color.BG_RED,
    }

    MESSAGECOLORS = {
        logging.INFO:      '',
        logging.CRITICAL:  Color.RED,
    }

    def __init__(self, record: logging.LogRecord):
        # The internal dict is used by Python logging library when formatting the message.
        # (inspired from library "colorlog").
        self.__dict__.update(record.__dict__)
        
        self.levelcolor = self.LEVELCOLORS.get(record.levelno, '')
        self.messagecolor = self.MESSAGECOLORS.get(record.levelno, self.levelcolor)

        for attname, value in Color.__dict__.items():
            if attname == 'NO_COLORS' or attname.startswith('_'):
                continue
            setattr(self, attname.lower(), value)


class ColoredFormatter(logging.Formatter):
    def formatMessage(self, record: logging.LogRecord) -> str:
        """Format a message from a record object."""
        wrapper = ColoredRecord(record)
        message = super().formatMessage(wrapper)
        return message


class ExitHandler(logging.Handler):
    """
    A logging handler that counts warnings and errors.
    
    If warnings and errors occured during the program execution, display counts at exit
    and set exit code to 68 (EADV) (if it was not explicitely set with `sys.exit` function).
    """
    _counts: dict[int, int] = {}
    _detected_exception: tuple[type[BaseException], BaseException, TracebackType|None] = None
    _detected_exit_code = 0

    _original_exit = sys.exit
    _original_excepthook = sys.excepthook

    _registered = False
    _logger: logging.Logger

    def __init__(self):
        if not self.__class__._registered:
            sys.exit = self.__class__._exit
            sys.excepthook = self.__class__._excepthook
            atexit.register(self.__class__._atexit)
            self.__class__._logger = logging.getLogger(f'{__name__}.{self.__class__.__qualname__}')
            self.__class__._registered = True
        
        super().__init__(level=logging.WARNING)

    def emit(self, record: logging.LogRecord):
        if record.levelno >= self.level:
            if not record.levelno in self.__class__._counts:
                self.__class__._counts[record.levelno] = 1
            else:
                self.__class__._counts[record.levelno] += 1
    
    @classmethod
    def _exit(cls, code: int = 0):
        cls._detected_exit_code = code
        cls._original_exit(code)
    
    @classmethod
    def _excepthook(cls, exc_type: type[BaseException], exc_value: BaseException, exc_traceback: TracebackType|None):
        cls._detected_exception = exc_type, exc_value, exc_traceback
        cls._original_exit(1)

    @classmethod
    def _atexit(cls):
        if cls._detected_exception:
            exc_type, exc_value, exc_traceback = cls._detected_exception

            msg = 'An unhandled exception occured\n'
            msg += ''.join(format_exception(exc_type, exc_value, exc_traceback)).strip()
            cls._logger.critical(msg)

        else:
            level = None
            msg = ''
            
            for levelno in sorted(cls._counts.keys(), reverse=True):
                if level is None:
                    level = levelno
                levelname = logging.getLevelName(levelno)
                msg += (', ' if msg else 'Logged ') + f'{levelname}: {cls._counts[levelno]:,}'

            if level is not None:
                cls._logger.log(level, msg)
                
                if level >= logging.WARNING:                
                    # Change exit code if it was not originally set explicitely to another value, using `sys.exit()`
                    if cls._detected_exit_code == 0:
                        if level == logging.WARNING:
                            exit_code = os.environ.get('WARNING_EXIT_CODE')
                            if exit_code is not None and exit_code.isdigit():
                                exit_code = int(exit_code)
                            else:
                                exit_code = 198
                        else:
                            exit_code = os.environ.get('ERROR_EXIT_CODE')
                            if exit_code is not None and exit_code.isdigit():
                                exit_code = int(exit_code)
                            else:
                                exit_code = 199
                        os._exit(exit_code)

#endregion


#region Files

def configure_smb_credentials(user: str = None, password: str = None, *, once = False):
    global _smb_credentials_configured
    
    from zut.files import _smb_credentials_configured, smbclient

    if once and _smb_credentials_configured is not None:
        return
    
    if not user:
        user = get_variable('SMB_USER')
        
    if not password:
        password = get_secret_variable('SMB_PASSWORD')

    if user or password:
        if not smbclient:
            raise ValueError(f"Package `smbprotocol` is required to specify smb credentials")
        smbclient.ClientConfig(username=user, password=password)
        _smb_credentials_configured = True
    else:
        _smb_credentials_configured = False

#endregion


#region Sort

def topological_sort(source: dict[T,None|T|Iterable[T]]) -> list[T]:
    """
    Perform a topological sort.

    - `source`: dictionnary associating keys to list of dependencies
    - returns a list of keys, sorted with dependencies first
    """
    #See: https://stackoverflow.com/a/11564323
    pending = [(key, set() if deps is None else (set([deps]) if isinstance(deps, type(key)) else set(deps))) for key, deps in source.items()] # copy deps so we can modify set in-place       
    emitted = []
    result = []

    while pending:
        next_pending = []
        next_emitted = []

        for entry in pending:
            key, deps = entry
            deps.difference_update(emitted) # remove deps we emitted last pass
            if deps: # still has deps? recheck during next pass
                next_pending.append(entry) 
            else: # no more deps? time to emit
                result.append(key)
                emitted.append(key) # <-- not required, but helps preserve original ordering
                next_emitted.append(key) # remember what we emitted for difference_update() in next pass

        if not next_emitted: # all entries have unmet deps, one of two things is wrong...
            raise ValueError("Cyclic or missing dependency detected: %r" % (next_pending,))
        
        pending = next_pending
        emitted = next_emitted

    return result

#endregion
