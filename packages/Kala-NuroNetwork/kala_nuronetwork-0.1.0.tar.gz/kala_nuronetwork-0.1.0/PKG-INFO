Metadata-Version: 2.1
Name: Kala_NuroNetwork
Version: 0.1.0
Summary: A hybrid quantum-classical neural network framework using Kala_Quantum and Kala_Torch
Home-page: https://github.com/Kalasaikamesh944/Kala_NuroNetwork
Author: N V R K SAI KAMESH YADAVALLI
Author-email: saikamesh.y@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: torch
Requires-Dist: Kala-Quantum-185
Requires-Dist: Kala_Torch


# Kala_NuroNetwork

Kala_NuroNetwork is a hybrid quantum-classical neural network framework that integrates Kala_Quantum and Kala_Torch for advanced machine learning tasks.

## Features

- **Quantum Layer**: Leverage quantum circuits with Hadamard and CNOT gates for preprocessing.
- **Classical Neural Network**: Includes fully connected layers for classical computation.
- **Trainer Class**: Train and evaluate models with ease.
- **Large Dataset Support**: Handle big data with efficient batching and parallelism.

## Installation

```bash
pip install Kala_Quantum Kala_Torch torch
```

## Usage

```python
from KalaNeroNetwork import KalaNuroNetwork, KalaNuroTrainer
import torch
import torch.nn as nn
import torch.optim as optim

# Define hyperparameters
input_size = 2
n_qubits = 2
hidden_size = 128
output_size = 2
batch_size = 128
epochs = 10

# Generate synthetic dataset
def generate_large_data(num_samples, input_size):
    data = torch.rand(num_samples, input_size)
    labels = (data.sum(axis=1) > 1.0).long()  # Binary classification based on sum threshold
    return data, labels

num_samples = 10000
data, labels = generate_large_data(num_samples, input_size)
dataset = torch.utils.data.TensorDataset(data, labels)
data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize model, optimizer, and criterion
model = KalaNuroNetwork(input_size, n_qubits, hidden_size, output_size)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Train and evaluate
trainer = KalaNuroTrainer(model, optimizer, criterion, device="cpu")

print("Starting training...")
trainer.train(data_loader, epochs)

print("Evaluating model...")
trainer.evaluate(data_loader)
```

## License

This project is licensed under the MIT License. See the LICENSE file for details.
