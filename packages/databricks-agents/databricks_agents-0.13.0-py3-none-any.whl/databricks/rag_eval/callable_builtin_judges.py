from typing import Any, Dict, List, Optional, Union

from mlflow import evaluation as mlflow_eval

from databricks.rag_eval import context, schemas
from databricks.rag_eval.config import assessment_config
from databricks.rag_eval.evaluation import entities
from databricks.rag_eval.mlflow import mlflow_utils


class CallableBuiltinJudge:
    """
    Callable object that can be used to evaluate inputs against
    the LLM judge service with the current assessment config.

    Args:
        config: The assessment config to use for the judge.
    """

    def __init__(self, config: assessment_config.BuiltinAssessmentConfig):
        self.config = config

    @context.eval_context
    def __call__(
        self,
        *,
        request: str,
        response: Optional[str] = None,
        retrieved_context: Optional[List[Dict[str, Any]]] = None,
        expected_response: Optional[str] = None,
        expected_retrieved_context: Optional[List[Dict[str, Any]]] = None,
        expected_facts: Optional[List[str]] = None,
        guidelines: Optional[List[str]] = None,
    ) -> Union[mlflow_eval.Assessment, List[mlflow_eval.Assessment]]:
        _validate_string_input("request", request)
        _validate_string_input("response", response)
        _validate_retrieved_context("retrieved_context", retrieved_context)
        _validate_string_input("expected_response", expected_response)
        _validate_retrieved_context(
            "expected_retrieved_context", expected_retrieved_context
        )
        _validate_repeated_string_input("expected_facts", expected_facts)
        _validate_repeated_string_input("guidelines", guidelines)
        managed_rag_client = context.get_context().build_managed_rag_client()
        eval_item = entities.EvalItem.from_dict(
            {
                schemas.REQUEST_COL: request,
                schemas.RESPONSE_COL: response,
                schemas.RETRIEVED_CONTEXT_COL: retrieved_context,
                schemas.EXPECTED_RESPONSE_COL: expected_response,
                schemas.EXPECTED_RETRIEVED_CONTEXT_COL: expected_retrieved_context,
                schemas.EXPECTED_FACTS_COL: expected_facts,
                schemas.GUIDELINES_COL: guidelines,
            }
        )
        assessment_result = managed_rag_client.get_assessment(
            eval_item=eval_item,
            config=self.config,
        )[0]
        return mlflow_utils.assessment_result_to_mlflow_assessments(assessment_result)


def _validate_string_input(param_name: str, input_value: Any) -> None:
    if input_value and not isinstance(input_value, str):
        raise ValueError(f"{param_name} must be a string. Got: {type(input_value)}")


def _validate_retrieved_context(
    param_name: str, retrieved_context: Optional[List[Dict[str, Any]]]
) -> None:
    if retrieved_context:
        if not isinstance(retrieved_context, list):
            raise ValueError(
                f"{param_name} must be a list of dictionaries. Got: {type(retrieved_context)}"
            )
        for context_dict in retrieved_context:
            if not isinstance(context_dict, dict):
                raise ValueError(
                    f"{param_name} must be a list of dictionaries. Got list of: {type(context_dict)}"
                )
            if "content" not in context_dict:
                raise ValueError(
                    f"Each context in {param_name} must have a 'content' key. Got: {context_dict}"
                )
            if set(context_dict.keys()) - {"doc_uri", "content"}:
                raise ValueError(
                    f"Each context in {param_name} must have only 'doc_uri' and 'content' keys. Got: {context_dict}"
                )


def _validate_repeated_string_input(param_name: str, input_value: Any) -> None:
    if input_value is None:
        return
    elif not isinstance(input_value, list):
        raise ValueError(f"{param_name} must be a list. Got: {type(input_value)}")

    for idx, value in enumerate(input_value):
        if not isinstance(value, str):
            raise ValueError(
                f"{param_name} must be a list of strings. Got: {type(value)} at index: {idx}"
            )


# use this docstring for the CallableBuiltinJudge class
CALLABLE_BUILTIN_JUDGE_DOCSTRING = """
        {judge_description}

        Args:
            request: Input to the application to evaluate, user’s question or query. For example, “What is RAG?”.
            response: Response generated by the application being evaluated.
            retrieved_context: Retrieval results generated by the retriever in the application being evaluated. 
                It should be a list of dictionaries with the following keys:
                    - doc_uri (Optional): The doc_uri of the context.
                    - content: The content of the context.
            expected_response: Ground-truth (correct) answer for the input request.
            expected_retrieved_context: Array of objects containing the expected retrieved context for the request 
                (if the application includes a retrieval step). It should be a list of dictionaries with the
                following keys:
                    - doc_uri (Optional): The doc_uri of the context.
                    - content: The content of the context.
            expected_facts: Array of strings containing facts expected in the correct response for the input request.
            guidelines: Array of strings containing the guidelines that the response should adhere to.
        Required input arguments:
            {required_args}

        Returns:
            Assessment result for the given input.
        """


# =================== Builtin Judges ===================

correctness = CallableBuiltinJudge(config=assessment_config.CORRECTNESS)
correctness.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The correctness LLM judge gives a binary evaluation and written "
    "rationale on whether the response generated by the agent is factually accurate and "
    "semantically similar to the provided expected response or expected facts.",
    required_args="request, response, oneof(expected_response, expected_facts)",
)
groundedness = CallableBuiltinJudge(config=assessment_config.GROUNDEDNESS)
groundedness.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The groundedness LLM judge returns a binary evaluation and written "
    "rationale on whether the generated response is factually consistent with the retrieved context.",
    required_args="request, response, retrieved_context",
)
safety = CallableBuiltinJudge(config=assessment_config.HARMFULNESS)
safety.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The safety LLM judge returns a binary rating and a written rationale "
    "on whether the generated response has harmful or toxic content.",
    required_args="request, response",
)
relevance_to_query = CallableBuiltinJudge(config=assessment_config.RELEVANCE_TO_QUERY)
relevance_to_query.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The relevance_to_query LLM judge determines whether the "
    "response is relevant to the input request.",
    required_args="request, response",
)
chunk_relevance = CallableBuiltinJudge(config=assessment_config.CHUNK_RELEVANCE)
chunk_relevance.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The chunk-relevance-precision LLM judge determines whether the chunks "
    "returned by the retriever are relevant to the input request. Precision is calculated as "
    "the number of relevant chunks returned divided by the total number of chunks returned. "
    "For example, if the retriever returns four chunks, and the LLM judge determines that three "
    "of the four returned documents are relevant to the request, then llm_judged/chunk_relevance/precision is 0.75.",
    required_args="request, retrieved_context",
)
context_sufficiency = CallableBuiltinJudge(config=assessment_config.CONTEXT_SUFFICIENCY)
context_sufficiency.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The context_sufficiency LLM judge determines whether the retriever "
    "has retrieved documents that are sufficient to produce the expected response or expected facts.",
    required_args="request, oneof(expected_response, expected_facts), retrieved_context",
)
guideline_adherence = CallableBuiltinJudge(config=assessment_config.GUIDELINE_ADHERENCE)
guideline_adherence.__doc__ = CALLABLE_BUILTIN_JUDGE_DOCSTRING.format(
    judge_description="The guideline_adherence LLM judge determines whether the response "
    "to the request adheres to the provided guidelines",
    required_args="request, response, guidelines",
)
