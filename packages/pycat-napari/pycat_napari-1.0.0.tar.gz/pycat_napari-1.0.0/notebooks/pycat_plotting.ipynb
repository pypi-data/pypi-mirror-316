{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import mplcursors\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats\n",
    "# Qt popups for matplots instead of inline plots in jupyter notebook\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the following cell if you want to use a file dialogue to choose the folders for files\n",
    "\n",
    "##### Note that each dataset (e.g. FUS FL, SS18 PLD, etc.) will require its variable be set in its own cell with this method\n",
    "\n",
    "FUS_FL_directory = select_directory()\n",
    "\n",
    "New Cell \n",
    "\n",
    "SS18_PLD_directory = select_directory() \n",
    "\n",
    "etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the Qt integration with Jupyter's event loop\n",
    "%gui qt\n",
    "\n",
    "from PyQt5.QtWidgets import QFileDialog\n",
    "\n",
    "def select_directory():\n",
    "    # No QApplication instance is needed here; %gui qt handles the integration\n",
    "    folder_path = QFileDialog.getExistingDirectory(None, \"Select Folder\")\n",
    "    return folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_combine_csv_files(root_directory, file_suffix, include_source_file=True):\n",
    "    \"\"\"\n",
    "    Reads CSV files with a specific suffix from all subdirectories within a given root directory,\n",
    "    optionally adds a source_file column indicating the relative path of the CSV file, combines\n",
    "    them into a single DataFrame, and returns it. This function navigates through the root directory \n",
    "    and its subdirectories, reads the CSV files that match the specified suffix, and optionally\n",
    "    adds metadata about the file's source path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root_directory : str\n",
    "        The path to the root directory containing subdirectories with CSV files.\n",
    "    file_suffix : str\n",
    "        The suffix that file names must end with to be included. This helps in filtering the relevant files.\n",
    "    include_source_file : bool, optional\n",
    "        If True, adds a 'source_file' column to the DataFrame indicating the relative path from\n",
    "        the root_directory to the file. Default is True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the combined data from all relevant CSV files. If no files are found\n",
    "        or all files are empty, returns an empty DataFrame.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Warning\n",
    "        If a file is empty or cannot be read, it logs a warning and skips that file.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function is designed to be robust in handling various errors such as file read errors or\n",
    "    non-existent paths. It is intended to be used in data preprocessing tasks where multiple CSV files\n",
    "    are to be consolidated into a single dataset for analysis.\n",
    "    \"\"\"\n",
    "    # List to hold the dataframes\n",
    "    dataframes = []\n",
    "\n",
    "\n",
    "    # Check files in the root directory\n",
    "    for filename in os.listdir(root_directory):\n",
    "        if filename.endswith(file_suffix):\n",
    "            file_path = os.path.join(root_directory, filename)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if not df.empty:\n",
    "                    if include_source_file:\n",
    "                        df['source_file'] = filename\n",
    "                    dataframes.append(df)\n",
    "                else:\n",
    "                    print(f\"Warning: '{file_path}' is empty and was skipped.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading '{file_path}': {e}\")\n",
    "\n",
    "    # Loop through each directory within the root_directory\n",
    "    for folder_name in os.listdir(root_directory):\n",
    "        directory = os.path.join(root_directory, folder_name)\n",
    "        if os.path.isdir(directory):  # Ensure it's a directory\n",
    "            # Loop through the files in the directory\n",
    "            for filename in os.listdir(directory):\n",
    "                if filename.endswith(file_suffix):\n",
    "                    # Construct the full file path\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    try:\n",
    "                        # Read the CSV file\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        if not df.empty:\n",
    "                            if include_source_file:\n",
    "                                # Add a column for the source file\n",
    "                                df['source_file'] = os.path.join(folder_name, filename)\n",
    "                            # Append the DataFrame to the list\n",
    "                            dataframes.append(df)\n",
    "                        else:\n",
    "                            print(f\"Warning: '{file_path}' is empty and was skipped.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading '{file_path}': {e}\")\n",
    "\n",
    "    # Combine all dataframes into one, if any were successfully added\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were read successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_combine_csv_files(root_directory, file_suffix, include_source_file=True):\n",
    "    dataframes = []\n",
    "\n",
    "    # Walk through all directories and subdirectories starting from the root\n",
    "    for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(file_suffix):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if not df.empty:\n",
    "                        if include_source_file:\n",
    "                            # Generate relative path from the root directory\n",
    "                            relative_path = os.path.relpath(file_path, root_directory)\n",
    "                            df['source_file'] = relative_path\n",
    "                        dataframes.append(df)\n",
    "                    else:\n",
    "                        print(f\"Warning: '{file_path}' is empty and was skipped.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading '{file_path}': {e}\")\n",
    "\n",
    "    # Combine all dataframes into one, if any were successfully added\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were read successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_combine_csv_files(root_directory, file_suffix, include_source_file=True):\n",
    "    dataframes = []\n",
    "\n",
    "    # Walk through all directories and subdirectories starting from the root\n",
    "    for dirpath, dirnames, filenames in os.walk(root_directory):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(file_suffix):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    if not df.empty:\n",
    "                        if include_source_file:\n",
    "                            # Generate relative path from the root directory\n",
    "                            relative_path = os.path.relpath(file_path, root_directory)\n",
    "                            df['source_file'] = relative_path\n",
    "\n",
    "                        # Append the DataFrame to the list\n",
    "                        dataframes.append(df)\n",
    "                        # Print file path and specified columns for debugging\n",
    "                        print(f\"Reading from {relative_path}\")\n",
    "                        #if 'label' in df.columns and 'intensity_total' in df.columns and 'puncta_intensity_total' in df.columns:\n",
    "                        #    print(df[['label', 'intensity_total', 'puncta_intensity_total']].head())\n",
    "                        #else:\n",
    "                        #    print(\"One or more specified columns are missing in this file.\")\n",
    "\n",
    "                    else:\n",
    "                        print(f\"Warning: '{file_path}' is empty and was skipped.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading '{file_path}': {e}\")\n",
    "\n",
    "    # Combine all dataframes into one, if any were successfully added\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no files were read successfully\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_relu(x, m, x0):\n",
    "    \"\"\"Generalized Rectified Linear Unit (ReLU) function, with the linear portion parameterized by the x-intercept.\"\"\"\n",
    "    return np.maximum(m * (x - x0), 0)\n",
    "\n",
    "def fit_c_sat_threshold(df_sorted, x_col, y_col, x_lim):\n",
    "    \"\"\"\n",
    "    Fits a generalized ReLU function to the x and y data specified in a DataFrame. This function is tailored\n",
    "    to optimize parameters for a model where the response begins at a certain threshold, considering data sorted\n",
    "    by the x values. The fitting process can be influenced by signal-to-noise ratio (SNR) as weights, though\n",
    "    this implementation requires adjustments to utilize SNR.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_sorted : pd.DataFrame\n",
    "        DataFrame containing the data, which must be pre-sorted based on the x_col values.\n",
    "    x_col : str\n",
    "        Name of the column in df_sorted to use as the x data.\n",
    "    y_col : str\n",
    "        Name of the column in df_sorted to use as the y data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    m_opt : float\n",
    "        Optimized slope parameter of the generalized ReLU function.\n",
    "    x0_opt : float\n",
    "        Optimized threshold parameter (x-offset) of the generalized ReLU function.\n",
    "    pcov : 2D array\n",
    "        Covariance matrix of the parameter estimates, indicative of the estimate uncertainties.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function assumes the presence of a 'standard deviation' (std_dev) which is used to influence\n",
    "    the bounds for the fit. If the dataset or selection of the x_col and y_col does not facilitate a\n",
    "    straightforward fitting, the function may fail to compute the parameters, handling errors by returning None.\n",
    "\n",
    "    The implementation expects numerical stability and finite values in the dataset. Handling for non-finite\n",
    "    values and other irregularities should be considered during pre-processing or within this function as needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        std_dev = df_sorted[x_col].std()\n",
    "\n",
    "        # Finding indices for first non-zero and last zero values in y_col\n",
    "        first_nonzero_idx = df_sorted[y_col].ne(0).idxmax()\n",
    "        last_zero_idx = df_sorted[df_sorted[y_col] == 0].index[-1]   \n",
    "        if first_nonzero_idx >= last_zero_idx:\n",
    "            # Swap the indices \n",
    "            first_nonzero_idx, last_zero_idx = last_zero_idx, first_nonzero_idx\n",
    "        \n",
    "        # Define target value based on standard deviation and find the max index for fitting\n",
    "        target_value = df_sorted[x_col].iloc[last_zero_idx] + 3*std_dev\n",
    "        soft_max_idx = df_sorted.index[-1] if target_value > df_sorted[x_col].max() else df_sorted[df_sorted[x_col] >= target_value].index[0]\n",
    "        hard_max_idx = df_sorted[df_sorted[x_col] > x_lim].index[0] if x_lim < df_sorted[x_col].max() else df_sorted.index[-1]\n",
    "\n",
    "        # Ensure x_data and y_data are 1D and finite\n",
    "        x_data = np.ravel(df_sorted[x_col][:hard_max_idx].to_numpy())\n",
    "        y_data = np.ravel(df_sorted[y_col][:hard_max_idx].to_numpy())\n",
    "\n",
    "        # Remove any rows with NaN or Inf values\n",
    "        finite_indices = np.isfinite(x_data) & np.isfinite(y_data)\n",
    "        x_data = x_data[finite_indices]\n",
    "        y_data = y_data[finite_indices]\n",
    "\n",
    "        # Check if 'gaussian_snr_estimate' column exists\n",
    "        if 'gaussian_snr_estimate' in df_sorted.columns:\n",
    "            # Extract the SNR data\n",
    "            snr_data = df_sorted['gaussian_snr_estimate']\n",
    "            \n",
    "            # Calculate initial weights based on SNR, handling NaNs by ignoring them initially\n",
    "            snr_weights = snr_data[first_nonzero_idx:last_zero_idx] / snr_data[first_nonzero_idx:last_zero_idx].max()\n",
    "            \n",
    "            # Normalize snr weights to be between 0.1 and 1, ignoring NaNs in the computation\n",
    "            snr_min = snr_weights.min(skipna=True)\n",
    "            snr_max = snr_weights.max(skipna=True)\n",
    "            snr_weights = 0.5 + 0.5 * (snr_weights - snr_min) / (snr_max - snr_min + 1e-8)\n",
    "\n",
    "            # Replace NaN values in normalized snr_weights with a default normalized value, e.g., 0.5\n",
    "            snr_weights.fillna(0.75, inplace=True)\n",
    "        else:\n",
    "            # If 'gaussian_snr_estimate' does not exist, use default weights\n",
    "            snr_weights = np.ones(len(df_sorted[first_nonzero_idx:last_zero_idx]))*0.75\n",
    "\n",
    "\n",
    "\n",
    "        # Determine weight adjustments based on signal-to-noise ratio (SNR)\n",
    "        #snr_data = df_sorted['gaussian_snr_estimate']\n",
    "        #snr_weights = snr_data[first_nonzero_idx:last_zero_idx] / snr_data[first_nonzero_idx:last_zero_idx].max()\n",
    "        # rescale snr weights to be between 0.1 and 1\n",
    "        #snr_weights = 0.1 + 0.9 * (snr_weights - snr_weights.min()) / (snr_weights.max() - snr_weights.min())\n",
    "\n",
    "        # Setup bounds for curve fitting\n",
    "        upper_bound_idx = df_sorted[df_sorted[x_col] > (df_sorted[x_col].iloc[last_zero_idx] + std_dev/2)].index[0]\n",
    "        # if the upper bound index is the last index, use last zero index instead\n",
    "        if upper_bound_idx == df_sorted.index[-1]:\n",
    "            upper_bound_idx = last_zero_idx\n",
    "            \n",
    "        bounds = ([-np.inf, df_sorted[x_col].iloc[first_nonzero_idx]], [np.inf, df_sorted[x_col].iloc[upper_bound_idx]])\n",
    "        print(bounds)\n",
    "        \n",
    "        '''        \n",
    "        # Initialize and adjust weights\n",
    "        weights = np.ones_like(x_data)\n",
    "        weights[:first_nonzero_idx] = np.linspace(1.0, 0.9, first_nonzero_idx)\n",
    "        weights[first_nonzero_idx:last_zero_idx] = snr_weights * (y_data[first_nonzero_idx:last_zero_idx] != 0)\n",
    "        weights[first_nonzero_idx:last_zero_idx] = 0.75 * (y_data[first_nonzero_idx:last_zero_idx] == 0)\n",
    "\n",
    "\n",
    "        # Additional weight adjustments if needed\n",
    "        if soft_max_idx < hard_max_idx:\n",
    "            weights_array2 = np.linspace(1.0, 0.9, len(x_data[soft_max_idx:hard_max_idx]))\n",
    "            weights[soft_max_idx:] = weights_array2\n",
    "\n",
    "\n",
    "        # Ensure weights are 1D if used\n",
    "        weights = np.ravel(weights)\n",
    "        '''\n",
    "        \n",
    "        weights = np.ones_like(x_data)\n",
    "\n",
    "        print(x_data.shape, y_data.shape, weights.shape)\n",
    "\n",
    "        # Fit the generalized ReLU model with weighted data\n",
    "        popt, pcov = curve_fit(generalized_relu, x_data, y_data, bounds=bounds, sigma=1/(weights + 1e-8), absolute_sigma=True)\n",
    "\n",
    "        # Extract optimized parameters\n",
    "        m_opt, x0_opt = popt\n",
    "\n",
    "        return m_opt, x0_opt, pcov\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during curve fitting: {e}\")\n",
    "        return None, None, None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(x_data, y_data, y_pred, m_opt, x0_opt):\n",
    "    \"\"\"\n",
    "    Calculates the confidence intervals for the predicted values of a fitted line.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : array-like\n",
    "        Independent variable values used in the fitting.\n",
    "    y_data : array-like\n",
    "        Observed values corresponding to x_data.\n",
    "    y_pred : array-like\n",
    "        Predicted values from the fitted model for x_data.\n",
    "    m_opt : float\n",
    "        Optimized slope parameter from the fit.\n",
    "    x0_opt : float\n",
    "        Optimized intercept parameter from the fit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_lower : array-like\n",
    "        Lower bounds of the 95% confidence interval for the predicted values.\n",
    "    y_upper : array-like\n",
    "        Upper bounds of the 95% confidence interval for the predicted values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The confidence intervals are calculated assuming normally distributed errors and use\n",
    "    the Student's t-distribution for the critical value. This method calculates point-wise\n",
    "    confidence intervals, which provide an estimated range that the true values are expected\n",
    "    to fall into with 95% confidence.\n",
    "    \"\"\"\n",
    "    # Confidence Interval Calculation and Plotting\n",
    "    alpha = 0.01  # 95% confidence interval\n",
    "    n = len(y_data)\n",
    "    p = len([m_opt, x0_opt])\n",
    "    dof = max(0, n - p)  # degrees of freedom\n",
    "    t_stat = scipy.stats.t.ppf(1 - alpha / 2, dof)\n",
    "    \n",
    "    # Standard error of the prediction\n",
    "    y_pred_std = np.sqrt(np.sum((y_data - y_pred)**2) / dof)\n",
    "    ci = t_stat * y_pred_std * np.sqrt(1/n + (x_data - np.mean(x_data))**2 / np.sum((x_data - np.mean(x_data))**2))\n",
    "    \n",
    "    y_upper = y_pred + ci\n",
    "    y_lower = y_pred - ci    \n",
    "\n",
    "    return y_lower, y_upper\n",
    "\n",
    "def calculate_r_squared(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the coefficient of determination, R^2, which quantifies the goodness of fit of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        True values for the dependent variable.\n",
    "    y_pred : array-like\n",
    "        Predicted values from the model for the same dependent variable.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R^2 : float\n",
    "        The coefficient of determination, a statistical measure of how well the regression predictions\n",
    "        approximate the real data points. An R^2 of 1 indicates perfect agreement.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    R^2 is a common measure of fit quality in linear regression and other modeling contexts. It\n",
    "    provides an indication of how much variance in the dependent variable is explained by the model.\n",
    "    \"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "def custom_legend(construct_name, color):\n",
    "    \"\"\"\n",
    "    Generates a custom legend for plots, associating colors and labels with specific data or model components.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    construct_name : str\n",
    "        The name of the construct to be labeled, representing a specific dataset or model part.\n",
    "    color : str\n",
    "        The color assigned to the construct in the plot.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function creates legend elements for data points, the fitted line, and confidence intervals\n",
    "    associated with a given construct, enhancing the visual interpretation of plotted data.\n",
    "    \"\"\"\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=f'{construct_name}: Data', alpha=0.2),\n",
    "        Line2D([0], [0], color=color, lw=4, label=f'{construct_name}: Fitted Line'),\n",
    "        Line2D([0], [0], color='w', lw=4, label=f'{construct_name}: 95% CI', alpha=0.2, markerfacecolor=color, markersize=15)\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements, loc='best', fontsize='small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_c_sat_func(ax, df, x_col, y_col, x_lim, construct_name, color, add_cursor=False):\n",
    "    \"\"\"\n",
    "    Plots the data, fitted line, and confidence intervals for a given DataFrame on a specified Axes object,\n",
    "    highlighting specific aspects such as data points, fitted function, and confidence intervals, with\n",
    "    optional interactivity via a data cursor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The Axes object where the data will be plotted.\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the data to be plotted. It should include the specified x_col and y_col.\n",
    "    x_col : str\n",
    "        The name of the column to use as the x-axis data.\n",
    "    y_col : str\n",
    "        The name of the column to use as the y-axis data.\n",
    "    construct_name : str\n",
    "        The name of the construct being analyzed, used for labeling in the plot.\n",
    "    color : str\n",
    "        The color to use for all plot elements associated with this construct.\n",
    "    add_cursor : bool, optional\n",
    "        If True, adds an interactive cursor that displays additional data points information on hover. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scatter : matplotlib.collections.PathCollection\n",
    "        The scatter plot object representing the data points.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function assumes a pre-existing curve fitting function `fit_c_sat_threshold` is available for use. It involves\n",
    "    sorting and cleaning the data, performing curve fitting, plotting the results, and optionally adding interactive\n",
    "    features. Errors during the fitting process are caught and logged, and the function will terminate early if fitting\n",
    "    fails. R-squared value for the fit is calculated and output to the console for reference.\n",
    "\n",
    "    The plot produced includes:\n",
    "    - Scatter plot of the raw data points.\n",
    "    - Line plot of the fitted model.\n",
    "    - Shaded area representing the 95% confidence intervals of the fit.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    fig, ax = plt.subplots()\n",
    "    plot_c_sat_func(ax, my_dataframe, 'time', 'response', 'Experiment 1', 'blue')\n",
    "    plt.show()\n",
    "\n",
    "    This function is flexible and intended for general use in data visualization tasks involving fitted models.\n",
    "    \"\"\"\n",
    "    # Ensure DataFrame is sorted by x_col for consistent plotting\n",
    "    #df_sorted = df.sort_values(by=x_col).reset_index(drop=True)\n",
    "    #df_sorted = df_sorted.dropna(subset=[x_col, y_col])  # Drop rows with NaN values in x_col or y_col\n",
    "    df_sorted = df.copy()\n",
    "\n",
    "    # Apply the curve fitting function\n",
    "    try:\n",
    "        m_opt, x0_opt, pcov = fit_c_sat_threshold(df_sorted, x_col, y_col, x_lim)\n",
    "        perr = np.sqrt(np.diag(pcov))  # Standard errors of the fitted parameters\n",
    "    except Exception as e:\n",
    "        print(f\"Error in curve fitting: {e}\")\n",
    "        return\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    x_data = df_sorted[x_col]\n",
    "    y_data = df_sorted[y_col]\n",
    "    y_pred = np.maximum(0, m_opt * (x_data - x0_opt))  # Predicted y values using the fitted function\n",
    "\n",
    "    # Scatter plot of the data points\n",
    "    scatter = ax.scatter(x_data, y_data, alpha=0.2, s=80, color=color) #, label=f'{construct_name}: Data')\n",
    "\n",
    "    # Plot the fitted line\n",
    "    ax.plot(x_data, y_pred, label=f'{construct_name}: Fitted Line (m={m_opt:.2f}, x0={x0_opt:.2f})', color=color, linewidth=2)\n",
    "\n",
    "    # Calculate and plot confidence intervals\n",
    "    ci_lower, ci_upper = calculate_confidence_interval(x_data, y_data, y_pred, m_opt, x0_opt)\n",
    "    ax.fill_between(x_data, ci_lower, ci_upper, color=color, alpha=0.2) #, label=f'{construct_name}: 95% Confidence Interval')\n",
    "\n",
    "    # R^2 Calculation\n",
    "    cutoff_idx = x_data[x_data > x_lim].index[0] if x_lim < x_data.max() else x_data.idxmax()\n",
    "    #print(y_data[cutoff_idx])\n",
    "    r_squared = calculate_r_squared(y_data[:cutoff_idx], y_pred[:cutoff_idx])\n",
    "    #r_squared = calculate_r_squared(y_data, y_pred)\n",
    "    print(f'{construct_name} - Fitted Parameters: m={m_opt:.2f} ± {perr[0]:.2f}, x0={x0_opt:.2f} ± {perr[1]:.2f}, R^2={r_squared:.3f}')\n",
    "\n",
    "    # Optionally add cursor if add_cursor is True\n",
    "    if add_cursor:\n",
    "        cursor = mplcursors.cursor(scatter, hover=True)\n",
    "        cursor.connect(\"add\", lambda sel: sel.annotation.set_text(\n",
    "            f'File: {df_sorted[\"source_file\"].iloc[sel.index]}\\nCell: {df_sorted[\"label\"].iloc[sel.index]}'))\n",
    "\n",
    "    return scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_func(ax, df, x_col, y_col, label, color):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot on a specified matplotlib axes object using data from a DataFrame. This function\n",
    "    plots data points based on specified column names for the x and y axes, applies a label, and sets the color\n",
    "    for the data points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object on which the data will be plotted.\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame containing the data to plot.\n",
    "    x_col : str\n",
    "        The name of the column in the DataFrame to use for the x-axis values.\n",
    "    y_col : str\n",
    "        The name of the column in the DataFrame to use for the y-axis values.\n",
    "    label : str\n",
    "        The label to use for the plotted data in the legend.\n",
    "    color : str\n",
    "        The color to apply to the scatter plot points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scatter : matplotlib.collections.PathCollection\n",
    "        The scatter plot object created by this function.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The function assumes the presence of `x_col` and `y_col` in the DataFrame.\n",
    "    - The plot points are semi-transparent (alpha set to 0.33) and have a size of 60 for visibility.\n",
    "    - This function adds the scatter plot to the given Axes object without modifying other aspects of the figure,\n",
    "      such as axis labels or titles. These should be configured separately if required.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter_plot_func(ax, df, 'time', 'response', 'Dataset Label', 'blue')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Response')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    This utility is especially useful for quick visual comparisons of datasets within the same figure.\n",
    "    \"\"\"\n",
    "    scatter = ax.scatter(df[x_col], df[y_col], color=color, alpha=0.33, s=60, label=label)\n",
    "\n",
    "\n",
    "    return scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scatter_plot(df_label_color_zip, x_col, y_col, x_title, y_title, x_lim, y_lim, fig_size, add_cursor=True, c_sat_mode=False):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot or a customized saturation plot for multiple datasets on a single figure.\n",
    "\n",
    "    This function creates a scatter plot for each dataset provided in the iterable `df_label_color_zip`, \n",
    "    allowing for customization of plot aesthetics and interactivity options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_label_color_zip : iterable of tuples\n",
    "        An iterable containing tuples of (DataFrame, label, color), where each tuple represents a dataset\n",
    "        to plot, the label for the dataset, and the color for the plot points or line.\n",
    "    x_col : str\n",
    "        The name of the column to use as the x-axis data.\n",
    "    y_col : str\n",
    "        The name of the column to use as the y-axis data.\n",
    "    x_title : str\n",
    "        The title to set for the x-axis.\n",
    "    y_title : str\n",
    "        The title to set for the y-axis.\n",
    "    x_lim : tuple\n",
    "        The limit for the x-axis (min, max).\n",
    "    y_lim : tuple\n",
    "        The limit for the y-axis (min, max).\n",
    "    fig_size : tuple\n",
    "        The size of the figure to create (width, height).\n",
    "    add_cursor : bool, optional\n",
    "        If True, adds an interactive cursor that displays additional data point information on hover. Default is True.\n",
    "    c_sat_mode : bool, optional\n",
    "        If True, the function will use `plot_c_sat_func` to plot data points and their corresponding saturation\n",
    "        thresholds, otherwise it uses `scatter_plot_func` for basic scatter plots. Default is False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        The function directly modifies the matplotlib figure and shows it.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - `mplcursors` is used to add interactivity to the plot elements, allowing for more informative visualizations.\n",
    "    - The function handles both basic scatter plots and more specialized plots based on the mode selected with `c_sat_mode`.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    df_label_color_zip = [(df1, 'Experiment 1', 'blue'), (df2, 'Experiment 2', 'green')]\n",
    "    generate_scatter_plot(df_label_color_zip, 'Time', 'Response', 'Time (s)', 'Response Value', (0, 100), (0, 10), (10, 8))\n",
    "\n",
    "    This example sets up a plot with specified axis limits and labels, suitable for presenting time-response data from multiple experiments.\n",
    "    \"\"\"\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(figsize=fig_size)\n",
    "\n",
    "    scatters = []  # List to hold scatter objects for cursor\n",
    "    data_reference = {}  # Dictionary to map scatters to dataframes\n",
    "\n",
    "    # Loop through datasets\n",
    "    for df, construct_label, color in df_label_color_zip:\n",
    "        # Ensure DataFrame is sorted by x_col for consistent plotting\n",
    "        df_sorted = df.sort_values(by=x_col).reset_index(drop=True)\n",
    "        df_sorted = df_sorted.dropna(subset=[x_col, y_col])  # Drop rows with NaN values in x_col or y_col\n",
    "        if c_sat_mode:\n",
    "            scatter = plot_c_sat_func(ax, df_sorted, x_col, y_col, x_lim[1], construct_label, color)\n",
    "        else:\n",
    "            scatter = scatter_plot_func(ax, df_sorted, x_col, y_col, construct_label, color)\n",
    "        scatters.append(scatter)\n",
    "        data_reference[scatter] = df_sorted  # Store reference to dataframe\n",
    "\n",
    "    # Configure global plot properties\n",
    "    ax.set_xlabel(x_title, fontsize=16, labelpad=12)\n",
    "    ax.set_ylabel(y_title, fontsize=16, labelpad=12)\n",
    "    #custom_legend(construct_name, color)  # Apply the custom legend\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_xlim(x_lim)\n",
    "    ax.set_ylim(y_lim)\n",
    "    fig.tight_layout()  # Optimize layout for readability on the figure level\n",
    "\n",
    "\n",
    "    if add_cursor:\n",
    "        # Initialize cursor for all scatters\n",
    "        cursor = mplcursors.cursor(scatters, hover=True)\n",
    "        cursor.connect(\"add\", lambda sel: sel.annotation.set_text(\n",
    "            f\"Location: {os.path.dirname(data_reference[sel.artist]['source_file'].iloc[sel.index])}\\n\"\n",
    "            f\"File: {os.path.basename(data_reference[sel.artist]['source_file'].iloc[sel.index])}\\n\"\n",
    "            f\"Cell: {data_reference[sel.artist]['label'].iloc[sel.index]}\"\n",
    "        ))\n",
    "\n",
    "    #if add_cursor:\n",
    "        # Initialize cursor for all scatters\n",
    "    #    cursor = mplcursors.cursor(scatters, hover=True)\n",
    "    #    cursor.connect(\"add\", lambda sel: sel.annotation.set_text(\n",
    "    #        f'File: {data_reference[sel.artist][\"source_file\"].iloc[sel.index]}\\nCell: {data_reference[sel.artist][\"label\"].iloc[sel.index]}'))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell is an example of how to set the directory path manually and read dataframe csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_FL_dir = select_directory()\n",
    "#FUS_FL_dir = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS del RGG2 RGG3 Analysis Combined'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUS_FL_dir = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS FL Analysis Combined'\n",
    "FUS_FL_df = read_and_combine_csv_files(FUS_FL_dir, '_cell_df.csv', include_source_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_FL_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following cell is an example of how to plot one dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your datasets\n",
    "df_list = [FUS_FL_df]\n",
    "construct_list = ['FUS RGG2']\n",
    "color_list = ['orange']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'puncta_intensity_total'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Puncta Intensity (a.u.)'\n",
    "x_limits = (0, 150)\n",
    "y_limits = (0, 50)\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=False, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_del_RGG2_RGG3_df['intensity_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FUS_del_RGG2_RGG3_df.copy()\n",
    "# Apply the filter and select specific columns\n",
    "filtered_df = df[(df['intensity_total'] > 0) & (df['intensity_total'] <= 20) &\n",
    "                 (df['puncta_intensity_total'] > 0) & (df['puncta_intensity_total'] <= 20)][['label', 'intensity_total', 'puncta_intensity_total', 'source_file']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) \n",
    "print(filtered_df['source_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe by the puncta total intensity column\n",
    "FUS_FL_df_sorted = FUS_FL_df.sort_values(by='puncta_intensity_total').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the first non-zero value in the puncta_intensity_total column\n",
    "fnz_idx = FUS_FL_df_sorted['puncta_intensity_total'].ne(0).idxmax()\n",
    "\n",
    "# print the total intensity and puncta total intensity value and the source file for this index\n",
    "FUS_FL_df_sorted.iloc[fnz_idx]['source_file'], FUS_FL_df_sorted.iloc[fnz_idx]['intensity_total'], FUS_FL_df_sorted.iloc[fnz_idx]['puncta_intensity_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete this row \n",
    "FUS_FL_df_sorted = FUS_FL_df_sorted.drop(fnz_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading multiple datasets as separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_FL_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS FL Analysis'\n",
    "#FUS_SS18_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS SS18 Analysis Combined'\n",
    "#SS18_PLD_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/SS18 PLD Analysis Combined'\n",
    "FUS_del_RGG2_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS del RGG2 Analysis'\n",
    "FUS_del_RGG3_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS del RGG3 Analysis'\n",
    "FUS_del_RGG2_RGG3_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Analysis Folder/FUS Analysis Combined (latest version)/FUS del RGG2 RGG3 Analysis'\n",
    "FUS_del_RRM_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Data Folder/New Q2 Data/Analysis/FUS Analysis/FUS Analysis Combined (latest version)/FUS del RRM Analysis Combined'\n",
    "FUS_del_ZnF_directory = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Sam Data Folder/New Q2 Data/Analysis/FUS Analysis/FUS Analysis Combined (latest version)/FUS del ZnF Analysis Combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_FL_df = read_and_combine_csv_files(FUS_FL_directory, '_cell_df.csv', include_source_file=True)\n",
    "#FUS_SS18_df = read_and_combine_csv_files(FUS_SS18_directory, '_cell_df.csv', include_source_file=True)\n",
    "#SS18_PLD_df = read_and_combine_csv_files(SS18_PLD_directory, '_cell_df.csv', include_source_file=True)\n",
    "##FUS_del_RGG2_df = read_and_combine_csv_files(FUS_del_RGG2_directory, '_cell_df.csv', include_source_file=True)\n",
    "##FUS_del_RGG3_df = read_and_combine_csv_files(FUS_del_RGG3_directory, '_cell_df.csv', include_source_file=True)\n",
    "FUS_del_RGG2_RGG3_df = read_and_combine_csv_files(FUS_del_RGG2_RGG3_directory, '_cell_df.csv', include_source_file=True)\n",
    "##FUS_del_RRM_df = read_csv_files_from_subdirs(FUS_del_RRM_directory, '_cell_df.csv', include_source_file=True)\n",
    "##FUS_del_ZnF_df = read_csv_files_from_subdirs(FUS_del_ZnF_directory, '_cell_df.csv', include_source_file=True)\n",
    "#FUS_FL_puncta_df = read_csv_files_from_subdirs(FUS_FL_directory, '_puncta_df.csv', include_source_file=True)\n",
    "#FUS_SS18_puncta_df = read_csv_files_from_subdirs(FUS_SS18_directory, '_puncta_df.csv', include_source_file=True)\n",
    "#SS18_PLD_puncta_df = read_csv_files_from_subdirs(SS18_PLD_directory, '_puncta_df.csv', include_source_file=True)\n",
    "#FUS_del_RGG2_puncta_df = read_csv_files_from_subdirs(FUS_del_RGG2_directory, '_puncta_df.csv', include_source_file=True)\n",
    "#FUS_del_RGG3_puncta_df = read_csv_files_from_subdirs(FUS_del_RGG3_directory, '_puncta_df.csv', include_source_file=True)\n",
    "#FUS_del_RGG2_RGG3_puncta_df = read_csv_files_from_subdirs(FUS_del_RGG2_RGG3_directory, '_puncta_df.csv', include_source_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_del_RGG2_RGG3_df.iloc[209]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up plot options and plotting multiple datasets on one graph (C-Sat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your datasets\n",
    "df_list = [FUS_FL_df, FUS_del_RGG2_df, FUS_del_RGG3_df, FUS_del_RGG2_RGG3_df]\n",
    "construct_list = ['FUS FL', 'FUS del RGG2', 'FUS del RGG3', 'FUS del RGG2 del RGG3']\n",
    "color_list = ['g', 'b', 'r', 'orange']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'puncta_intensity_total'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Puncta Intensity (a.u.)'\n",
    "x_limits = (0, 200)\n",
    "y_limits = (0, 50)\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generic scatter plot setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your datasets\n",
    "df_list = [FUS_FL_df, FUS_SS18_df, SS18_PLD_df]\n",
    "construct_list = ['FUS FL', 'FUS SS18', 'SS18 PLD', 'FUS del RGG2 del RGG3']\n",
    "color_list = ['g', 'b', 'r', 'orange']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'number_of_puncta'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Number of Foci per Cell'\n",
    "x_limits = (0, 200)\n",
    "y_limits = None\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different constucts plotted with C-Sat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [FUS_FL_df, FUS_del_RRM_df, FUS_del_ZnF_df]\n",
    "construct_list = ['FUS FL','FUS del RRM', 'FUS del ZnF']\n",
    "color_list = ['g', 'y', 'k']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'puncta_intensity_total'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Puncta Intensity (a.u.)'\n",
    "x_limits = (0, 150)\n",
    "y_limits = (0, 50)\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different constructs again, but with the same plot options, hence not listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [FUS_FL_df, FUS_del_RGG2_RGG3_df, FUS_del_RGG2_df, FUS_del_RGG3_df]\n",
    "construct_list = ['FUS FL','FUS del RGG2 del RGG3', 'FUS del RGG2', 'FUS del RGG3']\n",
    "color_list = ['g', 'orange', 'c', 'm']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of boxplot \n",
    "\n",
    "#### Can be easily modified to work with various metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_list and construct_list are already defined as per your message\n",
    "df_list = [FUS_FL_puncta_df, FUS_SS18_puncta_df, SS18_PLD_puncta_df, FUS_del_RGG2_RGG3_puncta_df]\n",
    "construct_list = ['FUS FL', 'FUS SS18', 'SS18 PLD', 'FUS del RGG2 del RGG3']\n",
    "color_list = ['g', 'b', 'r', 'orange']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Append each DataFrame in df_list to df with an additional 'construct' column\n",
    "for df_i, construct in zip(df_list, construct_list):\n",
    "    df_i = df_i.copy()\n",
    "    df_i['construct'] = construct\n",
    "    df = pd.concat([df, df_i])\n",
    "\n",
    "# Create a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='construct', y='micron area', hue='construct', data=df, palette=color_list, showfliers=False, legend=False)\n",
    "#change the axis titles \n",
    "plt.xlabel('Construct', fontsize=16, labelpad=12)\n",
    "plt.ylabel('Foci Area (um^2)', fontsize=16, labelpad=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of how to load and analyze other dataframes from PyCAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFP_mCherry_dir = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Anushka Data Folder/Colocalization analysis for mCherry and GFP/mcherry and GFP 5-2-2024'\n",
    "FUS_FOXG1_dir = '/Users/christian.neureuter/Library/CloudStorage/Box-Box/Nuclear Condensates Shared Folder/Anushka Data Folder/Colocalization analysis for FUSPLD and FOXG1/FUS-PLD and FOXG1 -IDR1 in vitro (2)/Colocalization analysis'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWCCA_df = read_and_combine_csv_files(GFP_mCherry_dir, '_PWCCA_coefficient_df.csv', include_source_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = PWCCA_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting rows for a specific method\n",
    "pearsons = df[df['Method'] == \"Pearson's R value\"]\n",
    "weighted_taus = df[df['Method'] == \"Weighted Tau value\"]\n",
    "\n",
    "# Summing the coefficients for each selected method\n",
    "sum_pearsons = pearsons.filter(like='Coefficient').mean(axis=1).values[0]\n",
    "sum_weighted_taus = weighted_taus.filter(like='Coefficient').mean(axis=1).values[0]\n",
    "\n",
    "print(\"Avg of Pearson's Coefficients:\", sum_pearsons)\n",
    "print(\"Avg of Weighted Tau's Coefficients:\", sum_weighted_taus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "Use this area to modify code, test different parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_FL_dir = select_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUS_del_RGG2_RGG3_dir = select_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from 03142024 FUS Full length (SAM)/image 3 03142024 FUS_placeholder_name_cell_df.csv\n",
      "Reading from 03142024 FUS Full length (SAM)/image 4 03142024 FUS_placeholder_name_cell_df.csv\n",
      "Reading from 03142024 FUS Full length (SAM)/image 1 03142024 FUS_placeholder_name_cell_df.csv\n",
      "Reading from 03072024 FUS Full length (AS)/image 2_cell_df.csv\n",
      "Reading from 03072024 FUS Full length (AS)/image 1_cell_df.csv\n",
      "Reading from 03072024 FUS Full length (AS)/image 4_cell_df.csv\n",
      "Reading from 03072024 FUS Full length (AS)/image 3_cell_df.csv\n",
      "Reading from 03132024 FUS Full length analysis (AS)/image 2_cell_df.csv\n",
      "Reading from 03132024 FUS Full length analysis (AS)/image 1_cell_df.csv\n",
      "Reading from 03132024 FUS Full length analysis (AS)/image 4_placeholder_name_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 1_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/image 7_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 3_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 4_savename_test_total_refined_puncta_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 6_total_refined_puncta_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 2_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 1 (CN)/Image 5_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 2 (CN)/FUS FL 2 dapi_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 2 (CN)/FUS FL 5_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 2 (CN)/FUS FL 1_cell_labeled_puncta_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 2 (CN)/FUS FL 4_labeled_cell_mask_cell_df.csv\n",
      "Reading from FUS FL Imaging 2 (CN)/FUS FL 3_labeled_cell_mask_cell_df.csv\n",
      "Reading from 03212024 FUS full length analysis (AS)/image 2_cell_df.csv\n",
      "Reading from 03212024 FUS full length analysis (AS)/image 1_cell_df.csv\n",
      "Reading from 03212024 FUS full length analysis (AS)/image 4_cell_df.csv\n",
      "Reading from 03212024 FUS full length analysis (AS)/image 3_cell_df.csv\n",
      "Reading from FUS FL Imaging 3 (CN)/020224 FUS FL IMAGE 1 GFP_cell_df.csv\n",
      "Reading from FUS FL Imaging 3 (CN)/020224 FUS FL IMAGE 4 GFP_cell_df.csv\n",
      "Reading from FUS FL Imaging 3 (CN)/020224 FUS FL IMAGE 3 GFP_cell_df.csv\n",
      "Reading from FUS FL Imaging 3 (CN)/020224 FUS FL IMAGE 2 GFP_cell_df.csv\n",
      "Reading from FUS FL Imaging 3 (CN)/020224 FUS FL IMAGE 5 GFP_cell_df.csv\n",
      "Reading from 03212024 FUS Full length (SAM)/image 3 FUS Full length 03212024_placeholder_name_cell_df.csv\n",
      "Reading from 03212024 FUS Full length (SAM)/image 1 FUS full length 03212024_placeholder_name_cell_df.csv\n",
      "Reading from 03212024 FUS Full length (SAM)/image 2 FUS Full length 03212024_placeholder_name_cell_df.csv\n",
      "Reading from 03212024 FUS Full length (SAM)/image 4 FUS full length 03212024_placeholder_name_cell_df.csv\n"
     ]
    }
   ],
   "source": [
    "FUS_FL_df = read_and_combine_csv_files(FUS_FL_dir, '_cell_df.csv', include_source_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from 032024 del rgg2 rgg3 (SAM)/032024 FUS DEL RGG2,3 IMAGE 5 GFP_cell_df.csv\n",
      "Reading from 032024 del rgg2 rgg3 (SAM)/032024 FUS DEL RGG2,3 IMAGE 2 GFP_cell_df.csv\n",
      "Reading from 032024 del rgg2 rgg3 (SAM)/032024 FUS DEL RGG2,3 IMAGE 3 GFP_cell_df.csv\n",
      "Reading from 032024 del rgg2 rgg3 (SAM)/032024 FUS DEL RGG2,3 IMAGE 4 GFP_cell_df.csv\n",
      "Reading from 032024 del rgg2 rgg3 (SAM)/032024 FUS DEL RGG2,3 IMAGE 1 GFP_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 7_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 2_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 1_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 8_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 4_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 3_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 6_cell_df.csv\n",
      "Reading from 02292024 Del RGG2 and 3 analysis (AS)/image 5_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 3_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 6_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 5_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 10_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 7_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 2_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 11_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 1_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 14_cell_df.csv\n",
      "Reading from 02072024 Del RGG2 and RGG3 (AS)/FUS DEL RGG2 RGG3 image 4_cell_df.csv\n",
      "Reading from FUS del RGG2 RGG3 Imaging 2 (SAM)/fus (del rgg2 rgg3) 1_cell_df.csv\n",
      "Reading from FUS del RGG2 RGG3 Imaging 2 (SAM)/fus (del rgg2 rgg3) 5_cell_df.csv\n",
      "Reading from FUS del RGG2 RGG3 Imaging 2 (SAM)/fus (del rgg2 rgg3) 3_cell_df.csv\n",
      "Reading from FUS del RGG2 and 3 imaging 3 (AS)/020224 FUS DEL RGG 2 RGG 3 IMAGE 4 GFP_cell_df.csv\n",
      "Reading from 03212024 FUS Del RGG2 and 3 analysis (AS)/image 1_cell_df.csv\n",
      "Reading from 03212024 FUS Del RGG2 and 3 analysis (AS)/image 4_cell_df.csv\n",
      "Reading from 03212024 FUS Del RGG2 and 3 analysis (AS)/image 6_cell_df.csv\n",
      "Reading from 032324 rgg2 rgg3(SAM)/022324 FUS DEL RGG2,3 IMAGE 5 GFP_cell_df.csv\n",
      "Reading from 032324 rgg2 rgg3(SAM)/022324 FUS DEL RGG2,3 IMAGE 3 GFP_cell_df.csv\n",
      "Reading from 12082023 FUS del RGG2 and 3 analysis (AS)/4_cell_df.csv\n",
      "Reading from 12082023 FUS del RGG2 and 3 analysis (AS)/1d_cell_df.csv\n",
      "Reading from 12082023 FUS del RGG2 and 3 analysis (AS)/5_cell_df.csv\n",
      "Reading from 12082023 FUS del RGG2 and 3 analysis (AS)/3d_cell_df.csv\n",
      "Reading from 12082023 FUS del RGG2 and 3 analysis (AS)/2d_cell_df.csv\n"
     ]
    }
   ],
   "source": [
    "FUS_del_RGG2_RGG3_df = read_and_combine_csv_files(FUS_del_RGG2_RGG3_dir, '_cell_df.csv', include_source_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your datasets\n",
    "df_list = [FUS_FL_df, FUS_SS18_df, SS18_PLD_df]\n",
    "construct_list = ['FUS FL', 'FUS SS18', 'SS18 PLD', 'FUS del RGG2 del RGG3']\n",
    "color_list = ['g', 'b', 'r', 'orange']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'puncta_intensity_total'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Puncta Intensity (a.u.)'\n",
    "x_limits = (0, 150)\n",
    "y_limits = (0, 50)\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantile bands and median line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FUS_del_RGG2_RGG3_df.copy()\n",
    "# Ensure DataFrame is sorted by x_col for consistent plotting\n",
    "df = df.sort_values(by='intensity_total').reset_index(drop=True)\n",
    "df = df.dropna(subset=['intensity_total', 'puncta_intensity_total'])  # Drop rows with NaN values in x_col or y_col\n",
    "\n",
    "\n",
    "# Calculate rolling quantiles\n",
    "window_size = 15  # You can adjust the window size as needed\n",
    "df['lower_quantile'] = df['puncta_intensity_total'].rolling(window=window_size, center=True).quantile(0.25)\n",
    "df['median'] = df['puncta_intensity_total'].rolling(window=window_size, center=True).quantile(0.5)\n",
    "df['upper_quantile'] = df['puncta_intensity_total'].rolling(window=window_size, center=True).quantile(0.75)\n",
    "\n",
    "# Plotting\n",
    "plt.fill_between(df['intensity_total'], df['lower_quantile'], df['upper_quantile'], color='gray', alpha=0.5, label='25th-75th percentile band')\n",
    "plt.plot(df['intensity_total'], df['median'], color='red', label='Median')\n",
    "plt.scatter(df['intensity_total'], df['puncta_intensity_total'], alpha=0.1)  # Optional: raw data points\n",
    "plt.xlabel('Total Intensity')\n",
    "plt.ylabel('Puncta Total Intensity')\n",
    "plt.title('Quantile Bands with Rolling Median Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowess smoothing of median line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Calculate rolling median\n",
    "window_size = 15  # You can adjust the window size as needed\n",
    "df['median'] = df['puncta_intensity_total'].rolling(window=window_size, center=True).quantile(0.5)\n",
    "\n",
    "# LOWESS smoothing on the rolling median\n",
    "lowess = sm.nonparametric.lowess(df['median'].dropna(), df['intensity_total'][df['median'].notna()], frac=0.5)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lowess[:, 0], lowess[:, 1], color='blue', label='LOWESS Smoothed Median')\n",
    "plt.scatter(df['intensity_total'], df['puncta_intensity_total'], alpha=0.1, label='Raw Data')  # Optional: raw data points\n",
    "plt.xlabel('Total Intensity')\n",
    "plt.ylabel('Puncta Total Intensity')\n",
    "plt.title('LOWESS Smoothing of Rolling Median Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the derivative of the lowess smoothed median line \n",
    "# Step 2: Calculate the derivative of the smoothed line\n",
    "x_smooth = lowess[:, 0]\n",
    "y_smooth = lowess[:, 1]\n",
    "dy_dx = np.diff(y_smooth) / np.diff(x_smooth)\n",
    "\n",
    "# Step 3: Plot the derivative\n",
    "plt.plot(x_smooth[:-1], dy_dx, label='Derivative of LOWESS Smoothed Median Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Derivative')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF of x-values\n",
    "cdf = df['intensity_total'].sort_values().reset_index(drop=True).expanding().mean() / df['intensity_total'].max()\n",
    "\n",
    "# Plotting\n",
    "plt.plot(cdf, np.linspace(0, 1, len(cdf)), color='purple')\n",
    "plt.xlabel('Total Intensity')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16871b4c0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = FUS_FL_df.copy()\n",
    "\n",
    "# Sort by 'intensity_total'\n",
    "df.sort_values('puncta_intensity_total', inplace=True)\n",
    "\n",
    "# Calculate the CDF for 'intensity_total'\n",
    "df['cdf'] = df['puncta_intensity_total'].rank(method='average', pct=True)\n",
    "\n",
    "# Plotting the CDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['puncta_intensity_total'], df['cdf'], label='CDF of FUS FL', color='g')\n",
    "plt.xlabel('Foci Total Intensity')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('Cumulative Distribution Function (CDF) of Total Intensity')\n",
    "plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = FUS_del_RGG2_RGG3_df.copy()\n",
    "\n",
    "# Sort by 'intensity_total'\n",
    "df.sort_values('puncta_intensity_total', inplace=True)\n",
    "\n",
    "# Calculate the CDF for 'intensity_total'\n",
    "df['cdf'] = df['puncta_intensity_total'].rank(method='average', pct=True)\n",
    "\n",
    "# Plotting the CDF\n",
    "plt.plot(df['puncta_intensity_total'], df['cdf'], label='CDF of FUS delRGG2RGG3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-inf, 0.1313249722090524], [inf, 53.81820727807605])\n",
      "(250,) (250,) (250,)\n",
      "FUS FL - Fitted Parameters: m=0.16 ± 0.00, x0=22.33 ± 1.03, R^2=0.848\n"
     ]
    }
   ],
   "source": [
    "# Define your datasets\n",
    "df_list = [FUS_FL_df]\n",
    "construct_list = ['FUS FL']\n",
    "color_list = ['g']\n",
    "\n",
    "# Combine the datasets with the construct names and colors\n",
    "df_label_color_zip = zip(df_list, construct_list, color_list)\n",
    "\n",
    "x_column = 'intensity_total'\n",
    "y_column = 'puncta_intensity_total'\n",
    "x_title = 'Total Intensity (a.u.)'\n",
    "y_title = 'Puncta Intensity (a.u.)'\n",
    "x_limits = (0, 150)\n",
    "y_limits = (0, 50)\n",
    "\n",
    "fig_size = (10, 6)\n",
    "\n",
    "generate_scatter_plot(df_label_color_zip, x_column, y_column, x_title, y_title, x_limits, y_limits, fig_size, add_cursor=True, c_sat_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devbio-syn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
