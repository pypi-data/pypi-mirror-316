# pylint: disable=C0116
"""
Test Module for Open Document (.odt) Figures example

This module contains tests to ensure that the Open Document files generated by
the `example_msl-odt_figures.py` script match the expected output file,
`example_msl-odt_figures.odt`, located in the `expected_outputs` directory.

The tests focus on the following aspects:
- The correct creation of `example_msl-odt_figures.odt` by running the
  `example_msl-odt_figures.py` script.
- Comparison of number of text elements in the generated and expected files.
- Comparison of text content between the generated and expected files.
- Verification of metadata (title and creator) between the two files.
- Comparison of figure number between generated and expected files.
- Comparison of figure properties between the two files.
- Comparison of figure caption properties.

Setup Process:
--------------
The test module utilises a pytest fixture to run the
`example_msl-odt_figures.py` script before any tests are executed.
This fixture:
- Runs the script to create `example_msl-odt_figures.odt`
  in the same directory as the script.
- Ensures the file has been created successfully.
- Cleans up by deleting the generated `example_msl-odt_figures.odt`
  after all tests have run.

Test Cases:
-----------
- test_text_elements_count: Asserts that the number of text elements in the
  generated and expected files are the same.
- test_text_content_equality: Compares the content of corresponding text
  elements in both files and asserts that they are identical.
- test_metadata: Validates that the title and creator metadata in both files
  match.

Usage:
------
Run the tests using pytest:
```bash
pytest path/to/your/test_file.py
"""

import subprocess
from pathlib import Path
from inspect import currentframe
from os import environ
from odf.opendocument import load
from odf.draw import Frame
from odf.meta import InitialCreator, UserDefined
import pytest

# Directory paths
# Directory where this test file lives. Should be tests
BASE_DIR = Path(__file__).resolve().parent
# Path to directory with creation script
SCRIPT_DIR = BASE_DIR / "../src/msl/examples/odt"
# Directory for expected output
EXPECTED_OUTPUTS_DIR = BASE_DIR / "expected_outputs"
# Test file creation script
SCRIPT_FILE = SCRIPT_DIR / "example_msl-odt_figures.py"
# Test file created
TEST_FILE = SCRIPT_DIR / "example_msl-odt_figures.odt"
# Expected output file
EXPECTED_FILE = EXPECTED_OUTPUTS_DIR / "example_msl-odt_figures.odt"

# Run the script to create example_msl-odt_figures.odt at the start


@pytest.fixture(scope="module", autouse=True)
def setup_test_file():
    cleanup = True  # Delete created .odt files when finished

    # Run the example script with the modified environment
    try:
        # Copy the current environment and set PYTHONPATH using pathlib
        env = environ.copy()
        env["PYTHONPATH"] = str(Path(__file__).resolve().parent.parent / "src")
        subprocess.run(
            ["python", str(SCRIPT_FILE)],
            cwd=str(SCRIPT_DIR),
            env=env,
            check=True,
            text=True,
            capture_output=True
        )
    except subprocess.CalledProcessError as e:
        print("Subprocess failed with return code:", e.returncode)
        print("Output:", e.output)
        print("Error:", e.stderr)
        raise

    # Check that the file was created
    assert TEST_FILE.exists(), f"{TEST_FILE} does not exist."
    yield  # This generator means that after all tests we clean up

    # Cleanup after all tests
    if TEST_FILE.exists() and cleanup:
        TEST_FILE.unlink()  # Delete the test file


def test_figure_elements_exist():
    test = currentframe().f_code.co_name
    if not EXPECTED_FILE.exists():
        pytest.skip(f"Skipping {test}: golden file not found.")
    # Load generated document
    doc2 = load(EXPECTED_FILE)

    # Check number of figure elements
    content2 = doc2.getElementsByType(Frame)
    assert content2, "No frame elements in golden document."


def test_figure_elements_generated():
    test = currentframe().f_code.co_name
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {test}: test file not found.")
    # Load generated document
    doc1 = load(TEST_FILE)

    # Check number of figure elements
    content1 = doc1.getElementsByType(Frame)
    assert content1, "No frame elements in generated document."


def test_figure_elements_count():
    test = currentframe().f_code.co_name
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {test}: test file not found.")
    if not EXPECTED_FILE.exists():
        pytest.skip(f"Skipping {test}: golden file not found.")
    # Load both documents
    doc1 = load(TEST_FILE)
    doc2 = load(EXPECTED_FILE)

    # Check number of frame elements
    num_frame1 = len(doc1.getElementsByType(Frame))
    num_frame2 = len(doc2.getElementsByType(Frame))
    assert num_frame1 == num_frame2, "Different numbers of Frames. "\
        "Test: {num_frame1} != Expected: {num_frame2}"


def test_figure_properties_equality():
    test = currentframe().f_code.co_name
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {test}: test file not found.")
    if not EXPECTED_FILE.exists():
        pytest.skip(f"Skipping {test}: golden file not found.")
    # Load both documents
    doc1 = load(TEST_FILE)
    doc2 = load(EXPECTED_FILE)

    # Extract figures from both documents
    content1 = doc1.getElementsByType(Frame)
    content2 = doc2.getElementsByType(Frame)

    # Check number of figures
    if not len(content1) == len(content2):
        pytest.skip(f"Skipping {test}: files are different.")

    # Compare details of each figure
    for i, (item1, item2) in enumerate(zip(content1, content2)):
        # Extract details for generated figure
        source1 = item1.getAttribute('name')
        width1 = item1.getAttribute('width')
        height1 = item1.getAttribute('height')

        # Extract details for golden figure
        source2 = item2.getAttribute('name')
        width2 = item2.getAttribute('width')
        height2 = item2.getAttribute('height')

        # Assertions for figure details
        print(f"Comparing Figure {i+1} source. "
              f"Test: {source1}, Expected: {source2}")
        assert source1 == source2, f"Figure {i+1} source does not match. " \
            "Test: {source1} != Expected: {source2}"
        assert width1 == width2, f"Figure {i+1} width does not match. " \
            f"Test: {width1} != Expected: {width2}"
        assert height1 == height2, f"Figure {i+1} height does not match. " \
            f"Test: {height1} != Expected: {height2}"


def test_figure_caption_equality():
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {currentframe().f_code.co_name}: "
                    "test file not found.")
    if not EXPECTED_FILE.exists():
        pytest.skip(f"Skipping {currentframe().f_code.co_name}: "
                    "golden file not found.")
    # Load both documents
    doc1 = load(TEST_FILE)
    doc2 = load(EXPECTED_FILE)

    # Extract figures from both documents
    content1 = doc1.getElementsByType(Frame)
    content2 = doc2.getElementsByType(Frame)

    # Check number of figures
    if not len(content1) == len(content2):
        pytest.skip(f"Skipping {currentframe().f_code.co_name}: "
                    "files are different.")

    # Compare captions of each figure
    for i, (item1, item2) in enumerate(zip(content1, content2)):
        # Compare captions - should be the next subsequent paragraph
        # Skip over any blank paragraphs until a valid one is found
        caption_paragraph1 = item1.parentNode.nextSibling
        caption_paragraph2 = item2.parentNode.nextSibling

        if caption_paragraph1 and caption_paragraph1.tagName == 'text:p' and \
                caption_paragraph2 and caption_paragraph2.tagName == 'text:p':
            # Check both are styled as captions
            caption_style1 = caption_paragraph1.getAttribute('stylename')
            caption_style2 = caption_paragraph2.getAttribute('stylename')

            assert caption_style1 == caption_style2, f"Figure {i+1} caption " \
                f"paragraph style is different. Test: {caption_style1} != " \
                f"Expected {caption_style2}"
            if caption_style1 == "Caption":
                # Check caption text for Test File
                assert caption_paragraph1.firstChild is not None, \
                    f"Caption text missing for Figure {i+1} in Test File."
                # Check caption text for Expected File
                assert caption_paragraph2.firstChild is not None, \
                    f"Caption text missing for Figure {i+1} in Expected File."

                # Compare caption texts
                caption_text1 = caption_paragraph1.firstChild.data
                caption_text2 = caption_paragraph2.firstChild.data
                print(f"Comparing Figure {i+1} captions. "
                      f"Test: {caption_text1}, Expected: {caption_text2}")
                assert caption_text1 == caption_text2, \
                    f"Figure {i+1} caption does not match. " \
                    f"Test: {caption_text1} != Expected: {caption_text2}"

# At present odt puts the following metadata into the file:
#     InitialCreator: "msl-odt"
#     CreationDate: creation datetime in UTC with minute resolution
#     UserDefined: name = Title, text = document name


def test_metadata_creator():
    test = currentframe().f_code.co_name
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {test}: test file not found.")
    if not EXPECTED_FILE.exists():
        pytest.skip(f"Skipping {test}: golden file not found.")
    # Load both documents
    doc1 = load(TEST_FILE)
    doc2 = load(EXPECTED_FILE)

    # Retrieve creator metadata directly
    creator1 = doc1.meta.getElementsByType(InitialCreator)
    creator2 = doc2.meta.getElementsByType(InitialCreator)
    # Extract text from the generator element
    creator_text1 = creator1[0].firstChild.data if creator1 else ""
    creator_text2 = creator2[0].firstChild.data if creator2 else ""
    print(f"Creator1: {creator_text1}")
    print(f"Creator2: {creator_text2}")
    assert creator_text1 == creator_text2, \
        "Creator metadata fields do not match. " \
        f"Test: {creator_text1} != Expected: {creator_text2}"


def test_metadata_title():
    test = currentframe().f_code.co_name
    if not TEST_FILE.exists():
        pytest.skip(f"Skipping {test}: test file not found.")
    if not EXPECTED_FILE.exists():
        pytest.skip("Skipping {test}: golden file not found.")
    # Load both documents
    doc1 = load(TEST_FILE)
    doc2 = load(EXPECTED_FILE)

    # Locate the title metadata which is stored as UserDefined
    title1 = next((elem for elem in doc1.meta.getElementsByType(UserDefined)
                   if elem.getAttribute("name") == "Title"), None)
    title2 = next((elem for elem in doc2.meta.getElementsByType(UserDefined)
                   if elem.getAttribute("name") == "Title"), None)

    # Compare the titles
    title_text1 = title1.firstChild.data \
        if title1 and title1.firstChild else ""
    title_text2 = title2.firstChild.data \
        if title2 and title2.firstChild else ""
    print(f"Title1: {title_text1}")
    print(f"Title2: {title_text2}")
    assert title_text1 == title_text2, \
        "Title medata data fields do not match. " \
        f"Test: {title_text1} != Expected: {title_text2}"
