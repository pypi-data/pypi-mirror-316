# DetailConfigs ghostos.framework.llms.llms::LLMsYamlConfig
services:
  - name: moonshot
    base_url: https://api.moonshot.cn/v1
    token: $MOONSHOT_API_KEY
  - name: openai
    base_url: https://api.openai.com/v1
    token: $OPENAI_API_KEY
    proxy: $OPENAI_PROXY
  - name: anthropic
    token: $ANTHROPIC_API_KEY
    proxy: $ANTHROPIC_PROXY
    base_url: https://api.anthropic.com/v1
  - name: deepseek
    token: $DEEPSEEK_API_KEY
    base_url: https://api.deepseek.com/beta
    # proxy: $OPENAI_PROXY
# Configure default LLM API here.
default: gpt-4o
# The models below can be edited as you want, see details: ghostos.core.llms.configs:ModelConf
# the key of models is a `llm_api_name`, value is a ModelConf instance.
models:
  moonshot-v1-8k:
    service: moonshot
    model: moonshot-v1-8k
  moonshot-v1-32k:
    service: moonshot
    model: moonshot-v1-32k
  moonshot-v1-128k:
    service: moonshot
    model: moonshot-v1-128k
  gpt-3.5-turbo:
    service: openai
    model: gpt-3.5-turbo
  gpt-4:
    service: openai
    model: gpt-4
  gpt-4-turbo:
    service: openai
    model: gpt-4-turbo
  gpt-4o:
    service: openai
    model: gpt-4o
  claude-3-5-sonnet: # 200K context window, 3$/M input, 3.75$/M cache write, 0.3$/M cache read, 15$/M output
    service: anthropic
    model: claude-3-5-sonnet-20240620
  claude-3-haiku: # 200K context window, 0.25$/M input, 0.3$/M cache write, 0.03$/M cache read, 1.25$/M output
    service: anthropic
    model: claude-3-haiku-20240307
  deepseek-chat: # 128k context window, 4k output window. 1Y/M input, 0.1Y/M cache hit, 2Y/M output
    service: deepseek
    model: deepseek/deepseek-chat
  deepseek-coder: # 128k context window, 8k output window. 1Y/M input, 0.1Y/M cache hit, 2Y/M output
    service: deepseek
    model: deepseek/deepseek-coder
